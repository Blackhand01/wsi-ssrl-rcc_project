Di seguito trovi quattro configurazioni - tutte con **50 epoche** - pensate per isolare fattori diversi che, secondo la letteratura MoCo v2 e le peculiaritÃ  del tuo dataset (2 500 / 5 000 patch, batch-per-epoch molto ridotti), meritano di essere analizzati nel paper.
Ogni esperimento modifica **solo pochi iper-parametri chiave** per facilitarne lâ€™interpretazione statistica.

---

## 1ï¸âƒ£  *Queue size & Temperature*: Â«moco\_v2\_smallQÂ»

```yaml
moco_v2_smallQ:
  type: ssl
  backbone: "resnet18"
  proj_dim: 256
  patch_size: 224
  augmentation:
    enabled: true
    horizontal_flip: true
    rotation: [90, 180, 270]     # piccola variazione per invarianze di orientamento
    gaussian_blur: true
    grayscale: true
    color_jitter: {brightness: 0.4, contrast: 0.4, saturation: 0.4, hue: 0.1}
  training:
    epochs:        50            # âœ” invariato
    batch_size:    128           # metÃ  del baseline â†’ code piÃ¹ frequente
    optimizer:     "sgd"
    learning_rate: 0.03
    weight_decay:  1e-4
    queue_size:    256           # ğŸ”»
    momentum:      0.99
    temperature:   0.07          # ğŸ”» default MoCo-v2 per set piccoli
    lr_schedule:   "cosine"
    warmup_epochs: 5
```

**PerchÃ©?**
Con \~1 500 sample/batch = 128 â‡’ solo 12 batch/epoch, la coda di 1 024 chiavi del baseline resta â€œvecchiaâ€ per molte epoche, introducendo rumore e possibile collasso (come si vede nel log con loss â‰ˆ 8). Ridurre la **queue** a 256 e la **temperatura** a 0.07 (valore ottimale per MoCo v2 quando Ã¨ presente lâ€™MLP) dovrebbe:

* aumentare la freschezza dei negativi;
* rendere la distribuzione dei logit piÃ¹ scalata;
* stabilizzare la discesa della loss.

---

## 2ï¸âƒ£  *Augmentazioni spinte*: Â«moco\_v2\_augStrongÂ»

```yaml
moco_v2_augStrong:
  type: ssl
  backbone: "resnet18"
  proj_dim: 256
  patch_size: 224
  augmentation:
    enabled: true
    horizontal_flip: true
    rotation: [0, 90, 180, 270]
    gaussian_blur: true
    grayscale: true
    color_jitter: {brightness: 0.8, contrast: 0.8, saturation: 0.8, hue: 0.2}  # ğŸ”º
  training:
    epochs:        50
    batch_size:    128
    optimizer:     "sgd"
    learning_rate: 0.03
    weight_decay:  1e-5
    queue_size:    512
    momentum:      0.99
    temperature:   0.20
    lr_schedule:   "cosine"
    warmup_epochs: 5
```

**PerchÃ©?**
Il paper MoCo v2 dimostra che **strong augmentation** (+ blur, + color-jitter) Ã¨ ortogonale al batch-size e porta a boost lineari e di transfer. Su un dataset di istopatologia con patch estremamente simili, aumentare drasticamente la diversitÃ  sintattica â†’ riduce â€œshortcut learningâ€ su texture locali e forza il modello a catturare pattern piÃ¹ robusti.

*Hypothesis:* F1 macro del classifier lineare â†‘, possibile leggera penalitÃ  su accuracy patch-level ma migliore generalizzazione paziente-level.

---

## 3ï¸âƒ£  *CapacitÃ  del backbone & Momentum*: Â«moco\_v2\_deeperÂ»

```yaml
moco_v2_deeper:
  type: ssl
  backbone: "resnet34"   # ğŸ”º piÃ¹ capiente
  proj_dim: 512          # ğŸ”º
  patch_size: 224
  augmentation:
    enabled: true
    horizontal_flip: true
    gaussian_blur: true
    grayscale: true
    color_jitter: {brightness: 0.4, contrast: 0.4, saturation: 0.4, hue: 0.1}
  training:
    epochs:        50
    batch_size:    64            # â†“ per limiti memoria GPU
    optimizer:     "sgd"
    learning_rate: 0.05          # LR âˆ batch (linear scaling rule)
    weight_decay:  1e-4
    queue_size:    512
    momentum:      0.995         # ğŸ”º encoder_k piÃ¹ â€œinerzialeâ€
    temperature:   0.20
    lr_schedule:   "cosine"
    warmup_epochs: 5
```

**PerchÃ©?**
Verifichiamo se un backbone **piÃ¹ profondo** + **proiezione 512-d** Ã¨ utile nonostante il dataset compatto.
Aumentare la **momentum** a 0.995 compensa la maggior instabilitÃ  dovuta al minor numero di batch (queue update piÃ¹ dolce).

*Goal:* misurare trade-off costi/benefici tra capacitÃ  e overfitting; utile per discutere se reti â€œleggereâ€ siano sufficienti in ambito medicale.

---

## 4ï¸âƒ£  *Ottimizzatore alternativo*: Â«moco\_v2\_adamÂ»

```yaml
moco_v2_adam:
  type: ssl
  backbone: "resnet18"
  proj_dim: 256
  patch_size: 224
  augmentation:
    enabled: true
    horizontal_flip: true
    gaussian_blur: true
    grayscale: true
    color_jitter: {brightness: 0.4, contrast: 0.4, saturation: 0.4, hue: 0.1}
  training:
    epochs:        50
    batch_size:    128
    optimizer:     "adam"        # ğŸ”„
    learning_rate: 0.001         # tipico per Adam
    weight_decay:  1e-6
    queue_size:    512
    momentum:      0.99
    temperature:   0.20
    lr_schedule:   "cosine"
    warmup_epochs: 5
```

**PerchÃ©?**
La letteratura MoCo usa quasi sempre **SGD+momentum**; testare **Adam** quantifica il contributo del solo schema dâ€™ottimizzazione su:

* velocitÃ  di convergenza (loss piÃ¹ bassa nelle prime epoche?);
* stabilitÃ  su dataset piccoli con feature giÃ  normalizzate;
* eventuale peggioramento nei transfer-task (Adam tende a sovradattare).

---

### Come presentare i risultati nel paper

| Exp | Principale variabile | Attesa vs baseline          | Se migliora â†’ â€¦                             |
| --- | -------------------- | --------------------------- | ------------------------------------------- |
| 1   | queue 256 + Ï„ 0.07   | loss â†“, feature quality â†‘   | argomentare importanza di negative freschi  |
| 2   | strong aug           | macro-F1 patch & paziente â†‘ | mostrare robustezza a trasformazioni        |
| 3   | ResNet34 + proj 512  | se â†‘ macro-F1 ma â†‘ tempo    | discutere trade-off capacitÃ  vs overfitting |
| 4   | Adam                 | curva loss piÃ¹ regolare?    | discutere ruolo dellâ€™ottimizzatore          |

Queste quattro varianti coprono **capacitÃ  del modello**, **dinamica di coda**, **forza delle viste** e **schema di ottimizzazione**: un ablation study completo e immediatamente confrontabile grazie alle stesse 50 epoche.
