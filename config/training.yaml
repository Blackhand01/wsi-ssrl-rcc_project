# -----------------------------------------------------------------------------
# 0. Working Environment
# -----------------------------------------------------------------------------
env_paths:
  colab: "/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project"
  local: "/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project"

# -----------------------------------------------------------------------------
# 1. Experiment Identifier
#    Leave empty to auto-generate a timestamp
# -----------------------------------------------------------------------------
exp_code: "20250723180604"
seed: 42
# -----------------------------------------------------------------------------
# 2. Cross-Validation
# -----------------------------------------------------------------------------
folds: [0, 1, 2, 3]
train_encoder_once: true

# -----------------------------------------------------------------------------
# 3. Dataset Paths (WebDataset .tar files)
# -----------------------------------------------------------------------------
data:
  dataset_id: "dataset_9f30917e"
  train: "data/processed/{dataset_id}/webdataset/fold{fold_idx}/train/patches-0000.tar"
  val:   "data/processed/{dataset_id}/webdataset/fold{fold_idx}/val/patches-0000.tar"
  test:  "data/processed/{dataset_id}/webdataset/test_holdout.tar"
#  classes: ["ccRCC", "pRCC", "CHROMO", "ONCO", "not_tumour"]

# -----------------------------------------------------------------------------
# 4. Output Directory and File Patterns
# -----------------------------------------------------------------------------
output:
  exp_dir:       "data/processed/{dataset_id}/experiments/{exp_code}/"
  exp_model_dir: "{exp_dir}{model_name}/"

  training:
    ckpt:      "{exp_model_dir}fold{fold_idx}/training/{model_name}_bestepoch{epoch:03d}_fold{fold_idx}.pt"
    features:  "{exp_model_dir}fold{fold_idx}/training/{model_name}_features_fold{fold_idx}.pt"
    clf:       "{exp_model_dir}fold{fold_idx}/training/{model_name}_classifier_fold{fold_idx}.joblib"
    scaler:    "{exp_model_dir}fold{fold_idx}/training/{model_name}_ts_scaler_fold{fold_idx}.joblib"
    log:       "{exp_model_dir}fold{fold_idx}/training/{model_name}_train_log_fold{fold_idx}.md"
    loss_json: "{exp_model_dir}fold{fold_idx}/training/{model_name}_train_valid_loss_fold{fold_idx}.json"

  inference:
    patch_preds:  "{exp_model_dir}fold{fold_idx}/inference/{model_name}_patch_preds_fold{fold_idx}.pt"
    patient_preds: "{exp_model_dir}fold{fold_idx}/inference/{model_name}_patient_preds_fold{fold_idx}.csv"
    mc_logits:    "{exp_model_dir}fold{fold_idx}/inference/{model_name}_mc_logits_fold{fold_idx}.npy"
    metrics:      "{exp_model_dir}fold{fold_idx}/inference/{model_name}_metrics_fold{fold_idx}.json"

  explain:
    gradcam_dir:  "{exp_model_dir}fold{fold_idx}/explain/{patient_id}/"
    metadata_csv: "{exp_model_dir}fold{fold_idx}/explain/{model_name}_metadata_gradcam_fold{fold_idx}.csv"

  aggregate:
    metrics:     "{exp_model_dir}_aggregate/{model_name}_metrics.json"
    summary_img: "{exp_model_dir}_aggregate/{model_name}_summary_agg.jpg"

  experiment_level:
    comparison_json: "{exp_dir}_experiment_level/models_comparison.json"
    comparison_img:  "{exp_dir}_experiment_level/models_comparison.jpg"

  readme: "{exp_dir}README_EXPERIMENT.md"

# -----------------------------------------------------------------------------
# 5. Evaluation Settings
# -----------------------------------------------------------------------------
evaluation:
  mc_dropout_passes: 50
  ece_bins:          10
  gradcam:
    top_k: 5
    layer: "layer4"

# -----------------------------------------------------------------------------
# 6. Models to Run
# -----------------------------------------------------------------------------
run_models: ["transfer","rotation", "simclr", "jepa", "supervised", "moco_v2"]

# -----------------------------------------------------------------------------
# 7. Model Configurations
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------  
# 7. Model Configurations – EXPERIMENT 1 (50 epochs, patch 512, stride 256)  
# -----------------------------------------------------------------------------  
models:

  simclr:                         # [Chen et al., 2020]:contentReference[oaicite:1]{index=1}  
    type: ssl
    backbone:    "resnet18"
    patch_size:  512
    stride:      256
    proj_dim:    128
    training:
      epochs:          25         # N = 50 richiesto
      batch_size:      64         # testato su Colab T4 / M1 Metal ≈6 GB
      optimizer:       "adam"
      learning_rate:   3e-4       # valore stabile per batch 64
      weight_decay:    1e-4
      temperature:     0.5
    augmentation:                 # forte data-aug → invarianza tessiturale
      horizontal_flip: true
      rotation:         [0,90,180,270]
      color_jitter:     {brightness: 0.4, contrast: 0.4, saturation: 0.4, hue: 0.1}
      gaussian_blur:    true

  moco_v2:
      type: ssl
      backbone: "resnet18"
      patch_size: 512
      stride: 256
      proj_dim: 256
      training:
        epochs: 25
        batch_size: 64
        optimizer: "sgd"
        learning_rate: 0.0075      # 0.03 × 64/256
        weight_decay: 1e-4
        queue_size: 512
        momentum: 0.99            # EMA per gli encoder
        opt_momentum: 0.9         # momentum vero di SGD
        temperature: 0.2
        lr_schedule: "cosine"
        warmup_epochs: 5
      augmentation:               # “aug+” di MoCo v2
        horizontal_flip: true
        color_jitter: {brightness: 0.4, contrast: 0.4, saturation: 0.4, hue: 0.1}
        gaussian_blur: true
        grayscale: 0.2            # p = 0.2 applicata nel trainer

  rotation:                       # [Gidaris et al., 2018]:contentReference[oaicite:3]{index=3}   
    type: ssl
    backbone:    "resnet18"
    patch_size:  512
    stride:      256
    training:
      epochs:          25
      batch_size:      64
      optimizer:       "sgd"
      learning_rate:   1e-2        # più basso di SimCLR (task più semplice)
      weight_decay:    1e-4

  jepa:                           # [Assran et al., 2023]:contentReference[oaicite:4]{index=4}  
    type: ssl
    backbone:      "resnet18"
    patch_size:    512
    stride:        256
    context_size:  512
    hidden_dim:    512
    training:
      epochs:          25
      batch_size:      32          # memoria ↑ (predizione multi-block)
      optimizer:       "adamw"
      learning_rate:   1e-4
      weight_decay:    1e-2
    augmentation:
      horizontal_flip: true

  supervised:
    type: sl
    backbone:    "resnet50"
    patch_size:  512
    stride:      256
    pretrained:  false
    training:
      epochs:          25
      batch_size:      32
      optimizer:       "sgd"
      momentum:        0.9
      learning_rate:   0.05        # step-decay ↘ ogni 15 epoche
      weight_decay:    1e-4

  transfer:
    type: sl
    backbone:    "resnet50"
    patch_size:  512
    stride:      256
    pretrained:  true              # ImageNet
    training:
      epochs:          25
      batch_size:      32
      optimizer:       "adam"
      learning_rate:   1e-4        # solo fine-tuning layer4+fc
      weight_decay:    1e-5