{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM1EMu0TVjvXT+3KMlzXyrV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### üß© Generating the Patch Sampling DataFrame\n","\n","This notebook builds a patch-level DataFrame used to extract informative image crops (patches) from RCC WSIs. The resulting `.parquet` file is used as input for the training phase. The notebook performs the following steps:\n","\n","1. **Load the configuration** from `preprocessing.yaml`, resolving environment paths and parameters.\n","2. **Select the active stage** (`debug` or `training`) and extract patching/sampling parameters (patch size, stride, seed, patients per class).\n","3. **Load and filter the metadata**, keeping only entries with valid XML annotations or ROI files.\n","4. **Optionally downsample patients**, selecting a subset per class if sampling is enabled.\n","5. **Build reusable ROI maps** for each subtype, mapping patients to annotated areas (either XML or ROI).\n","6. **Split patients into train/val/test**, based on configured proportions and a random seed.\n","7. **Sample patches per subtype and split**, ensuring balanced class distribution and extracting spatial coordinates within annotated areas.\n","8. **Build and export the patch DataFrame**, saving it as a `.parquet` file inside a uniquely versioned dataset folder.\n","9. **Log metadata and register the dataset**, writing a version descriptor `.md` and updating a central CSV registry.\n","10. **Run integrity checks**, including patch counts per class/split, duplicated patient IDs, and subtype overlaps.\n","11. **Summarize used WSIs**, printing dimensions and file sizes to ensure correctness.\n","\n","---\n","\n","| #  | **Section (Markdown Title)** | **Main Content (Documented Classes/Functions)**                                                                      | **Output**                |\n","| -- | ---------------------------- | -------------------------------------------------------------------------------------------------------------------- | ------------------------- |\n","| 1  | **Config & Path Loader**     | `ConfigLoader`, `PathResolver`: load `preprocessing.yaml`, resolve all placeholders and paths                        | `CFG`, `PATHS`            |\n","| 2  | **Stage Parameters**         | `StageManager`: extracts `patch_size`, `stride`, `patches_per_class`, split ratios, sampling seed, selected patients | `MGR`, `MGR.stage`        |\n","| 3  | **Helper Functions**         | `parse_rois`, `is_patch_informative`, `extract_patient_id`                                                           | ROI/XML/image utilities   |\n","| 4  | **Load and Filter Metadata** | Loads `metadata.csv`, filters rows with either XML or ROI info, applies optional downsampling                        | `metadata` (DataFrame)    |\n","| 5  | **ROI Maps Construction**    | `build_roi_map`: builds a map from `patient_id` to XML or ROI records, used for patch sampling                       | `roi_maps` dictionary     |\n","| 6  | **Patient-Level Split**      | Splits patients into train/val/test groups based on configured proportions                                           | `patient_split` (dict)    |\n","| 7  | **Patch Sampling**           | Stratified sampling of informative patches using `OpenSlide`, bounding box sampling from ROI or XML                  | list of `rows`            |\n","| 8  | **Build Patch DataFrame**    | Builds final `patch_df`, saves `.parquet`, writes `version_descriptor.md`, updates `dataset_registry.csv`            | `.parquet`, `.md`, `.csv` |\n","| 9  | **Preview & Sanity Check**   | Loads saved `.parquet`, previews shape/columns/nulls, counts patches by `subtype` and `split`                        | `loaded_df`, `print()`    |\n","| 10 | **Duplicate Patient IDs**    | Finds duplicated `patient_id`s and reports those appearing in more than one subtype                                  | printed logs              |\n","| 11 | **WSI Usage Summary**        | Prints paths, file sizes (bytes), width/height (pixels) of all WSIs used                                             | console table             |\n"],"metadata":{"id":"vEGVHsIk1fwn"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"Crb3Teblj_-e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751065663217,"user_tz":-120,"elapsed":5135,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"aba67d31-a9aa-4a35-8a86-7f2fb73c573a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!apt-get update -qq && apt-get install -qq -y libopenslide-dev\n","!pip install openslide-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQcnPjABqrjB","executionInfo":{"status":"ok","timestamp":1751065675993,"user_tz":-120,"elapsed":12780,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"cfdcbdbe-e004-4f9e-edf1-36606e411195"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Requirement already satisfied: openslide-python in /usr/local/lib/python3.11/dist-packages (1.4.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from openslide-python) (11.2.1)\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MLRuXVaSezb-","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1751065854888,"user_tz":-120,"elapsed":74,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"5253824d-cfeb-4166-bfb3-1c12d7029868"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'base'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-9-3955776694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# --- init config & paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mCFG\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mConfigLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYAML_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mPATHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPathResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ YAML loaded ‚Äì RAW root:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATHS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-9-3955776694.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, yaml_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_substitute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_select_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-9-3955776694.py\u001b[0m in \u001b[0;36m_substitute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# 1. base section\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         base = {k: v.replace('${RESOLVED_BASE_DIR}', str(self.base_dir))\n\u001b[1;32m     30\u001b[0m                 for k, v in base.items()}\n","\u001b[0;31mKeyError\u001b[0m: 'base'"]}],"source":["# Cell 1 ‚Äì carica YAML e risolvi percorsi\n","from pathlib import Path\n","import yaml, copy\n","\n","YAML_PATH = Path('/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/config/preprocessing.yaml')\n","if not YAML_PATH.exists():\n","    YAML_PATH = Path('config/preprocessing.yaml')\n","\n","class ConfigLoader:\n","    \"\"\"\n","    Load preprocessing.yaml, pick environment, substitute placeholders.\n","    \"\"\"\n","    def __init__(self, yaml_path: Path):\n","        self.raw = yaml.safe_load(yaml_path.read_text())\n","        self.base_dir = self._select_env()\n","        self.cfg = self._substitute()\n","\n","    def _select_env(self) -> Path:\n","        env = self.raw['env_paths']\n","        colab, local = Path(env['colab']), Path(env['local'])\n","        if colab.exists():  return colab\n","        if local.exists():  return local\n","        raise FileNotFoundError('No valid environment path.')\n","\n","    def _substitute(self) -> dict:\n","        cfg = copy.deepcopy(self.raw)\n","        # 1. base section\n","        base = cfg['base']\n","        base = {k: v.replace('${RESOLVED_BASE_DIR}', str(self.base_dir))\n","                for k, v in base.items()}\n","        cfg['base'] = base\n","        # 2. placeholder map\n","        ph = {'${RESOLVED_BASE_DIR}': str(self.base_dir), **{f'${{base.{k}}}': v for k, v in base.items()}}\n","        def repl(o):\n","            if isinstance(o, str):\n","                for k, v in ph.items():\n","                    o = o.replace(k, v)\n","            elif isinstance(o, dict):\n","                o = {k: repl(v) for k, v in o.items()}\n","            elif isinstance(o, list):\n","                o = [repl(v) for v in o]\n","            return o\n","        for sec in ['data_paths', 'stage_overrides', 'patching_defaults', 'split_by_patient']:\n","            cfg[sec] = repl(cfg[sec])\n","        return cfg\n","\n","class PathResolver:\n","    \"\"\"\n","    Translate data_paths (under data_root) into paths under dataraw_root.\n","    \"\"\"\n","    def __init__(self, cfg: dict):\n","        b, dp = cfg['base'], cfg['data_paths']\n","        self.project_root  = Path(b['project_root'])\n","        self.data_root     = Path(b['data_root'])\n","        self.raw_root      = Path(b['dataraw_root'])\n","        self.mapping_root  = Path(b['mapping_root'])\n","        self.metadata_root = Path(b['metadata_root'])\n","\n","        def raw(p): return self.raw_root / Path(p).relative_to(self.data_root)\n","\n","        # ccRCC / pRCC normal & pre\n","        self.ccrcc_wsi      = raw(dp['ccRCC']['wsi'])\n","        self.ccrcc_xml      = raw(dp['ccRCC']['xml'])\n","        self.pre_ccrcc_wsi  = raw(dp['ccRCC']['pre']['wsi'])\n","        self.pre_ccrcc_xml  = raw(dp['ccRCC']['pre']['xml'])\n","        self.prcc_wsi       = raw(dp['pRCC']['wsi'])\n","        self.prcc_xml       = raw(dp['pRCC']['xml'])\n","        self.pre_prcc_wsi   = raw(dp['pRCC']['pre']['wsi'])\n","        self.pre_prcc_xml   = raw(dp['pRCC']['pre']['xml'])\n","\n","        # CHROMO / ONCO\n","        self.chromo_wsi         = raw(dp['CHROMO']['wsi'])\n","        self.chromo_ann         = raw(dp['CHROMO']['annotations'])\n","        self.onco_wsi           = raw(dp['ONCO']['wsi'])\n","        self.onco_ann           = raw(dp['ONCO']['annotations'])\n","\n","# --- init config & paths\n","CFG   = ConfigLoader(YAML_PATH).cfg\n","PATHS = PathResolver(CFG)\n","print(\"‚úÖ YAML loaded ‚Äì RAW root:\", PATHS.raw_root)\n"]},{"cell_type":"code","source":["# Cell 2 ‚Äì determina stage attivo (debug / training) e parametri patching\n","import random\n","\n","class StageManager:\n","    \"\"\"\n","    Extract active stage and all sampling/patching params.\n","    Also track per-subtype which patients were selected.\n","    \"\"\"\n","    def __init__(self, cfg: dict):\n","        ov = cfg['stage_overrides']\n","        # decide training vs debug\n","        if ov['training']['sampling']['enabled']:\n","            self.stage = 'training'\n","        elif ov['debug']['sampling']['enabled']:\n","            self.stage = 'debug'\n","        else:\n","            raise RuntimeError(\"No sampling enabled in any stage!\")\n","\n","        # sampling params\n","        self.sampling_enabled   = ov[self.stage]['sampling']['enabled']\n","        self.patients_per_class = ov[self.stage]['sampling']['patients_per_class']\n","\n","        # patching params\n","        pch = ov[self.stage]['patching']\n","        self.patch_size      = pch['patch_size']\n","        self.stride          = pch['stride']\n","        self.patches_per_cls = pch['patches_per_class']\n","        self.random_seed     = pch['random_seed']\n","\n","        # split params\n","        split = cfg['split_by_patient']\n","        self.split_prop = split['proportions']\n","        self.split_seed = split['random_seed']\n","\n","        # **new flag**: per-subtype selected patients\n","        self.selected_patients = {}\n","\n","# init\n","MGR = StageManager(CFG)\n","print(f\"‚úÖ Active stage: {MGR.stage} | patch_size={MGR.patch_size}\")\n","random.seed(MGR.random_seed)"],"metadata":{"id":"3aTvl1h_e4Gh","executionInfo":{"status":"aborted","timestamp":1751065676069,"user_tz":-120,"elapsed":34,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 3 ‚Äì helper functions\n","import xml.etree.ElementTree as ET\n","import numpy as np\n","from PIL import Image\n","\n","def parse_rois(xml_path: Path):\n","    \"\"\"\n","    Parse ASAP XML and return list of (minx, maxx, miny, maxy, label).\n","    Non-tumor annotations are mapped to 'not_tumor'.\n","    \"\"\"\n","    rois, tree = [], ET.parse(xml_path)\n","    annots = tree.getroot().find('Annotations')\n","    if annots is None: return rois\n","    for a in annots.findall('Annotation'):\n","        group = a.attrib.get('PartOfGroup', '').lower()\n","        coords = a.find('Coordinates')\n","        xs = [float(c.attrib['X']) for c in coords.findall('Coordinate')]\n","        ys = [float(c.attrib['Y']) for c in coords.findall('Coordinate')]\n","        if not xs: continue\n","        rois.append((\n","            int(min(xs)), int(max(xs)),\n","            int(min(ys)), int(max(ys)),\n","            'tumor' if group == 'tumor' else 'not_tumor'\n","        ))\n","    return rois\n","\n","def is_patch_informative(pil_img, thresh=10):\n","    \"\"\"Return True if grayscale std-dev > thresh.\"\"\"\n","    return np.array(pil_img.convert('L')).std() > thresh\n","\n","def extract_patient_id(wsi_filename: str) -> str:\n","    \"\"\"\n","    Keep portion before '_' and join first two dot-segments.\n","    \"\"\"\n","    base = wsi_filename.split('_', 1)[0]\n","    parts = base.split('.')\n","    return '.'.join(parts[:2]) if len(parts) >= 2 else parts[0]\n"],"metadata":{"id":"W79dlOcSe7F8","executionInfo":{"status":"aborted","timestamp":1751065676079,"user_tz":-120,"elapsed":41,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 4 ‚Äì Load & filter metadata (now also for training sampling)\n","import pandas as pd\n","\n","metadata = pd.read_csv(PATHS.metadata_root / 'metadata.csv', dtype=str)\n","\n","# keep only rows with XML or ROI\n","has_xml = metadata['annotation_xml'].fillna('').ne('')\n","has_roi = metadata['roi_files'].fillna('').ne('')\n","metadata = metadata[has_xml | has_roi].reset_index(drop=True)\n","\n","# 1) count before sampling\n","unique_before = metadata['patient_id'].nunique()\n","print(f\"\\n‚ÑπÔ∏è Sampling active for stage '{MGR.stage}'\")\n","print(f\"   ‚Ä¢ Unique patients before sampling: {unique_before}\")\n","\n","# 2) apply down‚Äìsampling if enabled\n","if MGR.sampling_enabled:\n","    selected = []\n","    for subtype, grp in metadata.groupby('subtype'):\n","        pats = list(grp['patient_id'].unique()[:MGR.patients_per_class])\n","        print(f\"   ‚Üí {subtype}: selecting first {len(pats)} patients ‚Üí {pats}\")\n","        selected.extend(pats)\n","    metadata = metadata[metadata['patient_id'].isin(selected)].reset_index(drop=True)\n","\n","# 3) count after sampling\n","unique_after = metadata['patient_id'].nunique()\n","print(f\"   ‚Ä¢ Unique patients after sampling:  {unique_after}\")\n","\n","# 4) summary per subtype\n","print(\"\\n‚úÖ Patients per subtype (after sampling):\")\n","for subtype, grp in metadata.groupby('subtype'):\n","    up = grp['patient_id'].nunique()\n","    print(f\"   ‚Ä¢ {subtype:7}: {up} patients\")\n"],"metadata":{"id":"OW6Y600ie8V4","executionInfo":{"status":"aborted","timestamp":1751065676088,"user_tz":-120,"elapsed":45,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 5 ‚Äì costruisci roi_maps riutilizzabili\n","from collections import defaultdict\n","\n","def build_roi_map(sub_meta, subtype):\n","    \"\"\"\n","    Build map patient_id -> list of ROI records for a subtype.\n","    \"\"\"\n","    rm = defaultdict(list)\n","    for _, row in sub_meta.iterrows():\n","        pid = row['patient_id']\n","        if subtype in ('ccRCC', 'pRCC'):           # XML parsing\n","            xml_root = PATHS.pre_ccrcc_xml if row['source_dir'].startswith('pre') and subtype=='ccRCC' else \\\n","                       PATHS.pre_prcc_xml if row['source_dir'].startswith('pre') and subtype=='pRCC' else \\\n","                       PATHS.ccrcc_xml if subtype=='ccRCC' else PATHS.prcc_xml\n","            wsi_root = PATHS.pre_ccrcc_wsi if row['source_dir'].startswith('pre') and subtype=='ccRCC' else \\\n","                       PATHS.pre_prcc_wsi if row['source_dir'].startswith('pre') and subtype=='pRCC' else \\\n","                       PATHS.ccrcc_wsi if subtype=='ccRCC' else PATHS.prcc_wsi\n","            for xml_name in row['annotation_xml'].split(';'):\n","                xml_path = xml_root / xml_name\n","                if not xml_path.exists(): continue\n","                for *box, label in parse_rois(xml_path):\n","                    eff_sub = subtype if label == 'tumor' else 'not_tumor'\n","                    rm[pid].append(('xml', wsi_root / row['wsi_filename'], xml_path, box, eff_sub))\n","        else:                                      # CHROMO / ONCO ROI files\n","            ann_root = PATHS.onco_ann if subtype == 'ONCO' else PATHS.chromo_ann\n","            for roi in row['roi_files'].split(';'):\n","                rm[pid].append(('roi', ann_root/roi, None, None, subtype))\n","    return rm\n","\n","roi_maps = {s: build_roi_map(metadata[metadata.subtype == s], s)\n","            for s in metadata.subtype.unique()}"],"metadata":{"id":"z52q7W_AhDDb","executionInfo":{"status":"aborted","timestamp":1751065676097,"user_tz":-120,"elapsed":52,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 6 ‚Äì split pazienti secondo proporzioni da YAML\n","import math, random\n","from collections import Counter\n","rng_split = random.Random(MGR.split_seed)\n","\n","patient_split = {}\n","for subtype, grp in metadata.groupby('subtype'):\n","    pats = sorted(grp['patient_id'].unique())\n","    rng_split.shuffle(pats)\n","    n = len(pats)\n","    tr = max(1, math.floor(n * MGR.split_prop['train']))\n","    va = math.floor(n * MGR.split_prop['val'])\n","    te = n - tr - va\n","    if n >= 3 and va == 0:\n","        va, tr = 1, tr-1\n","    idx = 0\n","    for p in pats[idx:idx+tr]: patient_split[p] = 'train'\n","    idx += tr\n","    for p in pats[idx:idx+va]: patient_split[p] = 'val'\n","    idx += va\n","    for p in pats[idx:idx+te]: patient_split[p] = 'test'\n","\n","\n","print(\"\\n‚úÖ Patients per split (unique count):\")\n","for subtype in sorted(metadata['subtype'].unique()):\n","    df_sub = metadata[metadata['subtype'] == subtype]\n","    # pazienti unici di questo subtype\n","    upats = sorted(df_sub['patient_id'].unique())\n","    # conteggio univoco per split\n","    counts = {sp: len([p for p in upats if patient_split[p] == sp])\n","              for sp in ['train','val','test']}\n","    print(f\"  {subtype:7}: {counts}\")"],"metadata":{"id":"9GAsqy8we9ZT","executionInfo":{"status":"aborted","timestamp":1751065676104,"user_tz":-120,"elapsed":56,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 7 ‚Äì campionamento patch bilanciato\n","from openslide import OpenSlide\n","from tqdm.notebook import tqdm\n","\n","rng = random.Random(MGR.random_seed)\n","rows = []\n","splits = ['train', 'val', 'test']\n","\n","def choose_dirs(sub, src_dir):\n","    \"\"\"Return correct WSI dir given subtype and source_dir ('pre/‚Ä¶' or not).\"\"\"\n","    if sub == 'ccRCC':\n","        return PATHS.pre_ccrcc_wsi if src_dir.startswith('pre') else PATHS.ccrcc_wsi\n","    if sub == 'pRCC':\n","        return PATHS.pre_prcc_wsi  if src_dir.startswith('pre') else PATHS.prcc_wsi\n","    if sub == 'CHROMO': return PATHS.chromo_wsi\n","    if sub == 'ONCO':   return PATHS.onco_wsi\n","\n","for subtype in list(metadata.subtype.unique()) + ['not_tumor']:\n","    print(f\"\\n‚û°Ô∏è  Sampling {subtype}\")\n","    tgt = {sp: int(MGR.patches_per_cls * MGR.split_prop[sp]) for sp in splits}\n","    tgt['train'] += MGR.patches_per_cls - sum(tgt.values())  # fix rounding\n","\n","    # ROI map\n","    if subtype == 'not_tumor':\n","        roi_map = defaultdict(list)\n","        for src in ('ccRCC', 'pRCC'):\n","            for pid, lst in roi_maps[src].items():\n","                roi_map[pid] += [r for r in lst if r[4] == 'not_tumor']\n","    else:\n","        roi_map = roi_maps[subtype]\n","\n","    # iterate splits\n","    for sp in splits:\n","        need = tgt[sp]\n","        if need == 0: continue\n","        pids = [p for p in roi_map if patient_split.get(p) == sp]\n","        if not pids: raise RuntimeError(f\"No patients for {subtype}-{sp}\")\n","        pbar = tqdm(total=need, desc=f'{subtype}-{sp}', unit='patch', leave=False)\n","        collected = 0\n","        while collected < need:\n","            pid   = rng.choice(pids)\n","            cand  = [r for r in roi_map[pid] if r[4] == subtype] if subtype!='not_tumor' else roi_map[pid]\n","            if not cand: continue\n","            kind, p1, p2, box, _ = rng.choice(cand)\n","            try:\n","                slide = OpenSlide(str(p1))\n","            except Exception: continue\n","\n","            if kind == 'xml':\n","                minx, maxx, miny, maxy = box\n","                if maxx-minx < MGR.patch_size or maxy-miny < MGR.patch_size:\n","                    continue\n","                x = rng.randint(minx, maxx - MGR.patch_size)\n","                y = rng.randint(miny, maxy - MGR.patch_size)\n","                rec = {'wsi_path': str(p1), 'xml_path': str(p2), 'roi_file': None}\n","            else:\n","                W, H = slide.dimensions\n","                if W < MGR.patch_size or H < MGR.patch_size: continue\n","                x = rng.randint(0, W - MGR.patch_size)\n","                y = rng.randint(0, H - MGR.patch_size)\n","                rec = {'wsi_path': None, 'xml_path': None, 'roi_file': str(p1)}\n","\n","            rows.append({\n","                'subtype':     subtype,\n","                'patient_id':  pid,\n","                **rec,\n","                'x': x, 'y': y,\n","                'patch_size': MGR.patch_size,\n","                'split': sp\n","            })\n","            collected += 1\n","            pbar.update(1)\n","        pbar.close()"],"metadata":{"id":"3H4GnQQOsGnU","executionInfo":{"status":"aborted","timestamp":1751065676118,"user_tz":-120,"elapsed":64,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 8 ‚Äì build patch_df, preview, versioning & save in dataset_{uuid8} folder\n","from datetime import datetime\n","import uuid\n","from IPython.display import display\n","\n","# --- 1) Build finale ----------------------------------------------\n","patch_df = pd.DataFrame(rows)\n","patch_df['split'] = patch_df['patient_id'].map(patient_split).fillna('train')\n","\n","# --- 2) Preview quick check ---------------------------------------\n","print(f\"‚ÑπÔ∏è Shape: {patch_df.shape}\")\n","print(\"‚ÑπÔ∏è Columns:\")\n","print(patch_df.dtypes)\n","display(patch_df.head(5))\n","\n","# --- 3) Prepare naming & paths ------------------------------------\n","uuid8      = uuid.uuid4().hex[:8]\n","now        = datetime.now().strftime('%Y%m%d_%H%M')\n","stage      = MGR.stage\n","\n","# Filename senza classi\n","file_name  = \"__\".join([\n","    stage,\n","    f\"patch{MGR.patch_size}_stride{MGR.stride}\",\n","    f\"{MGR.patches_per_cls}patches\",\n","    f\"seed{MGR.random_seed}\",\n","    uuid8\n","]) + \".parquet\"\n","\n","# Cartella dedicata dataset_{uuid8}\n","proc_dir     = PATHS.project_root / 'data' / 'processed'\n","dataset_id = f\"dataset_{uuid8}\"\n","dataset_dir  = proc_dir / dataset_id\n","dataset_dir.mkdir(parents=True, exist_ok=True)\n","\n","patch_path   = dataset_dir / file_name\n","desc_path    = dataset_dir / f\"version_descriptor__{uuid8}.md\"\n","registry_csv = proc_dir / 'dataset_registry.csv'\n","\n","# --- 4) Save parquet ----------------------------------------------\n","patch_df.to_parquet(patch_path, index=False)\n","print(f\"‚úÖ patch_df saved ‚Üí {patch_path}\")\n","\n","# --- 5) Write version_descriptor with classes inside ---------------\n","descriptor = [\n","    \"# Dataset Version Descriptor\",\n","    \"\",\n","    f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n","    f\"**Stage:** {stage}\",\n","    f\"**Classes:** {', '.join(sorted(patch_df['subtype'].unique()))}\",\n","    f\"**Patch size:** {MGR.patch_size}\",\n","    f\"**Stride:** {MGR.stride}\",\n","    f\"**Patches per class:** {MGR.patches_per_cls}\",\n","    f\"**Random seed:** {MGR.random_seed}\",\n","    \"\",\n","    \"**Files generated:**\",\n","    f\"- {patch_path.name}\",\n","]\n","desc_path.write_text(\"\\n\".join(descriptor))\n","print(f\"üìù version_descriptor written ‚Üí {desc_path}\")\n","\n","# --- 6) Update global registry (no classes field) -----------------\n","import csv\n","registry_exists = registry_csv.exists()\n","with registry_csv.open('a', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    if not registry_exists:\n","        writer.writerow(['timestamp','stage','patch_size','stride','patches/class','seed','dataset_id'])\n","    writer.writerow([now, stage, MGR.patch_size, MGR.stride, MGR.patches_per_cls, MGR.random_seed, dataset_id])\n","print(f\"üìë dataset_registry updated ‚Üí {registry_csv}\")\n"],"metadata":{"id":"qwslwOj84nSq","executionInfo":{"status":"aborted","timestamp":1751065676125,"user_tz":-120,"elapsed":67,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 9 ‚Äì load parquet from dataset_{uuid8}, preview & sanity check\n","loaded_df = pd.read_parquet(patch_path)  # patch_path gi√† punta a dataset_{uuid8}/file.parquet\n","\n","print(f\"‚ÑπÔ∏è Loaded shape: {loaded_df.shape}\")\n","print(\"‚ÑπÔ∏è Columns/dtypes:\")\n","print(loaded_df.dtypes)\n","display(loaded_df)\n","print(\"\\n‚ÑπÔ∏è Null counts:\")\n","display(loaded_df.isnull().sum())\n","\n","print(\"\\n‚úÖ Patch count per subtype/split:\")\n","print(loaded_df.groupby(['subtype','split']).size())\n","\n","print(\"\\n‚úÖ Unique patients per subtype:\")\n","for sub, pats in loaded_df.groupby('subtype')['patient_id'].unique().items():\n","    print(f\"  ‚Ä¢ {sub:7}: {len(pats)} ‚Üí {list(pats)}\")\n"],"metadata":{"id":"YnarObjPMNrm","executionInfo":{"status":"aborted","timestamp":1751065676135,"user_tz":-120,"elapsed":75,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell X ‚Äì Numero di patch per paziente / classe / sorgente (WSI o ROI)\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","import pandas as pd\n","\n","# 0) Assicuriamoci di avere le colonne necessarie\n","# patch_df deve contenere: patient_id, subtype, wsi_path, roi_file\n","assert {'patient_id','subtype'}.issubset(patch_df.columns)\n","\n","# 1) Determina source_name (WSI stem oppure nome completo ROI)\n","def get_source_name(row):\n","    if pd.notna(row.get('wsi_path')):\n","        return Path(row['wsi_path']).stem\n","    if pd.notna(row.get('roi_file')):\n","        return Path(row['roi_file']).name\n","    return 'UNKNOWN'\n","\n","patch_df['source_name'] = patch_df.apply(get_source_name, axis=1)\n","\n","# 2) Raggruppa per patient_id, subtype, source_name e conta\n","counts = (\n","    patch_df\n","    .groupby(['patient_id','subtype','source_name'])\n","    .size()\n","    .reset_index(name='patch_count')\n","    .sort_values(['patient_id','subtype','source_name'])\n",")\n","\n","# 3) Crea etichetta per y-axis\n","counts['label'] = counts['subtype'] + ' | ' + counts['patient_id'] + ' / ' + counts['source_name']\n","\n","# 4) Plot\n","plt.figure(figsize=(14, max(4, len(counts)*0.35)))\n","sns.barplot(data=counts, y='label', x='patch_count', palette='viridis')\n","\n","plt.title(\"üìä Numero di patch per classe / paziente / sorgente\")\n","plt.xlabel(\"Numero di patch\")\n","plt.ylabel(\"Classe | Paziente / Sorgente\")\n","plt.grid(axis='x', linestyle='--', alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"dSBLlbh1571l","executionInfo":{"status":"aborted","timestamp":1751065676143,"user_tz":-120,"elapsed":79,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 10 ‚Äì Identify duplicated patient_ids and their subtypes\n","import pandas as pd\n","\n","# loaded_df √® il DataFrame caricato in Cell 9\n","# 1) Quanti patient_id totali sono duplicati (conteggio righe duplicate)\n","total_duplicates = loaded_df['patient_id'].duplicated().sum()\n","print(f\"üî¢ Total duplicate rows by patient_id: {total_duplicates}\")\n","\n","# 2) Quali patient_id compaiono in pi√π di una riga\n","dupe_ids = loaded_df['patient_id'][loaded_df['patient_id'].duplicated(keep=False)].unique()\n","print(f\"üîç Unique patient_ids that appear multiple times: {len(dupe_ids)}\\n{list(dupe_ids)}\\n\")\n","\n","# 3) Trovo quelli che sono presenti in pi√π di un subtype\n","multi = (loaded_df\n","         .groupby('patient_id')['subtype']\n","         .nunique()\n","         .loc[lambda x: x > 1]\n","         .index\n","        )\n","\n","print(\"‚ö†Ô∏è patient_id present in multiple subtypes:\")\n","for pid in sorted(multi):\n","    classes = loaded_df.loc[loaded_df['patient_id']==pid, 'subtype'].unique().tolist()\n","    print(f\"  ‚Ä¢ {pid}: {classes}\")\n"],"metadata":{"id":"ewCGif6nq_nE","executionInfo":{"status":"aborted","timestamp":1751065676152,"user_tz":-120,"elapsed":18293,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 11 ‚Äì Elenca le WSI utilizzate con dimensione file e dimensioni pixel\n","from pathlib import Path\n","import os\n","from openslide import OpenSlide\n","\n","# 1) Prendi i percorsi unici delle WSI dal DataFrame\n","wsi_paths = sorted({p for p in patch_df['wsi_path'].dropna()})\n","\n","print(f\"üî¢ Unique WSI slides used: {len(wsi_paths)}\\n\")\n","\n","# 2) Stampa tabella con path, file size (bytes), width e height (px)\n","print(f\"{'WSI Path':80} {'Size (bytes)':>12} {'Width':>6} {'Height':>6}\")\n","for p in wsi_paths:\n","    path = Path(p)\n","    # dimensione file\n","    try:\n","        size_bytes = path.stat().st_size\n","    except FileNotFoundError:\n","        size_bytes = None\n","\n","    # leggi solo l'header per ottenere dimensioni\n","    try:\n","        slide = OpenSlide(str(path))\n","        width, height = slide.dimensions\n","        slide.close()\n","    except Exception:\n","        width = height = None\n","\n","    print(f\"{str(path):80} {str(size_bytes):>12} {str(width):>6} {str(height):>6}\")\n"],"metadata":{"id":"UmrjoS3Z1z2r","executionInfo":{"status":"aborted","timestamp":1751065676158,"user_tz":-120,"elapsed":18296,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]}]}