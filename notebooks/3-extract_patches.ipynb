{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew3A4knQWElv"
   },
   "source": [
    "### 🧪 Extracting Patches into WebDataset Format \n",
    "\n",
    "This notebook extracts image patches from RCC WSIs based on coordinates provided in a `.parquet` file generated in Stage 2. Patches are saved in `WebDataset` format (`.tar`) for efficient downstream training. The output uses **flat, keyless JPEG entries** (no `__key__`), ensuring full sample accessibility during downstream loading.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "\n",
    "1. **Install required dependencies**, including `openslide`, `webdataset`, and compression tools.\n",
    "2. **Load configuration from YAML**, resolving environment paths and parameters like `patch_size`, `random_seed`, and active stage.\n",
    "3. **Locate and load the latest dataset folder**, automatically or manually, and read the `patch_df.parquet` file.\n",
    "4. **Initialize WebDataset shard writers** for each split (`train`, `val`, `test`) with a configurable shard size.\n",
    "5. **Iterate over unique sources (WSI and ROI)**, copy them locally, open them with `OpenSlide`, and extract patches as specified in `patch_df`.\n",
    "6. **Filter out non-informative patches**, using grayscale standard deviation as a quality check.\n",
    "7. **Save valid patches** into `.tar` shards as JPEGs, without keys or metadata — each image gets a flat incremental name (`000000.jpg`, `000001.jpg`, …).\n",
    "8. **Log errors and summary**, writing a JSON error report if needed.\n",
    "9. **Close shard writers and compute statistics**, including the number of patches per split and per subtype.\n",
    "10. **Display visual summary** with sampled thumbnails for each class/split.\n",
    "\n",
    "---\n",
    "\n",
    "### 📄 `3-extract_patches.ipynb` – Documentation Table\n",
    "\n",
    "| #  | **Section (Markdown Title)**  | **Main Content (Documented Classes/Functions)**                                                | **Output**                 |\n",
    "| -- | ----------------------------- | ---------------------------------------------------------------------------------------------- | -------------------------- |\n",
    "| 1  | **Install & Setup**           | System and pip installs for OpenSlide and WebDataset                                           | system packages            |\n",
    "| 2  | **Load Configuration**        | `ConfigLoader`, `PathResolver` resolve paths and environment from `preprocessing.yaml`         | `cfg`, `PATHS`             |\n",
    "| 3  | **Select Dataset**            | Load latest or manually defined `dataset_{uuid}` folder and load `.parquet` file               | `patch_df`, `unique_wsis`  |\n",
    "| 4  | **Shard Initialization**      | Create empty `ShardWriter` objects for `train`, `val`, `test`, one per split                   | `writers` dictionary       |\n",
    "| 5  | **Informative Patch Check**   | `is_patch_informative`: filters out blank/black patches                                        | utility function           |\n",
    "| 6  | **Patch Extraction Loop**     | Iterate over sources, copy WSI to local SSD, open, extract and filter patches, write to `.tar` | WebDataset shards (`.tar`) |\n",
    "| 7  | **Error Handling**            | Logs missing/corrupted slides or failed reads into a `.json` error file                        | `extract_errors.json`      |\n",
    "| 8  | **Close Writers & Summary**   | Closes all writers, prints total saved patches and logs if errors occurred                     | summary stats              |\n",
    "| 9  | **Shard Statistics Overview** | Prints stats per shard: size, patch count, number of valid shards                              | per-split `.tar` summary   |\n",
    "| 10 | **Per-Class Stats**           | Stats for each split (`train`, `val`, `test`) with per-subtype patch counts                    | printed overview           |\n",
    "| 11 | **Patch Thumbnails Preview**  | Randomly loads JPEGs from shards and shows a grid of sampled patches per class/split           | matplotlib image grid      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1750362510031,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "x2Xe82Er-4pb",
    "outputId": "894e605b-e394-49be-b4dc-dcc232d52a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11101,
     "status": "ok",
     "timestamp": 1750362521131,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "dTDGIU6dA-q_",
    "outputId": "fc6d705a-cc47-4399-a03d-51612da8b70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] Installing OpenSlide system libs…\n",
      "[SETUP] Installing Python wheels…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=\"pip install -qq --upgrade openslide-python openslide-bin webdataset tqdm matplotlib zarr 'numcodecs<0.8.0'\", returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess, sys, json, time, shutil, random, yaml, io, os, glob, tarfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# System packages\n",
    "print(\"[SETUP] Installing OpenSlide system libs…\")\n",
    "subprocess.run(\"apt-get -qq update\", shell=True, check=True)\n",
    "subprocess.run(\"apt-get -qq install -y openslide-tools libopenslide-dev\", shell=True, check=True)\n",
    "\n",
    "# Python wheels\n",
    "print(\"[SETUP] Installing Python wheels…\")\n",
    "subprocess.run(\n",
    "    \"pip install -qq --upgrade openslide-python openslide-bin \"\n",
    "    \"webdataset tqdm matplotlib zarr 'numcodecs<0.8.0'\",\n",
    "    shell=True,\n",
    "    check=True,\n",
    ")\n",
    "\n",
    "# Clean stray ~orch dirs (PyTorch leftovers if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1750362521180,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "-IFWU68o6BWD",
    "outputId": "e154e7f5-111f-4a52-fb6d-ac8ad573a3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CFG] Base root       : /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\n",
      "[CFG] RAW slides root : /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/raw/RCC_WSIs\n",
      "[CFG] Stage           : training\n",
      "[CFG] Patch size      : 512\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 – Configuration & Paths (new YAML schema – no resumable logic)\n",
    "\n",
    "YAML_PATH = Path(\"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/config/preprocessing.yaml\")\n",
    "if not YAML_PATH.exists():\n",
    "    sys.exit(f\"[FATAL] YAML config not found → {YAML_PATH}\")\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 1) ConfigLoader: resolves ${RESOLVED_BASE_DIR} and ${base.*}      #\n",
    "# ------------------------------------------------------------------#\n",
    "class ConfigLoader:\n",
    "    def __init__(self, path: Path):\n",
    "        raw = yaml.safe_load(path.read_text())\n",
    "        self.base_root = self._select_env(raw['environment'])\n",
    "        self.cfg = self._substitute(raw)\n",
    "\n",
    "    def _select_env(self, env):\n",
    "        colab, local = Path(env['colab']), Path(env['local'])\n",
    "        if colab.exists(): return colab\n",
    "        if local.exists(): return local\n",
    "        sys.exit(\"[FATAL] No environment path found in YAML\")\n",
    "\n",
    "    def _substitute(self, cfg_raw):\n",
    "        import copy\n",
    "        cfg = copy.deepcopy(cfg_raw)\n",
    "        # resolve base section\n",
    "        base = {k: v.replace('${RESOLVED_BASE_DIR}', str(self.base_root))\n",
    "                for k, v in cfg['base'].items()}\n",
    "        cfg['base'] = base\n",
    "        # placeholder map\n",
    "        ph = {'${RESOLVED_BASE_DIR}': str(self.base_root),\n",
    "              **{f'${{base.{k}}}': v for k, v in base.items()}}\n",
    "        def repl(o):\n",
    "            if isinstance(o, str):\n",
    "                for k, v in ph.items(): o = o.replace(k, v)\n",
    "            elif isinstance(o, dict):\n",
    "                o = {k: repl(v) for k, v in o.items()}\n",
    "            elif isinstance(o, list):\n",
    "                o = [repl(v) for v in o]\n",
    "            return o\n",
    "        for sec in ['data_paths', 'stage_overrides', 'patching_defaults', 'split_by_patient']:\n",
    "            cfg[sec] = repl(cfg[sec])\n",
    "        return cfg\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 2) PathResolver: maps data_paths → dataraw_root                   #\n",
    "# ------------------------------------------------------------------#\n",
    "class PathResolver:\n",
    "    def __init__(self, cfg):\n",
    "        b, dp = cfg['base'], cfg['data_paths']\n",
    "        self.project_root  = Path(b['project_root'])\n",
    "        self.raw_root      = Path(b['dataraw_root'])      # <- RAW slides\n",
    "        self.metadata_root = Path(b['metadata_root'])\n",
    "        self.patch_df_dir  = self.project_root / \"data\" / \"processed\"  # where the parquet will be saved\n",
    "\n",
    "        def raw(p):  # convert path under data_root ⇒ dataraw_root\n",
    "            return self.raw_root / Path(p).relative_to(Path(b['data_root']))\n",
    "\n",
    "        # ccRCC / pRCC + pre\n",
    "        self.ccrcc_wsi      = raw(dp['ccRCC']['wsi'])\n",
    "        self.pre_ccrcc_wsi  = raw(dp['ccRCC']['pre']['wsi'])\n",
    "        self.prcc_wsi       = raw(dp['pRCC']['wsi'])\n",
    "        self.pre_prcc_wsi   = raw(dp['pRCC']['pre']['wsi'])\n",
    "        # CHROMO / ONCO\n",
    "        self.chromo_wsi     = raw(dp['CHROMO']['wsi'])\n",
    "        self.onco_wsi       = raw(dp['ONCO']['wsi'])\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 3) Initialization                                                 #\n",
    "# ------------------------------------------------------------------#\n",
    "CFG   = ConfigLoader(YAML_PATH).cfg\n",
    "PATHS = PathResolver(CFG)\n",
    "\n",
    "# Select stage (debug / training) from YAML\n",
    "stage_key   = 'debug' if CFG['stage_overrides']['debug']['sampling']['enabled'] else 'training'\n",
    "stage_cfg   = CFG['stage_overrides'][stage_key]\n",
    "\n",
    "PATCH_SIZE  = stage_cfg['patching']['patch_size']\n",
    "RANDOM_SEED = stage_cfg['patching']['random_seed']\n",
    "SHARD_SIZE  = 5_000\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"[CFG] Base root       : {PATHS.project_root}\")\n",
    "print(f\"[CFG] RAW slides root : {PATHS.raw_root}\")\n",
    "print(f\"[CFG] Stage           : {stage_key}\")\n",
    "print(f\"[CFG] Patch size      : {PATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750362521189,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "ftlwtCWnk2X6",
    "outputId": "d9c86206-579f-4803-c1ad-d70478604d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Automatically selected latest dataset directory: dataset_7b24514c\n",
      "[DATA] Loading patch_df from: training__patch512_stride256__1000patches__seed42__7b24514c.parquet\n",
      "[DATA] patch_df rows: 5,000\n",
      "[DATA] unique sources to process: 82\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 – Locate the latest dataset_{uuid} and load its parquet\n",
    "import os, glob, pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set to True to automatically select the most recently modified dataset directory.\n",
    "# Set to False to manually specify the directory name below.\n",
    "AUTO_SELECT_LATEST_DATASET = True  # Set to True or False\n",
    "\n",
    "# If AUTO_SELECT_LATEST_DATASET is False, specify the directory name here.\n",
    "MANUAL_DATASET_NAME = \"dataset_9f30917e\"\n",
    "# --------------------\n",
    "\n",
    "# 1) Processed dataset root folder\n",
    "proc_dir = PATHS.project_root / \"data\" / \"processed\"\n",
    "\n",
    "# 2) Select dataset directory based on configuration\n",
    "if AUTO_SELECT_LATEST_DATASET:\n",
    "    ds_dirs = [p for p in proc_dir.glob(\"dataset_*\") if p.is_dir()]\n",
    "    if not ds_dirs:\n",
    "        sys.exit(\"[FATAL] No dataset_{uuid} directory found – run 2_generate_patch_df first\")\n",
    "    ds_dirs.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    dataset_dir = ds_dirs[0]\n",
    "    print(f\"[DATA] Automatically selected latest dataset directory: {dataset_dir.name}\")\n",
    "else:\n",
    "    dataset_dir = proc_dir / MANUAL_DATASET_NAME\n",
    "    if not dataset_dir.is_dir():\n",
    "        sys.exit(f\"[FATAL] Manually specified dataset directory not found: {dataset_dir}\")\n",
    "    print(f\"[DATA] Using manually specified dataset directory: {dataset_dir.name}\")\n",
    "\n",
    "# 3) Inside it, locate all .parquet files and sort them by last modification time\n",
    "parquet_files = list(dataset_dir.glob(\"*.parquet\"))\n",
    "if not parquet_files:\n",
    "    sys.exit(f\"[FATAL] No .parquet in {dataset_dir}\")\n",
    "\n",
    "parquet_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "PATCH_DF_PATH = parquet_files[0]\n",
    "print(f\"[DATA] Loading patch_df from: {PATCH_DF_PATH.name}\")\n",
    "\n",
    "# 4) Load the DataFrame\n",
    "patch_df = pd.read_parquet(PATCH_DF_PATH)\n",
    "print(f\"[DATA] patch_df rows: {len(patch_df):,}\")\n",
    "\n",
    "# 5) Build the list of unique sources (WSI + ROI files)\n",
    "unique_wsis = sorted(\n",
    "    set(\n",
    "        patch_df['wsi_path'].dropna().tolist() +\n",
    "        patch_df['roi_file'].dropna().tolist()\n",
    "    )\n",
    ")\n",
    "print(f\"[DATA] unique sources to process: {len(unique_wsis):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1750362521284,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "9cKFlK_4k54H",
    "outputId": "39d2221e-f7e5-4aec-a4ee-c64bb1360857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Checking WebDataset shards at: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_7b24514c/webdataset\n",
      "# writing /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_7b24514c/webdataset/train/patches-0000.tar 0 0.0 GB 0\n",
      "# writing /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_7b24514c/webdataset/val/patches-0000.tar 0 0.0 GB 0\n",
      "# writing /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_7b24514c/webdataset/test/patches-0000.tar 0 0.0 GB 0\n",
      "[INFO] Shard writers ready (empty).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 – Safe Shard Writers Initialization (train/val/test)\n",
    "import webdataset as wds\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Final path: {project_root}/data/processed/dataset_{uuid}/webdataset/{split}/patches-0000.tar\n",
    "WDATASET_DIR = dataset_dir / \"webdataset\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "print(f\"[INFO] Checking WebDataset shards at: {WDATASET_DIR}\")\n",
    "\n",
    "# 1. Check if non-empty shards already exist\n",
    "shards_found = False\n",
    "for sp in splits:\n",
    "    split_dir = WDATASET_DIR / sp\n",
    "    existing = list(split_dir.glob(\"patches-*.tar\"))\n",
    "    for tar in existing:\n",
    "        if tar.stat().st_size > 0:\n",
    "            print(f\"❌ Found existing shard: {tar} ({tar.stat().st_size/1024**2:.2f} MB)\")\n",
    "            shards_found = True\n",
    "\n",
    "if shards_found:\n",
    "    print(\"\\n[ABORT] At least one shard already exists and is non-empty.\")\n",
    "    print(\"        Rename or delete existing files before re-running this cell.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 2. Create empty directories for each split if no shards were found\n",
    "for sp in splits:\n",
    "    (WDATASET_DIR / sp).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. Initialize shard writers\n",
    "writers = {\n",
    "    sp: wds.ShardWriter(\n",
    "        str(WDATASET_DIR / sp / \"patches-%04d.tar\"),\n",
    "        maxcount=SHARD_SIZE\n",
    "    )\n",
    "    for sp in splits\n",
    "}\n",
    "\n",
    "print(\"[INFO] Shard writers ready (empty).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4aklxgz9UuD"
   },
   "outputs": [],
   "source": [
    "# Cell 3 bis – Helper: discard “black” / non-informative patches\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def is_patch_informative(pil_img: Image.Image, thresh: int = 10) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the grayscale standard deviation is > thresh.\n",
    "    Used to avoid saving black or low-information patches.\n",
    "    \"\"\"\n",
    "    gray = pil_img.convert(\"L\")\n",
    "    return np.array(gray).std() > thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1750362521326,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "icP-C8czEM6q",
    "outputId": "6a42f193-bd3a-4135-9b03-fd34d5162df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches-0000.tar          →   0.00 MB\n",
      "patches-0000.tar          →   0.00 MB\n",
      "patches-0000.tar          →   0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell X – Print shard file sizes for each split\n",
    "import tarfile, os, glob\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    tar_paths = sorted(glob.glob(str(WDATASET_DIR / split / \"*.tar\")))\n",
    "    for tp in tar_paths:\n",
    "        size_mb = os.path.getsize(tp) / (1024**2)\n",
    "        print(f\"{Path(tp).name:<25} → {size_mb:6.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fcb7a76e32414075849842761fb60204",
      "dd34b63d1b69412399cdcae9e3f1c284",
      "86213419e7d7456e9fdbebd6272a6e06",
      "bba95f68e79d42f398d4edf60ef695da",
      "13f520ee00fb403c835b94a91986097f",
      "5ac9c39c11224d99be6b112be427473b",
      "5f795e6f0b494703aaf6536092cbcb9b",
      "cada00fdfdbe46109575ab6f7f660075",
      "a299536f92ba46039f0d748ab2984019",
      "a4ebac758079444fb7d2fa0a39c44b79",
      "d28b2ea7decd4043910b642a293d285f"
     ]
    },
    "executionInfo": {
     "elapsed": 2274397,
     "status": "ok",
     "timestamp": 1750364795725,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "GYb0AOoNk68q",
    "outputId": "1e0e96c0-5894-40af-c807-b834a25b0296"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb7a76e32414075849842761fb60204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing source:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processing source: 1.svs\n",
      "  [OK] saved 92 patches from 1.svs\n",
      "\n",
      "[INFO] Processing source: 10.svs\n",
      "  [OK] saved 65 patches from 10.svs\n",
      "\n",
      "[INFO] Processing source: 11.svs\n",
      "  [OK] saved 65 patches from 11.svs\n",
      "\n",
      "[INFO] Processing source: 12.svs\n",
      "  [OK] saved 94 patches from 12.svs\n",
      "\n",
      "[INFO] Processing source: 13.tif\n",
      "  [OK] saved 94 patches from 13.tif\n",
      "\n",
      "[INFO] Processing source: 2.svs\n",
      "  [OK] saved 102 patches from 2.svs\n",
      "\n",
      "[INFO] Processing source: 3.svs\n",
      "  [OK] saved 41 patches from 3.svs\n",
      "\n",
      "[INFO] Processing source: 4.svs\n",
      "  [OK] saved 54 patches from 4.svs\n",
      "\n",
      "[INFO] Processing source: 5.svs\n",
      "  [OK] saved 28 patches from 5.svs\n",
      "\n",
      "[INFO] Processing source: 6.svs\n",
      "  [OK] saved 33 patches from 6.svs\n",
      "\n",
      "[INFO] Processing source: 7.svs\n",
      "  [OK] saved 42 patches from 7.svs\n",
      "\n",
      "[INFO] Processing source: 8.svs\n",
      "  [OK] saved 214 patches from 8.svs\n",
      "\n",
      "[INFO] Processing source: 9.svs\n",
      "  [OK] saved 66 patches from 9.svs\n",
      "\n",
      "[INFO] Processing source: 1.svs\n",
      "  [OK] saved 72 patches from 1.svs\n",
      "\n",
      "[INFO] Processing source: 10.tif\n",
      "  [OK] saved 36 patches from 10.tif\n",
      "\n",
      "[INFO] Processing source: 11.tif\n",
      "  [OK] saved 32 patches from 11.tif\n",
      "\n",
      "[INFO] Processing source: 12.tif\n",
      "  [OK] saved 36 patches from 12.tif\n",
      "\n",
      "[INFO] Processing source: 13.tif\n",
      "  [OK] saved 33 patches from 13.tif\n",
      "\n",
      "[INFO] Processing source: 14.tif\n",
      "  [OK] saved 18 patches from 14.tif\n",
      "\n",
      "[INFO] Processing source: 15.tif\n",
      "  [OK] saved 34 patches from 15.tif\n",
      "\n",
      "[INFO] Processing source: 16.tif\n",
      "  [OK] saved 22 patches from 16.tif\n",
      "\n",
      "[INFO] Processing source: 17.tif\n",
      "  [OK] saved 22 patches from 17.tif\n",
      "\n",
      "[INFO] Processing source: 18.tif\n",
      "  [OK] saved 32 patches from 18.tif\n",
      "\n",
      "[INFO] Processing source: 19.tif\n",
      "  [OK] saved 28 patches from 19.tif\n",
      "\n",
      "[INFO] Processing source: 2.svs\n",
      "  [OK] saved 69 patches from 2.svs\n",
      "\n",
      "[INFO] Processing source: 20.tif\n",
      "  [OK] saved 27 patches from 20.tif\n",
      "\n",
      "[INFO] Processing source: 21.tif\n",
      "  [OK] saved 20 patches from 21.tif\n",
      "\n",
      "[INFO] Processing source: 22.tif\n",
      "  [OK] saved 55 patches from 22.tif\n",
      "\n",
      "[INFO] Processing source: 23.tif\n",
      "  [OK] saved 48 patches from 23.tif\n",
      "\n",
      "[INFO] Processing source: 24.tif\n",
      "  [OK] saved 52 patches from 24.tif\n",
      "\n",
      "[INFO] Processing source: 25.tif\n",
      "  [OK] saved 44 patches from 25.tif\n",
      "\n",
      "[INFO] Processing source: 3.svs\n",
      "  [OK] saved 59 patches from 3.svs\n",
      "\n",
      "[INFO] Processing source: 4.svs\n",
      "  [OK] saved 42 patches from 4.svs\n",
      "\n",
      "[INFO] Processing source: 5.svs\n",
      "  [OK] saved 50 patches from 5.svs\n",
      "\n",
      "[INFO] Processing source: 6.svs\n",
      "  [OK] saved 59 patches from 6.svs\n",
      "\n",
      "[INFO] Processing source: 7.svs\n",
      "  [OK] saved 43 patches from 7.svs\n",
      "\n",
      "[INFO] Processing source: 8.tif\n",
      "  [OK] saved 33 patches from 8.tif\n",
      "\n",
      "[INFO] Processing source: 9.tif\n",
      "  [OK] saved 31 patches from 9.tif\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A1-1.ccRCC.scn\n",
      "  [OK] saved 21 patches from HP19.10064.A1-1.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A1.ccRCC.scn\n",
      "  [OK] saved 15 patches from HP19.10064.A1.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A10.ccRCC.scn\n",
      "  [OK] saved 43 patches from HP19.10064.A10.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A13.ccRCC.scn\n",
      "  [OK] saved 60 patches from HP19.10064.A13.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A14.ccRCC.scn\n",
      "  [OK] saved 6 patches from HP19.10064.A14.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A16.ccRCC.scn\n",
      "  [OK] saved 47 patches from HP19.10064.A16.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A6.ccRCC.scn\n",
      "  [OK] saved 53 patches from HP19.10064.A6.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.10064.A7.ccRCC.scn\n",
      "  [OK] saved 36 patches from HP19.10064.A7.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.4372.A3.ccRCC.scn\n",
      "  [OK] saved 139 patches from HP19.4372.A3.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.4372.A4.ccRCC.scn\n",
      "  [OK] saved 41 patches from HP19.4372.A4.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.4372.A6.ccRCC.scn\n",
      "  [OK] saved 70 patches from HP19.4372.A6.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.4372.A7.ccRCC.scn\n",
      "  [OK] saved 43 patches from HP19.4372.A7.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.4372.A8.ccRCC.scn\n",
      "  [OK] saved 31 patches from HP19.4372.A8.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.5524.A2.ccRCC.scn\n",
      "  [OK] saved 73 patches from HP19.5524.A2.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.5524.A3.ccRCC.scn\n",
      "  [OK] saved 117 patches from HP19.5524.A3.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.5524.A4.ccRCC.scn\n",
      "  [OK] saved 81 patches from HP19.5524.A4.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A1.ccRCC.scn\n",
      "  [OK] saved 12 patches from HP19.754.A1.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A3.ccRCC.scn\n",
      "  [OK] saved 33 patches from HP19.754.A3.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A4.ccRCC.scn\n",
      "  [OK] saved 25 patches from HP19.754.A4.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A5.ccRCC.scn\n",
      "  [OK] saved 50 patches from HP19.754.A5.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A6.ccRCC.scn\n",
      "  [OK] saved 82 patches from HP19.754.A6.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A7.ccRCC.scn\n",
      "  [OK] saved 40 patches from HP19.754.A7.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A8.ccRCC.scn\n",
      "  [OK] saved 21 patches from HP19.754.A8.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.754.A9.ccRCC.scn\n",
      "  [OK] saved 22 patches from HP19.754.A9.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.8394.A1.ccRCC.scn\n",
      "  [OK] saved 230 patches from HP19.8394.A1.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.8394.A3.ccRCC.scn\n",
      "  [OK] saved 62 patches from HP19.8394.A3.ccRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP17.11714.A6-1.pRCC.scn\n",
      "  [OK] saved 73 patches from HP17.11714.A6-1.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP17.11714.A6.pRCC.scn\n",
      "  [OK] saved 84 patches from HP17.11714.A6.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP17.11714.A8.pRCC.scn\n",
      "  [OK] saved 121 patches from HP17.11714.A8.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP17.7980.A2.pRCC.scn\n",
      "  [OK] saved 99 patches from HP17.7980.A2.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP17.7980.B.pRCC.scn\n",
      "  [OK] saved 109 patches from HP17.7980.B.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP17.7980.B2.pRCC.scn\n",
      "  [OK] saved 80 patches from HP17.7980.B2.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A10.pRCC.scn\n",
      "  [OK] saved 54 patches from HP18.11474.A10.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A11.pRCC.scn\n",
      "  [OK] saved 78 patches from HP18.11474.A11.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A13.pRCC.scn\n",
      "  [OK] saved 51 patches from HP18.11474.A13.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A14.pRCC.scn\n",
      "  [OK] saved 16 patches from HP18.11474.A14.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A5.pRCC.scn\n",
      "  [OK] saved 14 patches from HP18.11474.A5.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A6.pRCC.scn\n",
      "  [OK] saved 23 patches from HP18.11474.A6.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A7.pRCC.scn\n",
      "  [OK] saved 12 patches from HP18.11474.A7.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A8.pRCC.scn\n",
      "  [OK] saved 13 patches from HP18.11474.A8.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.11474.A9.pRCC.scn\n",
      "  [OK] saved 50 patches from HP18.11474.A9.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.5818.A3.pRCC.scn\n",
      "  [OK] saved 168 patches from HP18.5818.A3.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP18.5818.A5.pRCC.scn\n",
      "  [OK] saved 102 patches from HP18.5818.A5.pRCC.scn\n",
      "\n",
      "[INFO] Processing source: HP19.1773.A10.pRCC.scn\n",
      "  [OK] saved 308 patches from HP19.1773.A10.pRCC.scn\n",
      "\n",
      "✅ Patches successfully saved this run: 4,895\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 – Patch extraction (all WSI/ROI with bounding box & filtering)\n",
    "import openslide, io, shutil\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Local cache for WSI files\n",
    "WSI_LOCAL_DIR = Path(\"/content/WSI_cache\")\n",
    "WSI_LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- DEV option: limit number of patches per WSI (only in debug stage) ---\n",
    "MAX_PATCH_PER_WSI = 10 if stage_key == \"debug\" else None\n",
    "\n",
    "total_patches = 0\n",
    "error_log     = []\n",
    "\n",
    "for wsi_path in tqdm(unique_wsis, desc=\"Processing source\"):\n",
    "    src = Path(wsi_path)\n",
    "    print(f\"\\n[INFO] Processing source: {src.name}\")\n",
    "    if not src.exists():\n",
    "        print(\"  [WARN] source not found on disk – skipped\")\n",
    "        error_log.append({\"src\": wsi_path, \"error\": \"not_found\"})\n",
    "        continue\n",
    "\n",
    "    # 1) copy to local SSD\n",
    "    dst = WSI_LOCAL_DIR / src.name\n",
    "    if not dst.exists():\n",
    "        try:\n",
    "            shutil.copy(src, dst)\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] copy failed: {e}\")\n",
    "            error_log.append({\"src\": wsi_path, \"error\": f\"copy_failed: {e}\"})\n",
    "            continue\n",
    "\n",
    "    # 2) open the slide\n",
    "    try:\n",
    "        slide = openslide.OpenSlide(str(dst))\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] cannot open slide: {e}\")\n",
    "        error_log.append({\"src\": wsi_path, \"error\": f\"open_failed: {e}\"})\n",
    "        continue\n",
    "\n",
    "    W, H = slide.dimensions\n",
    "    sub_df = patch_df[\n",
    "        (patch_df[\"wsi_path\"] == wsi_path) |\n",
    "        (patch_df[\"roi_file\"]  == wsi_path)\n",
    "    ]\n",
    "\n",
    "    if sub_df.empty:\n",
    "        slide.close()\n",
    "        dst.unlink(missing_ok=True)\n",
    "        continue\n",
    "\n",
    "    # shuffle and optionally limit for debug\n",
    "    sub_df = sub_df.sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if MAX_PATCH_PER_WSI:\n",
    "        sub_df = sub_df.head(MAX_PATCH_PER_WSI)\n",
    "\n",
    "    patch_cnt = 0\n",
    "    for _, row in sub_df.iterrows():\n",
    "        x, y = int(row[\"x\"]), int(row[\"y\"])\n",
    "\n",
    "        # bounding-box check\n",
    "        if x + PATCH_SIZE > W or y + PATCH_SIZE > H:\n",
    "            continue\n",
    "\n",
    "        # read region and convert to RGB\n",
    "        try:\n",
    "            img = slide.read_region((x, y), 0, (PATCH_SIZE, PATCH_SIZE)).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # discard black / low-information patches\n",
    "        if not is_patch_informative(img):\n",
    "            continue\n",
    "\n",
    "        # write to .tar: no explicit key → WebDataset will assign flat names\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format=\"JPEG\", quality=90)\n",
    "        writers[row[\"split\"]].write({\"jpg\": buf.getvalue()})\n",
    "\n",
    "        patch_cnt     += 1\n",
    "        total_patches += 1\n",
    "\n",
    "    print(f\"  [OK] saved {patch_cnt} patches from {src.name}\")\n",
    "    slide.close()\n",
    "    dst.unlink(missing_ok=True)\n",
    "\n",
    "print(f\"\\n✅ Patches successfully saved this run: {total_patches:,}\")\n",
    "if error_log:\n",
    "    err_file = dataset_dir / \"extract_errors.json\"\n",
    "    err_file.write_text(json.dumps(error_log, indent=2))\n",
    "    print(f\"⚠️  Errors logged → {err_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1750364795816,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "s3N8jAzqk8ES",
    "outputId": "a6057f1e-e3fe-4630-cafe-af1a3f7a2d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUMMARY] WebDataset dir → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_7b24514c/webdataset\n",
      "[SUMMARY] Total patches  → 4895\n",
      "[SUMMARY] No errors 🎉\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 – Close writers & summary\n",
    "\n",
    "for w in writers.values():\n",
    "    w.close()\n",
    "\n",
    "print(f\"[SUMMARY] WebDataset dir → {WDATASET_DIR}\")\n",
    "print(f\"[SUMMARY] Total patches  → {total_patches}\")\n",
    "\n",
    "if error_log:\n",
    "    err_path = dataset_dir / \"extract_errors.json\"\n",
    "    err_path.write_text(json.dumps(error_log, indent=2))\n",
    "    print(f\"[SUMMARY] Errors logged → {err_path}\")\n",
    "else:\n",
    "    print(\"[SUMMARY] No errors 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8045,
     "status": "ok",
     "timestamp": 1750364803906,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "icYswDmWk9gJ",
    "outputId": "0cc23cc9-fcd6-4765-bb04-95bd753e1384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STATS] WebDataset overview\n",
      "• Shard files found : 3\n",
      "  - patches-0000.tar     →   963 samples,  81.35 MB\n",
      "  - patches-0000.tar     →  2945 samples, 256.21 MB\n",
      "  - patches-0000.tar     →   987 samples,  85.16 MB\n",
      "\n",
      "• Valid shards          : 3/3\n",
      "• Total samples saved   : 4,895\n",
      "\n",
      "[STATS • DETAILED] Split / subtype overview\n",
      "\n",
      "🔹 Split: train\n",
      "   • shards      : 1\n",
      "   • samples     : 2,945\n",
      "   • size (MB)   : 256.21\n",
      "   • per class   :\n",
      "       - CHROMO  : 598\n",
      "       - ONCO    : 598\n",
      "       - not     : 581\n",
      "       - ccRCC   : 582\n",
      "       - pRCC    : 586\n",
      "\n",
      "🔹 Split: val\n",
      "   • shards      : 1\n",
      "   • samples     : 987\n",
      "   • size (MB)   : 85.16\n",
      "   • per class   :\n",
      "       - CHROMO  : 198\n",
      "       - ONCO    : 200\n",
      "       - ccRCC   : 192\n",
      "       - not     : 199\n",
      "       - pRCC    : 198\n",
      "\n",
      "🔹 Split: test\n",
      "   • shards      : 1\n",
      "   • samples     : 963\n",
      "   • size (MB)   : 81.35\n",
      "   • per class   :\n",
      "       - CHROMO  : 194\n",
      "       - ONCO    : 199\n",
      "       - ccRCC   : 188\n",
      "       - not     : 186\n",
      "       - pRCC    : 196\n",
      "\n",
      "==============================\n",
      "TOTAL shards    : 3\n",
      "TOTAL samples   : 4,895\n",
      "TOTAL size (GB) : 0.41 GB\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 – WebDataset statistics: shard overview\n",
    "import os, glob, tarfile\n",
    "\n",
    "print(\"\\n[STATS] WebDataset overview\")\n",
    "\n",
    "tar_paths = sorted(glob.glob(str(WDATASET_DIR / \"*\" / \"patches-*.tar\")))\n",
    "print(f\"• Shard files found : {len(tar_paths)}\")\n",
    "\n",
    "total_samples, valid_shards = 0, 0\n",
    "\n",
    "for tp in tar_paths:\n",
    "    size_mb = os.path.getsize(tp) / (1024**2)\n",
    "    try:\n",
    "        with tarfile.open(tp, \"r\") as tar:\n",
    "            n_members  = len(tar.getmembers())\n",
    "            n_samples  = n_members // 1           # 1 file (.jpg) per sample\n",
    "            total_samples += n_samples\n",
    "            valid_shards  += 1\n",
    "            print(f\"  - {Path(tp).name:<20} → {n_samples:5d} samples, {size_mb:6.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - {Path(tp).name:<20} → ERROR ({e})\")\n",
    "\n",
    "print(f\"\\n• Valid shards          : {valid_shards}/{len(tar_paths)}\")\n",
    "print(f\"• Total samples saved   : {total_samples:,}\")\n",
    "\n",
    "# – Detailed statistics per split and class\n",
    "import os, tarfile, json\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n[STATS • DETAILED] Split / subtype overview\")\n",
    "\n",
    "split_stats     = defaultdict(lambda: {\"shards\":0, \"samples\":0, \"size_mb\":0.0,\n",
    "                                       \"per_class\": Counter()})\n",
    "total_size_mb   = 0.0\n",
    "total_samples   = 0\n",
    "total_shards    = 0\n",
    "\n",
    "for split_dir in (WDATASET_DIR / \"train\", WDATASET_DIR / \"val\", WDATASET_DIR / \"test\"):\n",
    "    split = split_dir.name\n",
    "    for tar_path in sorted(split_dir.glob(\"patches-*.tar\")):\n",
    "        size_mb = os.path.getsize(tar_path) / (1024**2)\n",
    "        try:\n",
    "            with tarfile.open(tar_path, \"r\") as tar:\n",
    "                keys = [m.name for m in tar.getmembers() if m.isfile() and m.name.endswith(\".jpg\")]\n",
    "                n_samples = len(keys)\n",
    "                # extract subtype from the saved key (format: '{subtype}_{patient}_{x}_{y}.jpg')\n",
    "                for k in keys:\n",
    "                    subtype = Path(k).stem.split(\"_\", 1)[0]\n",
    "                    split_stats[split][\"per_class\"][subtype] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️  Could not open {tar_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # aggregate stats\n",
    "        split_stats[split][\"shards\"]  += 1\n",
    "        split_stats[split][\"samples\"] += n_samples\n",
    "        split_stats[split][\"size_mb\"] += size_mb\n",
    "\n",
    "        total_shards   += 1\n",
    "        total_samples  += n_samples\n",
    "        total_size_mb  += size_mb\n",
    "\n",
    "# ── Pretty print ───────────────────────────────────────────────────────────────\n",
    "for split, info in split_stats.items():\n",
    "    print(f\"\\n🔹 Split: {split}\")\n",
    "    print(f\"   • shards      : {info['shards']}\")\n",
    "    print(f\"   • samples     : {info['samples']:,}\")\n",
    "    print(f\"   • size (MB)   : {info['size_mb']:.2f}\")\n",
    "    print(\"   • per class   :\")\n",
    "    for cls, cnt in info[\"per_class\"].items():\n",
    "        print(f\"       - {cls:8}: {cnt:,}\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"TOTAL shards    : {total_shards}\")\n",
    "print(f\"TOTAL samples   : {total_samples:,}\")\n",
    "print(f\"TOTAL size (GB) : {total_size_mb/1024:.2f} GB\")\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219,
     "output_embedded_package_id": "1r3bTJiXgG3IRLVxTNvvjQNPMWPY-F2na"
    },
    "executionInfo": {
     "elapsed": 9943,
     "status": "ok",
     "timestamp": 1750367687368,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "GNUEGGtgOX1d",
    "outputId": "08731de1-4134-4a42-9f59-62847bd5c68d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7 – Display thumbnails per class (20 patches per class) directly from .tar files on Drive\n",
    "import tarfile\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Parameters\n",
    "CLASSES     = [\"CHROMO\", \"ONCO\", \"ccRCC\", \"pRCC\", \"not_tumor\"]\n",
    "SPLITS      = [\"train\", \"val\", \"test\"]\n",
    "N_PER_CLASS = 25\n",
    "\n",
    "# 2) WebDataset directory\n",
    "WDATASET_DIR = dataset_dir / \"webdataset\"\n",
    "print(f\"[INFO] WebDataset dir: {WDATASET_DIR}\")\n",
    "\n",
    "# 3) Collect thumbnails per class\n",
    "random.seed(RANDOM_SEED)\n",
    "thumbs = {cls: [] for cls in CLASSES}\n",
    "\n",
    "for split in SPLITS:\n",
    "    for tar_path in sorted((WDATASET_DIR / split).glob(\"patches-*.tar\")):\n",
    "        with tarfile.open(tar_path, \"r\") as tar:\n",
    "            members = [m for m in tar.getmembers() if m.isfile() and m.name.endswith(\".jpg\")]\n",
    "            random.shuffle(members)\n",
    "\n",
    "            for m in members:\n",
    "                subtype = Path(m.name).stem.rsplit(\"_\", 3)[0]\n",
    "                if subtype in thumbs and len(thumbs[subtype]) < N_PER_CLASS:\n",
    "                    img_data = tar.extractfile(m).read()\n",
    "                    thumbs[subtype].append((Image.open(BytesIO(img_data)), m.name))  # add file name\n",
    "\n",
    "            if all(len(thumbs[c]) >= N_PER_CLASS for c in CLASSES):\n",
    "                break\n",
    "    if all(len(thumbs[c]) >= N_PER_CLASS for c in CLASSES):\n",
    "        break\n",
    "\n",
    "# 4) Grid: 1 row per class, N columns\n",
    "n_rows, n_cols = len(CLASSES), N_PER_CLASS\n",
    "fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                         figsize=(n_cols * 2.2, max(2, n_rows) * 2.2),\n",
    "                         squeeze=False)\n",
    "\n",
    "# 5) Show each thumbnail\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    for j in range(n_cols):\n",
    "        ax = axes[i, j]\n",
    "        ax.axis(\"off\")\n",
    "        if j < len(thumbs[cls]):\n",
    "            img, fname = thumbs[cls][j]\n",
    "            ax.imshow(img)\n",
    "            if j == 0:\n",
    "                ax.set_title(cls, loc=\"left\", fontsize=12)\n",
    "\n",
    "# 6) Finalize layout\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNykAiAKlB32WmUcNFRn/Cq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13f520ee00fb403c835b94a91986097f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ac9c39c11224d99be6b112be427473b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f795e6f0b494703aaf6536092cbcb9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86213419e7d7456e9fdbebd6272a6e06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cada00fdfdbe46109575ab6f7f660075",
      "max": 82,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a299536f92ba46039f0d748ab2984019",
      "value": 82
     }
    },
    "a299536f92ba46039f0d748ab2984019": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4ebac758079444fb7d2fa0a39c44b79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bba95f68e79d42f398d4edf60ef695da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4ebac758079444fb7d2fa0a39c44b79",
      "placeholder": "​",
      "style": "IPY_MODEL_d28b2ea7decd4043910b642a293d285f",
      "value": " 82/82 [37:54&lt;00:00, 43.27s/it]"
     }
    },
    "cada00fdfdbe46109575ab6f7f660075": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d28b2ea7decd4043910b642a293d285f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd34b63d1b69412399cdcae9e3f1c284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ac9c39c11224d99be6b112be427473b",
      "placeholder": "​",
      "style": "IPY_MODEL_5f795e6f0b494703aaf6536092cbcb9b",
      "value": "Processing source: 100%"
     }
    },
    "fcb7a76e32414075849842761fb60204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd34b63d1b69412399cdcae9e3f1c284",
       "IPY_MODEL_86213419e7d7456e9fdbebd6272a6e06",
       "IPY_MODEL_bba95f68e79d42f398d4edf60ef695da"
      ],
      "layout": "IPY_MODEL_13f520ee00fb403c835b94a91986097f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
