{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I N_FOLD (N_FOLD = 4) fold non sono 4 ‚Äúversioni‚Äù alternative del tuo modello da confrontare e poi scegliere la migliore, ma piuttosto  4 partizioni **complementari** dello stesso dataset che usi **tutte** per:\n",
        "\n",
        "1. **Stimare la generalizzazione in modo solido**\n",
        "\n",
        "   * In ciascun fold tieni fuori \\~ 1/N_FOLD dei pazienti per la validazione e trai indicazioni su come andr√† il modello su nuovi pazienti.\n",
        "   * Alla fine riporti la **media** (¬± deviazione) delle metriche sui N_FOLD fold: quella √® la stima *complessiva* delle prestazioni, non scegli ‚Äúil migliore‚Äù perch√© non √® un‚Äôipotesi di ottimizzazione ma di valutazione.\n",
        "\n",
        "2. **Tuning iper‚Äêparametri senza over-fit**\n",
        "\n",
        "   * Se vuoi ottimizzare learning rate, weight-decay o temperature, misuri l‚Äôaccuracy (e le altre metriche) su ciascun fold per una configurazione, poi scegli la configurazione che **in media** lavora meglio.\n",
        "   * Non scegli ‚Äúil fold migliore‚Äù ma la **combo di iper-parametri** che massimizza la media cross-fold.\n",
        "\n",
        "3. **Identificare instabilit√† o gruppi ‚Äúdifficili‚Äù**\n",
        "\n",
        "   * Se vedi che in un fold particular (cio√® su quei 6 pazienti di validazione) il modello crolla rispetto agli altri, capisci che c‚Äô√® qualcosa di particolare in quei soggetti: magari artefatti, stain diversi, classe sbilanciata.\n",
        "   * Ti aiuta a diagnosticare problemi di bias su specifici pazienti o sottogruppi.\n",
        "\n",
        "---\n",
        "\n",
        "### Cosa ottieni praticamente\n",
        "\n",
        "* **Metrica finale** ‚Üí ad esempio ‚Äúaccuracy paziente‚Äêlevel = 0.82 ¬± 0.05‚Äù (media e std)\n",
        "* **Curva di training/val per ogni fold** ‚Üí ti mostra over-fit o under-fit\n",
        "* **Heatmap ‚Äúfold-wise‚Äù** ‚Üí eventuali casi di failure diagnostico\n",
        "\n",
        "---\n",
        "\n",
        "### In sintesi\n",
        "\n",
        "| Scopo dei fold      | Come li usi                                                     | Risultato che migliora la tua analisi                                      |\n",
        "| ------------------- | --------------------------------------------------------------- | -------------------------------------------------------------------------- |\n",
        "| **Valutazione**     | addestri N_FOLD volte (train su 4/N_FOLD, val su 1/N_FOLD) e calcoli media/std | Stima di performance **pi√π affidabile** e non ‚Äúpazzo‚Äù per un singolo split |\n",
        "| **Tuning**          | cerchi gli iper-param che massimizzano la media cross-fold      | Parametri scelti **in modo robusto**, meno over-fit                        |\n",
        "| **Diagnosi errori** | identifichi fold ‚Äúdeboli‚Äù con metriche drammatiche              | Capisci se c‚Äô√® un sottoinsieme di pazienti problematico                    |\n",
        "| **Reporting**       | fornisci intervallo di confidenza (es. 95%)                     | Risultati **clinicamente credibili** per l‚Äôospedale                        |\n",
        "\n",
        "Non ‚Äúscegli‚Äù un fold migliore: **usali tutti** per capire come si comporta il tuo SSL+probe in scenari diversi e per **tirare fuori** una metrica unica (media¬±std) che sia solida e ripetibile.\n"
      ],
      "metadata": {
        "id": "3V6jenkhIwM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 ‚Äì Environment Setup & Dependencies\n",
        "import os, sys, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üì¶ [DEBUG] Avvio configurazione ambiente...\")\n",
        "\n",
        "# --- Colab detection ---------------------------------------------------------#\n",
        "IN_COLAB = Path(\"/content\").exists()\n",
        "if IN_COLAB:\n",
        "    print(\"üìç [DEBUG] Ambiente Google Colab rilevato.\")\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "else:\n",
        "    print(\"üíª [DEBUG] Ambiente locale rilevato (VSCode o simile).\")\n",
        "\n",
        "# --- Project root ------------------------------------------------------------#\n",
        "ENV_PATHS = {\n",
        "    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n",
        "    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n",
        "}\n",
        "PROJECT_ROOT = Path(ENV_PATHS[\"colab\" if IN_COLAB else \"local\"]).resolve()\n",
        "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
        "print(f\"üìÅ [DEBUG] PROJECT_ROOT ‚Üí {PROJECT_ROOT}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6GTjvztQka6",
        "outputId": "6bb701d0-5d8b-4319-d115-01248c3f913f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ [DEBUG] Avvio configurazione ambiente...\n",
            "üìç [DEBUG] Ambiente Google Colab rilevato.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üìÅ [DEBUG] PROJECT_ROOT ‚Üí /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egulbakn_YGW",
        "outputId": "46977489-f191-4559-c4cb-4900a6541d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Fold0 salvato: train=1500 patch, val=500, test_holdout=500\n",
            "\n",
            "üìã Fold0 train\n",
            "  pazienti per classe: {'CHROMO': 3, 'ONCO': 3, 'ccRCC': 3, 'not_tumor': 6, 'pRCC': 3}\n",
            "  patch per classe:    {'CHROMO': 300, 'ONCO': 300, 'ccRCC': 300, 'not_tumor': 300, 'pRCC': 300}\n",
            "\n",
            "üìã Fold0 val\n",
            "  pazienti per classe: {'CHROMO': 1, 'ONCO': 1, 'ccRCC': 1, 'not_tumor': 2, 'pRCC': 1}\n",
            "  patch per classe:    {'CHROMO': 100, 'ONCO': 100, 'ccRCC': 100, 'not_tumor': 100, 'pRCC': 100}\n",
            "‚úîÔ∏è Fold1 salvato: train=1525 patch, val=475 patch\n",
            "\n",
            "üìã Fold1 train\n",
            "  pazienti per classe: {'CHROMO': 3, 'ONCO': 3, 'ccRCC': 3, 'not_tumor': 6, 'pRCC': 3}\n",
            "  patch per classe:    {'CHROMO': 303, 'ONCO': 308, 'ccRCC': 295, 'not_tumor': 305, 'pRCC': 314}\n",
            "\n",
            "üìã Fold1 val\n",
            "  pazienti per classe: {'CHROMO': 1, 'ONCO': 1, 'ccRCC': 1, 'not_tumor': 2, 'pRCC': 1}\n",
            "  patch per classe:    {'CHROMO': 97, 'ONCO': 92, 'ccRCC': 105, 'not_tumor': 95, 'pRCC': 86}\n",
            "‚úîÔ∏è Fold2 salvato: train=1476 patch, val=524 patch\n",
            "\n",
            "üìã Fold2 train\n",
            "  pazienti per classe: {'CHROMO': 3, 'ONCO': 3, 'ccRCC': 3, 'not_tumor': 6, 'pRCC': 3}\n",
            "  patch per classe:    {'CHROMO': 298, 'ONCO': 293, 'ccRCC': 300, 'not_tumor': 297, 'pRCC': 288}\n",
            "\n",
            "üìã Fold2 val\n",
            "  pazienti per classe: {'CHROMO': 1, 'ONCO': 1, 'ccRCC': 1, 'not_tumor': 2, 'pRCC': 1}\n",
            "  patch per classe:    {'CHROMO': 102, 'ONCO': 107, 'ccRCC': 100, 'not_tumor': 103, 'pRCC': 112}\n",
            "‚úîÔ∏è Fold3 salvato: train=1495 patch, val=505 patch\n",
            "\n",
            "üìã Fold3 train\n",
            "  pazienti per classe: {'CHROMO': 3, 'ONCO': 3, 'ccRCC': 3, 'not_tumor': 6, 'pRCC': 3}\n",
            "  patch per classe:    {'CHROMO': 300, 'ONCO': 300, 'ccRCC': 294, 'not_tumor': 301, 'pRCC': 300}\n",
            "\n",
            "üìã Fold3 val\n",
            "  pazienti per classe: {'CHROMO': 1, 'ONCO': 1, 'ccRCC': 1, 'not_tumor': 2, 'pRCC': 1}\n",
            "  patch per classe:    {'CHROMO': 100, 'ONCO': 100, 'ccRCC': 106, 'not_tumor': 99, 'pRCC': 100}\n",
            "‚úîÔ∏è Fold4 salvato: train=1504 patch, val=496 patch\n",
            "\n",
            "üìã Fold4 train\n",
            "  pazienti per classe: {'CHROMO': 3, 'ONCO': 3, 'ccRCC': 3, 'not_tumor': 6, 'pRCC': 3}\n",
            "  patch per classe:    {'CHROMO': 299, 'ONCO': 299, 'ccRCC': 311, 'not_tumor': 297, 'pRCC': 298}\n",
            "\n",
            "üìã Fold4 val\n",
            "  pazienti per classe: {'CHROMO': 1, 'ONCO': 1, 'ccRCC': 1, 'not_tumor': 2, 'pRCC': 1}\n",
            "  patch per classe:    {'CHROMO': 101, 'ONCO': 101, 'ccRCC': 89, 'not_tumor': 103, 'pRCC': 102}\n",
            "\n",
            "üîé Verifica che i pazienti nei fold siano differenti:\n",
            "  ‚Ä¢ Fold1 vs Fold2 ‚Üí comuni (train): 8 | comuni (val): 0\n",
            "  ‚Ä¢ Fold1 vs Fold3 ‚Üí comuni (train): 8 | comuni (val): 0\n",
            "  ‚Ä¢ Fold1 vs Fold4 ‚Üí comuni (train): 8 | comuni (val): 0\n",
            "  ‚Ä¢ Fold2 vs Fold3 ‚Üí comuni (train): 8 | comuni (val): 0\n",
            "  ‚Ä¢ Fold2 vs Fold4 ‚Üí comuni (train): 8 | comuni (val): 0\n",
            "  ‚Ä¢ Fold3 vs Fold4 ‚Üí comuni (train): 8 | comuni (val): 0\n",
            "\n",
            "üéâ Generazione & verifica distribuzione completata per tutti i fold!\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Cell 2 ‚Äì Generazione e stampa distribuzione dei fold patient-level\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Directory di output\n",
        "OUT_DIR = PROJECT_ROOT / f\"data/processed/dataset_{DATASET_ID}/folds_metadata\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Numero di fold downstream (escludendo il test hold-out)\n",
        "N_FOLD = 4\n",
        "\n",
        "# 1) Carica tutto il DataFrame\n",
        "df = pd.read_parquet(PARQUET_PATH)\n",
        "\n",
        "# 2) Fold0: ricicla split originario\n",
        "df_train0 = df[df['split']=='train'].drop(columns='split').reset_index(drop=True)\n",
        "df_val0   = df[df['split']=='val']  .drop(columns='split').reset_index(drop=True)\n",
        "df_test   = df[df['split']=='test'] .drop(columns='split').reset_index(drop=True)\n",
        "\n",
        "df_train0.to_parquet(OUT_DIR/'patch_df_train_fold0.parquet',      index=False)\n",
        "df_val0  .to_parquet(OUT_DIR/'patch_df_val_fold0.parquet',        index=False)\n",
        "df_test  .to_parquet(OUT_DIR/'patch_df_test_holdout.parquet',     index=False)\n",
        "\n",
        "print(f\"‚úîÔ∏è Fold0 salvato: train={len(df_train0)} patch, val={len(df_val0)}, test_holdout={len(df_test)}\")\n",
        "\n",
        "# Funzione di verifica distribuzione\n",
        "ALL_CLASSES = sorted(df['subtype'].unique())\n",
        "def print_distribution(df_part, part_name, fold_idx):\n",
        "    pid_counts   = df_part.groupby('subtype')['patient_id'].nunique().to_dict()\n",
        "    patch_counts = df_part['subtype'].value_counts().to_dict()\n",
        "    print(f\"\\nüìã Fold{fold_idx} {part_name}\")\n",
        "    print(\"  pazienti per classe:\", {c: pid_counts.get(c,0) for c in ALL_CLASSES})\n",
        "    print(\"  patch per classe:   \", {c: patch_counts.get(c,0) for c in ALL_CLASSES})\n",
        "\n",
        "# Verifica Fold0\n",
        "print_distribution(df_train0, \"train\", 0)\n",
        "print_distribution(df_val0,   \"val\",   0)\n",
        "\n",
        "# 3) Prepara df_folds escludendo il test originario\n",
        "df_folds = df[df['split'] != 'test'].drop(columns='split').reset_index(drop=True)\n",
        "\n",
        "# 4) Costruisci DataFrame pazienti unici con etichetta principale\n",
        "patient_df = (\n",
        "    df_folds\n",
        "    .groupby('patient_id')['subtype']\n",
        "    .agg(lambda x: x.mode().iloc[0])\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Per controllo: tiene traccia dei pazienti visti\n",
        "fold_pid_sets = {}\n",
        "\n",
        "# 5) StratifiedKFold su pazienti\n",
        "skf = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\n",
        "for fold_idx, (train_pid_idx, val_pid_idx) in enumerate(skf.split(patient_df, patient_df['subtype']), start=1):\n",
        "    train_pids = patient_df.loc[train_pid_idx, 'patient_id']\n",
        "    val_pids   = patient_df.loc[val_pid_idx,   'patient_id']\n",
        "\n",
        "    df_tr = df_folds[df_folds['patient_id'].isin(train_pids)].reset_index(drop=True)\n",
        "    df_va = df_folds[df_folds['patient_id'].isin(val_pids)]  .reset_index(drop=True)\n",
        "\n",
        "    df_tr.to_parquet(OUT_DIR/f\"patch_df_train_fold{fold_idx}.parquet\", index=False)\n",
        "    df_va.to_parquet(OUT_DIR/f\"patch_df_val_fold{fold_idx}.parquet\",   index=False)\n",
        "\n",
        "    print(f\"‚úîÔ∏è Fold{fold_idx} salvato: train={len(df_tr)} patch, val={len(df_va)} patch\")\n",
        "    print_distribution(df_tr, \"train\", fold_idx)\n",
        "    print_distribution(df_va,   \"val\",   fold_idx)\n",
        "\n",
        "    # Salva set di pazienti per controllo\n",
        "    fold_pid_sets[f\"fold{fold_idx}_train\"] = set(train_pids)\n",
        "    fold_pid_sets[f\"fold{fold_idx}_val\"] = set(val_pids)\n",
        "\n",
        "# 6) Verifica incrociata: i pazienti cambiano nei fold?\n",
        "print(\"\\nüîé Verifica che i pazienti nei fold siano differenti:\")\n",
        "for i in range(1, N_FOLD):\n",
        "    for j in range(i+1, N_FOLD+1):\n",
        "        inters_train = fold_pid_sets[f\"fold{i}_train\"] & fold_pid_sets[f\"fold{j}_train\"]\n",
        "        inters_val   = fold_pid_sets[f\"fold{i}_val\"]   & fold_pid_sets[f\"fold{j}_val\"]\n",
        "        print(f\"  ‚Ä¢ Fold{i} vs Fold{j} ‚Üí comuni (train): {len(inters_train)} | comuni (val): {len(inters_val)}\")\n",
        "\n",
        "print(\"\\nüéâ Generazione & verifica distribuzione completata per tutti i fold!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Cell 3 ‚Äì Salvataggio riepilogo distribuzione e pazienti per fold\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "folds_md_path = OUT_DIR / \"folds_summary.md\"\n",
        "folds_csv_path = OUT_DIR / \"folds_patients.csv\"\n",
        "\n",
        "lines_md = []\n",
        "lines_csv = [\"fold,split,n_patients,patient_ids\"]\n",
        "\n",
        "for fold_idx in range(N_FOLD + 1):\n",
        "    fold_tag = f\"fold{fold_idx}\"\n",
        "\n",
        "    # Carica i file del fold\n",
        "    df_tr = pd.read_parquet(OUT_DIR / f\"patch_df_train_{fold_tag}.parquet\")\n",
        "    df_va = pd.read_parquet(OUT_DIR / f\"patch_df_val_{fold_tag}.parquet\")\n",
        "\n",
        "    lines_md.append(f\"‚úîÔ∏è Fold{fold_idx} salvato: train={len(df_tr)} patch, val={len(df_va)} patch\\n\")\n",
        "\n",
        "    for split_name, df_part in [(\"train\", df_tr), (\"val\", df_va)]:\n",
        "        pid_counts   = df_part.groupby('subtype')['patient_id'].nunique().to_dict()\n",
        "        patch_counts = df_part['subtype'].value_counts().to_dict()\n",
        "\n",
        "        lines_md.append(f\"üìã Fold{fold_idx} {split_name}\")\n",
        "        lines_md.append(f\"  pazienti per classe: {pid_counts}\")\n",
        "        lines_md.append(f\"  patch per classe:    {patch_counts}\\n\")\n",
        "\n",
        "        # Riga CSV per questo split\n",
        "        patient_ids = sorted(df_part['patient_id'].unique())\n",
        "        lines_csv.append(f\"{fold_tag},{split_name},{len(patient_ids)},{';'.join(patient_ids)}\")\n",
        "\n",
        "# Verifica incrociata dei pazienti tra i fold\n",
        "lines_md.append(\"üîé Verifica che i pazienti nei fold siano differenti:\")\n",
        "for i in range(1, N_FOLD):\n",
        "    for j in range(i+1, N_FOLD+1):\n",
        "        pid_i_tr = set(pd.read_parquet(OUT_DIR / f\"patch_df_train_fold{i}.parquet\")['patient_id'].unique())\n",
        "        pid_j_tr = set(pd.read_parquet(OUT_DIR / f\"patch_df_train_fold{j}.parquet\")['patient_id'].unique())\n",
        "        pid_i_val = set(pd.read_parquet(OUT_DIR / f\"patch_df_val_fold{i}.parquet\")['patient_id'].unique())\n",
        "        pid_j_val = set(pd.read_parquet(OUT_DIR / f\"patch_df_val_fold{j}.parquet\")['patient_id'].unique())\n",
        "\n",
        "        train_inter = pid_i_tr & pid_j_tr\n",
        "        val_inter   = pid_i_val & pid_j_val\n",
        "        lines_md.append(f\"  ‚Ä¢ Fold{i} vs Fold{j} ‚Üí comuni (train): {len(train_inter)} | comuni (val): {len(val_inter)}\")\n",
        "\n",
        "# Scrivi file Markdown\n",
        "with open(folds_md_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(lines_md))\n",
        "\n",
        "# Scrivi file CSV\n",
        "with open(folds_csv_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(lines_csv))\n",
        "\n",
        "print(f\"üìù File riepilogo scritto in: {folds_md_path}\")\n",
        "print(f\"üìÑ CSV pazienti scritto in:   {folds_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyd0frfUWpGN",
        "outputId": "c907f8e7-62f6-499c-9062-ad1bf0e35b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ File fold_patient_summary.csv salvato in: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/folds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  create webdatasets folds\n",
        "# ============================================================\n",
        "!pip install --quiet webdataset tqdm\n",
        "\n",
        "import tarfile, shutil\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import webdataset as wds\n",
        "\n",
        "# --- PATHS ---------------------------------------------------\n",
        "DATASET_ID  = \"9f30917e\"\n",
        "PR          = Path(\"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\")\n",
        "DS_DIR      = PR / f\"data/processed/dataset_{DATASET_ID}\"\n",
        "FOLDS_DIR   = DS_DIR / \"folds_metadata\"\n",
        "WEBDIR      = DS_DIR / \"webdataset\"\n",
        "WEBDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CANON_TRAIN = WEBDIR / \"train/patches-0000.tar\"\n",
        "CANON_VAL   = WEBDIR / \"val/patches-0000.tar\"\n",
        "CANON_TEST  = WEBDIR / \"test/patches-0000.tar\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 0) Fold0 = copia grezza (+ holdout)\n",
        "# ------------------------------------------------------------------\n",
        "for split, src in ((\"train\", CANON_TRAIN), (\"val\", CANON_VAL)):\n",
        "    dst = WEBDIR / f\"fold0/{split}\"\n",
        "    dst.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy2(src, dst / \"patches-0000.tar\")\n",
        "shutil.copy2(CANON_TEST, WEBDIR / \"test_holdout.tar\")\n",
        "print(\"‚úîÔ∏è  Copiato fold0 + test_holdout\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1) Indicizza tutti e tre i tar canonici\n",
        "# ------------------------------------------------------------------\n",
        "index = {}      # key -> (tar_path, member_name)\n",
        "tar_cache = {}\n",
        "\n",
        "def build_index(tar_path):\n",
        "    t = tarfile.open(tar_path, \"r\")\n",
        "    tar_cache[tar_path] = t\n",
        "    for m in t.getmembers():\n",
        "        if m.isfile() and m.name.endswith(\".jpg\"):\n",
        "            index[Path(m.name).stem] = (tar_path, m.name)\n",
        "\n",
        "for tp in (CANON_TRAIN, CANON_VAL, CANON_TEST):\n",
        "    build_index(tp)\n",
        "print(f\"üîç Indicizzati {len(index)} patch totali\")\n",
        "\n",
        "def get_bytes(key):\n",
        "    tar_path, member = index[key]\n",
        "    return tar_cache[tar_path].extractfile(member).read()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2) Funzione helper con filtraggio delle chiavi mancanti\n",
        "# ------------------------------------------------------------------\n",
        "def parquet_to_tar(parq_path: Path, out_tar: Path, desc: str):\n",
        "    df = pd.read_parquet(parq_path)\n",
        "    # calcola le chiavi clean\n",
        "    df[\"key\"] = df.apply(lambda r: f\"{r.subtype}_{r.patient_id.replace('.','')}_{int(r.x)}_{int(r.y)}\", axis=1)\n",
        "    # filtra solo quelle presenti\n",
        "    mask = df[\"key\"].isin(index)\n",
        "    missing = (~mask).sum()\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è  {desc}: {missing} patch scartate perch√© non trovate nei tar originali\")\n",
        "    df = df[mask].reset_index(drop=True)\n",
        "\n",
        "    # scrivi il tar\n",
        "    with wds.TarWriter(str(out_tar)) as sink:\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
        "            img_bytes = get_bytes(row[\"key\"])\n",
        "            sink.write({\n",
        "                \"__key__\": row[\"key\"],\n",
        "                \"jpg\": img_bytes,\n",
        "                \"cls\": row.subtype.encode(\"utf-8\"),\n",
        "            })\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3) Genera fold1‚Ä•fold3\n",
        "# ------------------------------------------------------------------\n",
        "for fold in range(1, 4):\n",
        "    for split in (\"train\", \"val\"):\n",
        "        parq = FOLDS_DIR / f\"patch_df_{split}_fold{fold}.parquet\"\n",
        "        out_dir = WEBDIR / f\"fold{fold}/{split}\"\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        out_tar = out_dir / \"patches-0000.tar\"\n",
        "        parquet_to_tar(parq, out_tar, f\"fold{fold}-{split}\")\n",
        "\n",
        "print(\"üéâ  WebDataset per fold1-3 costruiti con successo\")\n"
      ],
      "metadata": {
        "id": "t70nXv6Ydd0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "090884bb-ce35-46ec-a02a-0b9dd869f3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/train/patches-0000.tar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-41-4206003872.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWEBDIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"fold0/{split}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"patches-0000.tar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCANON_TEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEBDIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"test_holdout.tar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úîÔ∏è  Copiato fold0 + test_holdout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/train/patches-0000.tar'"
          ]
        }
      ]
    }
  ]
}