{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13301,
     "status": "ok",
     "timestamp": 1749305405756,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "CPkMc3ONzi0J",
    "outputId": "b0dee59a-0134-494c-e50a-e2e92b126243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ [DEBUG] Avvio configurazione ambiente...\n",
      "üíª [DEBUG] Ambiente locale rilevato (VSCode o simile).\n",
      "üìÅ [DEBUG] PROJECT_ROOT ‚Üí /Users/mimmo/Desktop/mimmo/MLA/project_FP03/wsi-ssrl-rcc_project\n",
      "üîß [DEBUG] Installazione pacchetti mancanti: ['pillow', 'pyyaml']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äì Environment Setup & Dependencies\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì¶ [DEBUG] Avvio configurazione ambiente...\")\n",
    "\n",
    "# --- Colab detection ---------------------------------------------------------#\n",
    "IN_COLAB = Path(\"/content\").exists()\n",
    "if IN_COLAB:\n",
    "    print(\"üìç [DEBUG] Ambiente Google Colab rilevato.\")\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "else:\n",
    "    print(\"üíª [DEBUG] Ambiente locale rilevato (VSCode o simile).\")\n",
    "\n",
    "# --- Project root ------------------------------------------------------------#\n",
    "ENV_PATHS = {\n",
    "    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n",
    "    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n",
    "}\n",
    "PROJECT_ROOT = Path(ENV_PATHS[\"colab\" if IN_COLAB else \"local\"]).resolve()\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "print(f\"üìÅ [DEBUG] PROJECT_ROOT ‚Üí {PROJECT_ROOT}\")\n",
    "\n",
    "!pip install --quiet torch torchvision webdataset tqdm pillow\n",
    "\n",
    "# --- Dependencies (installa solo se mancano) ---------------------------------#\n",
    "def _pip_install(pkgs):\n",
    "    import importlib.util, sys, subprocess\n",
    "    missing = [p for p in pkgs if importlib.util.find_spec(p) is None]\n",
    "    if missing:\n",
    "        print(f\"üîß [DEBUG] Installazione pacchetti mancanti: {missing}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n",
    "    else:\n",
    "        print(\"‚úÖ [DEBUG] Tutti i pacchetti richiesti sono gi√† installati.\")\n",
    "\n",
    "_pip_install([\"torch\", \"torchvision\", \"webdataset\", \"tqdm\", \"pillow\", \"pyyaml\", \"joblib\"])\n",
    "\n",
    "# helper per il tarball del dataset (usato in Cell 2)\n",
    "DATA_TARBALL = PROJECT_ROOT / \"data\" / \"processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8UIj4XOZP6U9"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mimmo/Desktop/mimmo/MLA/project_FP03/wsi-ssrl-rcc_project/src/utils/training_utils.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m utils_mod  \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)     \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mand\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mloader, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load spec for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mutils_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mutils_mod\u001b[49m\u001b[43m)\u001b[49m                     \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils.training_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m utils_mod        \u001b[38;5;66;03m# register in sys.modules\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] Loaded utils.training_utils from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mutils_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1016\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1073\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mimmo/Desktop/mimmo/MLA/project_FP03/wsi-ssrl-rcc_project/src/utils/training_utils.py'"
     ]
    }
   ],
   "source": [
    "# Cell 3 ‚Äì Dynamic import of utils.training_utils\n",
    "import sys\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) locate & load the module file\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\" / \"training_utils.py\"\n",
    "spec       = importlib.util.spec_from_file_location(\"utils.training_utils\", str(utils_path))\n",
    "utils_mod  = importlib.util.module_from_spec(spec)     # type: ignore[arg-type]\n",
    "assert spec and spec.loader, f\"Cannot load spec for {utils_path}\"\n",
    "spec.loader.exec_module(utils_mod)                     # type: ignore[assignment]\n",
    "sys.modules[\"utils.training_utils\"] = utils_mod        # register in sys.modules\n",
    "print(f\"[DEBUG] Loaded utils.training_utils from {utils_path}\")\n",
    "\n",
    "# 2) import what we need\n",
    "from utils.training_utils import (\n",
    "    TRAINER_REGISTRY,\n",
    "    get_latest_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "\n",
    "print(\"[DEBUG] Imported:\")\n",
    "print(\"  ‚Ä¢ TRAINER_REGISTRY keys:\", list(TRAINER_REGISTRY.keys()))\n",
    "print(\"  ‚Ä¢ get_latest_checkpoint ‚Üí\", get_latest_checkpoint)\n",
    "print(\"  ‚Ä¢ load_checkpoint       ‚Üí\", load_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwx0_Pr9E59j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] EXP_CODE ‚Üí 20250626011748\n",
      "[DEBUG] TRAIN ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/train/patches-0000.tar\n",
      "[DEBUG] VAL ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/val/patches-0000.tar\n",
      "[DEBUG] TEST ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test/patches-0000.tar\n",
      "[DEBUG] EXP_BASE ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250626011748\n",
      "[DEBUG] Scritto   /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250626011748/training_20250626011748.yaml\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 ‚Äì Configuration & Directory Setup (formatted and absolute paths)\n",
    "import yaml, datetime, os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Definisci PROJECT_ROOT come nel Cell 1 ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "# Adatta il path a dove hai montato il tuo Drive\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî Carica la config da training.yaml ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "cfg_path = PROJECT_ROOT / \"config\" / \"training.yaml\"\n",
    "cfg      = yaml.safe_load(cfg_path.read_text())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 0) EXP_CODE: riprendi da YAML ‚Üí env ‚Üí genera nuovo                 #\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "yaml_exp = cfg.get(\"exp_code\", \"\")           # <‚îÄ nuovo parametro\n",
    "env_exp  = os.environ.get(\"EXP_CODE\", \"\")\n",
    "\n",
    "if yaml_exp:                                 # 1) priorit√† al file YAML\n",
    "    EXP_CODE = yaml_exp\n",
    "elif env_exp:                                # 2) poi variabile d‚Äôambiente\n",
    "    EXP_CODE = env_exp\n",
    "else:                                        # 3) altrimenti nuovo timestamp\n",
    "    EXP_CODE = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# salva nel processo per eventuali figli\n",
    "os.environ[\"EXP_CODE\"] = EXP_CODE\n",
    "\n",
    "print(f\"[DEBUG] EXP_CODE ‚Üí {EXP_CODE}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1) Carica configurazione generale                                  #\n",
    "# ------------------------------------------------------------------ #\n",
    "cfg_path  = PROJECT_ROOT / \"config\" / \"training.yaml\"\n",
    "cfg       = yaml.safe_load(cfg_path.read_text())\n",
    "DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n",
    "cfg[\"exp_code\"] = EXP_CODE     # lo inseriamo nel dict per eventuali usi downstream\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2) Percorsi assoluti (train / val / test)                          #\n",
    "# ------------------------------------------------------------------ #\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    rel = cfg[\"data\"][split].format(dataset_id=DATASET_ID)\n",
    "    abs_ = (PROJECT_ROOT / rel).resolve()\n",
    "    cfg[\"data\"][split] = str(abs_)\n",
    "    print(f\"[DEBUG] {split.upper()} ‚Üí {abs_}\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3) Struttura directory esperimento                                 #\n",
    "# ------------------------------------------------------------------ #\n",
    "EXP_ROOT = PROJECT_ROOT / \"data\" / \"processed\" / str(DATASET_ID)\n",
    "EXP_BASE = EXP_ROOT / \"experiments\" / EXP_CODE                   # unica per tutta la run\n",
    "EXP_BASE.mkdir(parents=True, exist_ok=True)\n",
    "(EXP_ROOT / \"experiments.md\").touch(exist_ok=True)               # indice globale\n",
    "\n",
    "print(f\"[DEBUG] EXP_BASE ‚Üí {EXP_BASE}\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 4) Salva la **copia YAML** solo se non esiste gi√†                  #\n",
    "# ------------------------------------------------------------------ #\n",
    "exp_yaml = EXP_BASE / f\"training_{EXP_CODE}.yaml\"\n",
    "if not exp_yaml.exists():                          # evita duplicati\n",
    "    exp_yaml.write_text(yaml.dump(cfg, sort_keys=False))\n",
    "    print(f\"[DEBUG] Scritto   {exp_yaml}\")\n",
    "else:\n",
    "    print(f\"[DEBUG] Config gi√† presente ‚Üí {exp_yaml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Registered trainers: ['simclr', 'moco_v2', 'rotation', 'jigsaw', 'supervised', 'transfer']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ‚Äì Import all trainer modules\n",
    "import importlib, sys\n",
    "from utils.training_utils import TRAINER_REGISTRY\n",
    "\n",
    "trainer_mods = [\n",
    "    \"trainers.simclr\",\n",
    "    \"trainers.moco_v2\",\n",
    "    \"trainers.rotation\",\n",
    "    #\"trainers.jigsaw\",\n",
    "    \"trainers.jepa\",\n",
    "    \"trainers.supervised\",   \n",
    "    \"trainers.transfer\",\n",
    "]\n",
    "\n",
    "for m in trainer_mods:\n",
    "    importlib.reload(sys.modules[m]) if m in sys.modules else importlib.import_module(m)\n",
    "\n",
    "print(f\"[DEBUG] Registered trainers: {list(TRAINER_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% -------------------------------------------------------------------- #\n",
    "# Cell 6 ‚Äì Helper utilities (Tee, paths, selezione, ‚Ä¶)                    #\n",
    "# ----------------------------------------------------------------------- #\n",
    "import contextlib, sys, time, inspect\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from utils.training_utils import get_latest_checkpoint, load_checkpoint\n",
    "from trainers.train_classifier import train_classifier\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "# I/O helpers                                                             #\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "class _Tee:\n",
    "    \"\"\"Duplica stdout / stderr su console *e* file.\"\"\"\n",
    "    def __init__(self, *targets): self.targets = targets\n",
    "    def write(self, data):  [t.write(data) and t.flush() for t in self.targets]\n",
    "    def flush(self):        [t.flush()      for t in self.targets]\n",
    "\n",
    "def _global_experiments_append(line: str):\n",
    "    \"\"\"Aggiunge una riga all‚Äôindice globale `experiments.md`.\"\"\"\n",
    "    exp_md = EXP_ROOT / \"experiments.md\"\n",
    "    with exp_md.open(\"a\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "# Path builders & artefact checks                                         #\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "def _paths(model_name: str) -> dict[str, Path]:\n",
    "    \"\"\"Restituisce tutti i path (dir/log/ckpt/features/clf) per un modello.\"\"\"\n",
    "    mdir = EXP_BASE / model_name\n",
    "    mdir.mkdir(parents=True, exist_ok=True)\n",
    "    return {\n",
    "        \"dir\"      : mdir,\n",
    "        \"log\"      : mdir / f\"log_{EXP_CODE}.md\",\n",
    "        \"features\" : mdir / f\"{model_name}_features.pt\",\n",
    "        \"clf\"      : mdir / f\"{model_name}_classifier.joblib\",\n",
    "        \"ckpt_pref\": f\"{model_name}_epoch\",        # usato da save_checkpoint\n",
    "    }\n",
    "\n",
    "def _completed(paths: dict, is_ssl: bool) -> bool:\n",
    "    \"\"\"True se TUTTI gli artefatti richiesti sono gi√† presenti.\"\"\"\n",
    "    ckpt_ok = get_latest_checkpoint(paths[\"dir\"]) is not None\n",
    "    if not ckpt_ok:\n",
    "        return False\n",
    "    if is_ssl:\n",
    "        return paths[\"features\"].exists() and paths[\"clf\"].exists()\n",
    "    return True\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "# Selezione modelli & trainer helpers                                     #\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "def _select_models(cfg: dict) -> dict[str, dict]:\n",
    "    sel = cfg.get(\"run_model\", \"all\").lower()\n",
    "    return {\n",
    "        n: c for n, c in cfg[\"models\"].items()\n",
    "        if sel in (\"all\", n) or\n",
    "           (sel == \"sl\"  and c.get(\"type\") == \"sl\") or\n",
    "           (sel == \"ssl\" and c.get(\"type\") == \"ssl\")\n",
    "    }\n",
    "\n",
    "def _init_trainer(name: str, m_cfg: dict, data_cfg: dict, ckpt_dir: Path):\n",
    "    Trainer = TRAINER_REGISTRY[name]\n",
    "    tr = Trainer(m_cfg, data_cfg)\n",
    "    tr.ckpt_dir = ckpt_dir         # salva i .pt qui\n",
    "    return tr\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "# Training / resume / artefacts                                           #\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "def _run_full_training(trainer, epochs: int):\n",
    "    \"\"\"Loop di training (identico al codice esistente, solo incapsulato).\"\"\"\n",
    "    has_val = hasattr(trainer, \"validate_epoch\")\n",
    "    total_batches = getattr(trainer, \"batches_train\", None)\n",
    "    if total_batches is None:\n",
    "        try: total_batches = len(trainer.train_loader)\n",
    "        except TypeError: total_batches = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time(); run_loss = run_corr = seen = 0\n",
    "        print(f\"--- Epoch {epoch}/{epochs} ---\")\n",
    "        for i, batch in enumerate(trainer.train_loader, 1):\n",
    "            sig = inspect.signature(trainer.train_step)\n",
    "            res = trainer.train_step(batch) if len(sig.parameters) == 1 \\\n",
    "                  else trainer.train_step(*batch)\n",
    "            if len(res) == 4: _, loss, corr, bs = res\n",
    "            else:             loss, bs = res; corr = 0\n",
    "            run_loss += loss * bs; run_corr += corr; seen += bs\n",
    "\n",
    "            # progress bar\n",
    "            if total_batches:\n",
    "                pct = (i/total_batches)*100\n",
    "                eta = ((time.time()-t0)/i)*(total_batches-i)\n",
    "                msg = (f\"  Batch {i}/{total_batches} ({pct:.1f}%) | \"\n",
    "                       f\"Loss: {run_loss/seen:.4f}\")\n",
    "                if has_val: msg += f\" | Acc: {run_corr/seen:.3f}\"\n",
    "                msg += f\" | Elapsed: {time.time()-t0:.1f}s | ETA: {eta:.1f}s\"\n",
    "            else:\n",
    "                msg = f\"  Batch {i} | Loss: {run_loss/seen:.4f}\"\n",
    "                if has_val: msg += f\" | Acc: {run_corr/seen:.3f}\"\n",
    "                msg += f\" | Elapsed: {time.time()-t0:.1f}s\"\n",
    "            print(msg)\n",
    "\n",
    "        if has_val:\n",
    "            v_loss, v_acc = trainer.validate_epoch()\n",
    "            trainer.post_epoch(epoch, v_acc)\n",
    "            print(f\"Val -> Loss: {v_loss:.4f} | Acc: {v_acc:.3f}\")\n",
    "        else:\n",
    "            trainer.post_epoch(epoch, run_loss/seen)\n",
    "\n",
    "        print(f\"Epoch {epoch} completed in {time.time()-t0:.1f}s\\n\")\n",
    "\n",
    "def _resume_or_train(trainer, paths: dict, epochs: int):\n",
    "    ckpt = get_latest_checkpoint(paths[\"dir\"])\n",
    "    if ckpt:\n",
    "        print(f\"‚è© Resume da {ckpt.name}\")\n",
    "\n",
    "        # 1) MoCoV2Trainer\n",
    "        if hasattr(trainer, \"encoder_q\") and hasattr(trainer, \"projector_q\"):\n",
    "            model = torch.nn.Sequential(trainer.encoder_q, trainer.projector_q)\n",
    "            optim = trainer.optimizer\n",
    "\n",
    "        # 2) SimCLR (e altri SSL che hanno encoder + projector)\n",
    "        elif hasattr(trainer, \"encoder\") and hasattr(trainer, \"projector\"):\n",
    "            model = torch.nn.Sequential(trainer.encoder, trainer.projector)\n",
    "            optim = getattr(trainer, \"optimizer\", None)\n",
    "\n",
    "        # 3) Jigsaw (encoder + head)\n",
    "        elif hasattr(trainer, \"encoder\") and hasattr(trainer, \"head\"):\n",
    "            model = torch.nn.Sequential(trainer.encoder, trainer.head)\n",
    "            optim = trainer.optimizer\n",
    "\n",
    "        # 4) RotationTrainer (ha .model)\n",
    "        elif hasattr(trainer, \"model\"):\n",
    "            model = trainer.model\n",
    "            optim = trainer.optimizer\n",
    "\n",
    "        else:\n",
    "            raise AttributeError(f\"Impossibile fare resume su {trainer!r}\")\n",
    "\n",
    "        # carica pesi (+ optimizer se presente)\n",
    "        load_checkpoint(ckpt, model, optim)\n",
    "        return  # fine: skip training\n",
    "\n",
    "    # se non c‚Äô√® checkpoint, fai training da zero\n",
    "    _run_full_training(trainer, epochs)\n",
    "\n",
    "\n",
    "def _ensure_ssl_artifacts(trainer, paths: dict):\n",
    "    if not paths[\"features\"].exists():\n",
    "        trainer.extract_features_to(str(paths[\"features\"]))\n",
    "    if not paths[\"clf\"].exists():\n",
    "        train_classifier(str(paths[\"features\"]), str(paths[\"clf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMV3BYIyE7FM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è© Training delegato a SLURM: skip esecuzione locale.\n"
     ]
    }
   ],
   "source": [
    "# %% -------------------------------------------------------------------- #\n",
    "# Cell 7 ‚Äì Modular Launch & Auto-Recover                                  #\n",
    "# ----------------------------------------------------------------------- #\n",
    "def launch_training(cfg: dict) -> None:\n",
    "    \"\"\"Lancia (o recupera) tutti i modelli selezionati.\"\"\"\n",
    "    for name, m_cfg in _select_models(cfg).items():\n",
    "        paths   = _paths(name)\n",
    "        is_ssl  = m_cfg.get(\"type\") == \"ssl\"\n",
    "        epochs  = int(m_cfg[\"training\"][\"epochs\"])\n",
    "\n",
    "        # ---------- logging (append) ----------------------------------- #\n",
    "        with open(paths[\"log\"], \"a\") as lf, \\\n",
    "             contextlib.redirect_stdout(_Tee(sys.stdout, lf)), \\\n",
    "             contextlib.redirect_stderr(_Tee(sys.stderr, lf)):\n",
    "\n",
    "            if _completed(paths, is_ssl):\n",
    "                print(f\"‚úÖ Artefatti completi per '{name}' ‚Äì skip\\n\")\n",
    "                continue\n",
    "\n",
    "            trainer = _init_trainer(name, m_cfg, cfg[\"data\"], paths[\"dir\"])\n",
    "            print(f\"Device: {trainer.device} üöÄ  Starting training for '{name}'\")\n",
    "            print(f\"‚Üí Model config: {m_cfg}\\n\")\n",
    "\n",
    "            _resume_or_train(trainer, paths, epochs)\n",
    "\n",
    "            if is_ssl:\n",
    "                _ensure_ssl_artifacts(trainer, paths)\n",
    "\n",
    "            # Aggiorna indice globale\n",
    "            last_ckpt = get_latest_checkpoint(paths[\"dir\"])\n",
    "            rel = last_ckpt.relative_to(EXP_ROOT) if last_ckpt else \"-\"\n",
    "            _global_experiments_append(f\"| {EXP_CODE} | {name} | {epochs} | {rel} |\")\n",
    "\n",
    "# üöÄ Avvio immediato\n",
    "# üöÄ Avvio immediato SOLO se siamo in Colab\n",
    "if IN_COLAB:\n",
    "    launch_training(cfg)\n",
    "else:\n",
    "    print(\"‚è© Training delegato a SLURM: skip esecuzione locale.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Terminale VSCode (Remote SSH o locale)**\n",
    "   Dopo aver sottoposto il job, prendi il `JOBID` dallo stdout di `sbatch`.\n",
    "   Poi apri un terminale in VSCode e digiti:\n",
    "\n",
    "   ```bash\n",
    "   ssh mla_group_01@legionlogin.polito.it\n",
    "   cd /home/mla_group_01/wsi-ssrl-rcc_project\n",
    "   tail -f rcc_ssrl_launch_<JOBID>.out\n",
    "   ```\n",
    "\n",
    "   In questo modo vedrai **in tempo reale** tutti i `print` man mano che il training avanza.\n",
    "\n",
    "2. **VSCode ‚ÄúRemote SSH‚Äù Extension**\n",
    "   Se installi l‚Äôestensione Remote SSH di VSCode, puoi:\n",
    "\n",
    "   * Connetterti direttamente al nodo di login (`legionlogin.polito.it`)\n",
    "   * Aprire il file `rcc_ssrl_launch_<JOBID>.out` nell‚Äôeditor\n",
    "   * Abilitare ‚ÄúAuto Save‚Äù e ‚ÄúFollow Tail‚Äù (clic destro ‚Üí *Tail Follow*), per visualizzare i nuovi messaggi senza uscire dall‚ÄôIDE."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMUn+/uyACrBWAZGivnsuX",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wsi-ssrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
