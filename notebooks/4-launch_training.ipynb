{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":28771,"status":"error","timestamp":1752630876773,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"},"user_tz":240},"id":"2GFLbAUPJoOq","outputId":"ef34ed5c-d409-43be-b208-a1ea3d0c413c"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     case = d.expect([\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return self.expect_list(compiled_pattern_list,\n\u001b[0m\u001b[1;32m    355\u001b[0m                 timeout, searchwindowsize, async_)\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"bi1bbemBZE99"},"source":["## 1. ğŸ§  Self-Supervised Models (SimCLR, MoCo, Rotation, JEPA)\n","\n","### 1. Pre-training\n","\n","* Allena lâ€™**encoder una sola volta**, su tutti i pazienti di `train_fold0`.\n","* Salva i checkpoint (encoder + projector).\n","\n","### 2. Estrazione delle feature\n","\n","* Carica il **checkpoint fisso** del pretraining.\n","* Estrai le feature su tutti i fold (`train_fold{i}`, `val_fold{i}`, `test_holdout`).\n","* Salva i file come `{model}_features_fold{i}.pt`.\n","\n","### 3. Linear-Probe per ogni fold `i âˆˆ [0, ..., N-1]`\n","\n","* **Train**: allena la testa lineare (LogReg, MLP) sulle feature di `train_fold{i}`.\n","* **Validation**: usa `val_fold{i}` per:\n","\n","  * early stopping,\n","  * salvare il miglior probe,\n","  * applicare **Temperature Scaling**.\n","* **Test**: valuta il probe calibrato su `test_holdout`, usando anche **MC-Dropout**.\n","\n","### 4. Aggregazione finale\n","\n","* Raccogli i risultati su `test_holdout` per ogni fold.\n","* Calcola **media Â± deviazione standard** per ogni metrica (Accuracy, F1, AUC, ECE, incertezza).\n","* Questo Ã¨ il risultato finale del tuo modello SSL + probe.\n","\n","> ğŸ”’ **Non riaddestrare lâ€™encoder nei fold 1â€“N.**\n","> Lo scopo Ã¨ testarne la **capacitÃ  di generalizzazione task-agnostica**, non adattarlo a ogni fold.\n","\n","---\n","\n","## 2. ğŸ§ª Supervised & Transfer Learning\n","\n","Qui **non hai un encoder fisso**. Ogni fold ha il proprio training da zero.\n","\n","### Per ciascun fold `i âˆˆ [0, ..., N-1]`\n","\n","* **Train**: allena lâ€™intero modello (es. ResNet-50) su `train_fold{i}`.\n","* **Validation**: usa `val_fold{i}` per early-stopping e calibrazione.\n","* **Test**: valuta sempre su `test_holdout` (o sul val-fold se non esiste un holdout).\n","* Salva un checkpoint per ogni fold (`supervised_fold{i}.pt`, `transfer_fold{i}.pt`, ecc.).\n","\n","### Aggregazione finale\n","\n","* Come per gli SSL, calcola la **media Â± deviazione** delle metriche su `test_holdout` per i vari fold.\n","* Non scegli il modello col miglior punteggio, ma riporti la **media aggregata**.\n","\n","> â„¹ï¸ **Facoltativo**: puoi evidenziare il fold piÃ¹ vicino alla media, **solo a scopo illustrativo**.\n","\n","---\n","\n","## 3. ğŸ” Confronto tra pipeline SSL e SL\n","\n","| Step                   | SSL models (SimCLR, MoCo, ...)  | SL/Transfer models              |\n","| ---------------------- | ------------------------------- | ------------------------------- |\n","| **Encoder training**   | Solo su fold0                   | Uno per ogni fold               |\n","| **Feature extraction** | Uno per ogni fold               | â€“                               |\n","| **Probe/classifier**   | Uno per ogni fold               | Uno per ogni fold               |\n","| **Inference**          | Su `test_holdout` per ogni fold | Su `test_holdout` per ogni fold |\n","| **Checkpoint**         | Encoder 1Ã—                      | 1Ã— per fold                     |\n","| **Output finale**      | Media Â± std su tutti i fold     | Media Â± std su tutti i fold     |\n","\n","---\n","\n","## ğŸ¯ PerchÃ© questa architettura?\n","\n","* Nei **modelli SSL**, vogliamo dimostrare che un encoder **generalizza** a nuovi pazienti come **feature extractor**, senza mai essere fine-tuned.\n","* Nei **modelli supervisionati**, alleniamo da capo su ogni fold per valutare una baseline comparabile (ma meno task-agnostica).\n","\n","In entrambi i casi:\n","\n","âœ… **Non scegli il miglior fold**,\n","âœ… **Riporti solo le medie cross-fold**,\n","âœ… **Dimostri affidabilitÃ , generalizzazione e riproducibilitÃ **.\n","\n","---\n","\n","## ğŸ“Š Output finale\n","\n","Le metriche aggregate sono presentate in una tabella riassuntiva:\n","\n","| Model             | Accuracy (Î¼Â±Ïƒ) | Macro-F1 (Î¼Â±Ïƒ) | ECE (Î¼Â±Ïƒ)   | Uncertainty (Î¼Â±Ïƒ) |\n","| ----------------- | -------------- | -------------- | ----------- | ----------------- |\n","| SimCLR + probe    | 0.83 Â± 0.04    | 0.79 Â± 0.06    | 0.03 Â± 0.01 | 0.12 Â± 0.03       |\n","| MoCo-v2 + probe   | 0.78 Â± 0.05    | â€¦              | â€¦           | â€¦                 |\n","| Rotation + probe  | 0.74 Â± 0.08    | â€¦              | â€¦           | â€¦                 |\n","| Supervised        | 0.80 Â± 0.03    | â€¦              | â€¦           | â€¦                 |\n","| Transfer learning | 0.82 Â± 0.04    | â€¦              | â€¦           | â€¦                 |\n","\n","> Ogni riga rappresenta una valutazione **completa e aggregata** del modello, utile per confronti clinici e accademici.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":73,"status":"aborted","timestamp":1752630876794,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"},"user_tz":240},"id":"CPkMc3ONzi0J"},"outputs":[],"source":["# ## Cell 1 â€“ Environment Setup & Dependencies\n","# Compatibile con Google Colab (GPU/CPU) e ambiente locale (VS Code).\n","\n","# %%\n","import os, sys, subprocess, importlib.util\n","from pathlib import Path\n","\n","print(\"ğŸ“¦ [DEBUG] Avvio configurazione ambienteâ€¦\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 1) Rileva ambiente (Colab vs locale)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","IN_COLAB = Path(\"/content\").exists()\n","if IN_COLAB:\n","    print(\"ğŸ“ [DEBUG] Google Colab rilevato.\")\n","    from google.colab import drive  # type: ignore\n","    drive.mount(\"/content/drive\", force_remount=False)\n","else:\n","    print(\"ğŸ’» [DEBUG] Ambiente locale (VS Code / CLI) rilevato.\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 2) Definisci PROJECT_ROOT (ENV > default mapping)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DEFAULT_ENV_PATHS = {\n","    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n","    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n","}\n","PROJECT_ROOT = Path(os.getenv(\n","    \"PROJECT_ROOT\",\n","    DEFAULT_ENV_PATHS[\"colab\" if IN_COLAB else \"local\"])\n",").resolve()\n","\n","sys.path.append(str(PROJECT_ROOT / \"src\"))\n","print(f\"ğŸ“ [DEBUG] PROJECT_ROOT â†’ {PROJECT_ROOT}\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 3) Utility per installare pacchetti mancanti\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def _missing(pkgs):\n","    return [p for p in pkgs if importlib.util.find_spec(p) is None]\n","\n","def _install(pkgs, idx_url=None):\n","    if not pkgs:\n","        return\n","    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"]\n","    if idx_url:\n","        cmd += [\"--index-url\", idx_url]\n","    subprocess.check_call(cmd + pkgs)\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 4) Verifica / installa PyTorch (se non presente)\n","#    â€¢ In Colab non sovrascrive la versione pre-installata\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","TORCH_PKGS = [\"torch\", \"torchvision\", \"torchaudio\"]\n","\n","if _missing([\"torch\"]):\n","    print(\"ğŸ”§ [DEBUG] PyTorch non trovato â†’ installazione in corsoâ€¦\")\n","    if IN_COLAB:\n","        GPU = Path(\"/usr/local/cuda\").exists()\n","        INDEX = \"https://download.pytorch.org/whl/cu121\" if GPU else \"https://download.pytorch.org/whl/cpu\"\n","        _install(TORCH_PKGS, INDEX)\n","    else:  # locale: lascia scegliere all'utente il build corretto\n","        _install(TORCH_PKGS)\n","else:\n","    import torch\n","    print(f\"âœ… [DEBUG] PyTorch giÃ  presente ({torch.__version__})\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 5) Installazione pacchetti ausiliari (sempre sicura)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","AUX_PKGS = [\"webdataset\", \"tqdm\", \"pillow\", \"pyyaml\", \"joblib\"]\n","missing_aux = _missing(AUX_PKGS)\n","if missing_aux:\n","    print(f\"ğŸ”§ [DEBUG] Installazione pacchetti ausiliari mancanti: {missing_aux}\")\n","    _install(missing_aux)\n","else:\n","    print(\"âœ… [DEBUG] Pacchetti ausiliari giÃ  presenti.\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 6) Info dispositivo\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import torch\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"ğŸ–¥ï¸ [DEBUG] Torch device disponibile â†’ {DEVICE}\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 7) Costante path per Data Tarball (utilizzata negli step successivi)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DATA_TARBALL = PROJECT_ROOT / \"data\" / \"processed\"\n","print(f\"ğŸ“¦ [DEBUG] DATA_TARBALL â†’ {DATA_TARBALL}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":80,"status":"aborted","timestamp":1752630876806,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"},"user_tz":240},"id":"jwx0_Pr9E59j"},"outputs":[],"source":["# %% -------------------------------------------------------------------- #\n","# Cell 2 â€“ Configurazione & Setup Esperimento                            #\n","# ----------------------------------------------------------------------- #\n","import yaml\n","import datetime\n","import os\n","import pprint\n","import random\n","import numpy as np\n","import torch\n","from pathlib import Path\n","\n","# â”€â”€â”€ 1) Carica il file di configurazione â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","cfg_path = PROJECT_ROOT / \"config\" / \"training.yaml\"\n","assert cfg_path.exists(), f\"âŒ File mancante: {cfg_path}\"\n","cfg = yaml.safe_load(cfg_path.read_text())\n","print(f\"ğŸ“„ [DEBUG] Config caricata da: {cfg_path}\")\n","\n","# â”€â”€â”€ 2) Genera EXP_CODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","yaml_exp = cfg.get(\"exp_code\", \"\")\n","env_exp  = os.getenv(\"EXP_CODE\", \"\")\n","if yaml_exp:\n","    EXP_CODE, src = yaml_exp, \"YAML\"\n","elif env_exp:\n","    EXP_CODE, src = env_exp, \"ENV\"\n","else:\n","    EXP_CODE, src = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"), \"TIMESTAMP\"\n","os.environ[\"EXP_CODE\"] = EXP_CODE\n","cfg[\"exp_code\"] = EXP_CODE\n","print(f\"ğŸ”‘ [DEBUG] EXP_CODE â†’ {EXP_CODE} (fonte: {src})\")\n","\n","# â”€â”€â”€ 3) Parametri dataset & folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n","FOLDS = cfg.get(\"folds\", [0])\n","TRAIN_ENCODER_ONCE = cfg.get(\"train_encoder_once\", False)\n","\n","print(f\"ğŸ§¬ [DEBUG] DATASET_ID         = {DATASET_ID}\")\n","print(f\"ğŸ” [DEBUG] Folds              = {FOLDS}\")\n","print(f\"ğŸ”’ [DEBUG] train_encoder_once = {TRAIN_ENCODER_ONCE}\")\n","\n","# â”€â”€â”€ 4) Crea la directory dellâ€™esperimento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","exp_dir_rel = cfg[\"output\"][\"exp_dir\"].format(\n","    dataset_id=DATASET_ID,\n","    exp_code=EXP_CODE\n",")\n","# Use PROJECT_ROOT without early resolve to ensure Drive path\n","EXP_BASE = PROJECT_ROOT / exp_dir_rel\n","# Safety check: ensure EXP_BASE is under PROJECT_ROOT\n","if not str(EXP_BASE.resolve()).startswith(str(PROJECT_ROOT.resolve())):\n","    raise RuntimeError(f\"ğŸš¨ EXP_BASE NON sotto PROJECT_ROOT! â†’ {EXP_BASE}\")\n","# Create experiment directory\n","EXP_BASE.mkdir(parents=True, exist_ok=True)\n","print(f\"ğŸ“‚ [DEBUG] EXP_BASE â†’ {EXP_BASE.resolve()}\")\n","\n","# â”€â”€â”€ 5) Salva snapshot del file YAML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","snap = EXP_BASE / f\"training_{EXP_CODE}.yaml\"\n","if not snap.exists():\n","    snap.write_text(yaml.dump(cfg, sort_keys=False))\n","    print(f\"ğŸ’¾ [DEBUG] Salvato snapshot â†’ {snap.resolve()}\")\n","else:\n","    print(f\"â„¹ï¸  [DEBUG] Snapshot giÃ  presente â†’ {snap.resolve()}\")\n","\n","# â”€â”€â”€ 6) Imposta seed per la riproducibilitÃ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","SEED = cfg.get(\"seed\", 42)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","print(f\"ğŸ² [DEBUG] Seed globale impostato a: {SEED}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVDklAZ1Yurj","executionInfo":{"status":"aborted","timestamp":1752630876808,"user_tz":240,"elapsed":75,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"outputs":[],"source":["# ## Cell 3 â€“ Import & Registrazione dei Trainer\n","# Import dinamico dei modelli definiti in YAML e verifica del registry + tipo (SSL/SL).\n","import sys\n","import importlib\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","\n","# 1) Leggi i nomi dei modelli e i loro tipi da YAML\n","model_cfgs = cfg[\"models\"]\n","model_names = list(model_cfgs.keys())\n","print(f\"ğŸ”„ [DEBUG] Modelli configurati in YAML: {model_names}\")\n","\n","# 2) Import dinamico di ciascun modulo trainers.{model}\n","for name in model_names:\n","    module_name = f\"trainers.{name}\"\n","    try:\n","        if module_name in sys.modules:\n","            importlib.reload(sys.modules[module_name])\n","            print(f\"âœ… [DEBUG] Ricaricato {module_name}\")\n","        else:\n","            importlib.import_module(module_name)\n","            print(f\"âœ… [DEBUG] Importato {module_name}\")\n","    except ImportError as e:\n","        print(f\"âŒ [DEBUG] Errore importazione {module_name}: {e}\")\n","\n","# 3) Verifica che tutti i modelli siano registrati e mostra il loro tipo\n","missing = [n for n in model_names if n not in TRAINER_REGISTRY]\n","if missing:\n","    print(f\"âŒ [DEBUG] Trainer mancanti nel registry: {missing}\")\n","else:\n","    print(\"ğŸ“š [DEBUG] Trainer registry contiene e corrispondenti tipi:\")\n","    for name in model_names:\n","        # Usa solo il type dal file YAML per ogni modello\n","        ttype = model_cfgs[name].get(\"type\", \"unknown\").upper()\n","        print(f\"  â€¢ {name:<12} â†’ {ttype}\")\n","\n","# 3) Verifica funzioni in sottomoduli\n","import importlib, pkgutil, inspect\n","\n","print(\"ğŸ” [DEBUG] Sotto-moduli caricati da utils.training_utils:\")\n","for mod in (\"registry\", \"device_io\", \"data_utils\", \"model_utils\", \"metrics\"):\n","    m = importlib.import_module(f\"utils.training_utils.{mod}\")\n","    print(f\"  â€¢ {mod:<12} â†’ functions:\", [n for n, o in inspect.getmembers(m) if inspect.isfunction(o)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zie12NaWYurk","executionInfo":{"status":"aborted","timestamp":1752630876809,"user_tz":240,"elapsed":70,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"outputs":[],"source":["# %% -------------------------------------------------------------------- #\n","# Cell 4 â€“ Helper utilities (Tee, paths, selezione, â€¦)                    #\n","# ----------------------------------------------------------------------- #\n","import contextlib\n","import sys\n","import time\n","import math\n","import inspect\n","from pathlib import Path\n","from typing import Any\n","\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.device_io import (\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json,\n","    save_joblib,\n",")\n","from utils.training_utils.data_utils import count_samples\n","from trainers.train_classifier import train_classifier\n","from utils.training_utils.metrics import apply_temperature_scaling\n","\n","\n","class _Tee:\n","    \"\"\"\n","    Classe per duplicare lo stdout/stderr su piÃ¹ target (es. console + file).\n","    \"\"\"\n","    def __init__(self, *tgts):\n","        self.tgts = tgts\n","\n","    def write(self, data: str):\n","        for t in self.tgts:\n","            t.write(data)\n","            t.flush()\n","\n","    def flush(self):\n","        for t in self.tgts:\n","            t.flush()\n","\n","\n","def _global_experiments_append(line: str):\n","    \"\"\"\n","    Appende una riga al file esperimenti globale (esperiments.md).\n","    \"\"\"\n","    global_file = EXP_BASE.parent.parent / \"experiments.md\"\n","    with open(global_file, \"a\") as f:\n","        f.write(line.rstrip() + \"\\n\")\n","\n","\n","# %% ----------------------------------------------------------------------- #\n","# Cell 4 â€“ Helper utilities (Tee, paths, selezione, â€¦)                    #\n","# ------------------------------------------------------------------------ #\n","import os\n","from pathlib import Path\n","from typing import Any\n","\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.device_io import (\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json,\n","    save_joblib,\n",")\n","from utils.training_utils.data_utils import count_samples\n","from trainers.train_classifier import train_classifier\n","from utils.training_utils.metrics import apply_temperature_scaling\n","\n","\n","def _paths(cfg: dict, model: str, fold: int) -> dict[str, Path]:\n","    \"\"\"\n","    Costruisce tutti i path di output (training, inference, explain, aggregate, ecc.)\n","    per uno specifico modello e fold, sempre ancorati a PROJECT_ROOT.\n","    \"\"\"\n","    # Parametri di formattazione iniziali\n","    ph = {\n","        'dataset_id': cfg['data']['dataset_id'],\n","        'exp_code': cfg['exp_code'],\n","        'model_name': model,\n","        'fold_idx': fold,\n","    }\n","    # 1) exp_dir relativo\n","    rel_exp_dir = cfg['output']['exp_dir'].format(**ph)\n","    ph['exp_dir'] = rel_exp_dir\n","    # 2) exp_model_dir relativo (usa ph['exp_dir'])\n","    rel_exp_model_dir = cfg['output']['exp_model_dir'].format(**ph)\n","\n","    # Costruisci path assoluti sotto PROJECT_ROOT\n","    exp_dir       = (PROJECT_ROOT / rel_exp_dir)\n","    exp_model_dir = (PROJECT_ROOT / rel_exp_model_dir)\n","\n","    # Directory per training, inference, explain, aggregate, experiment level\n","    tr = exp_model_dir / f\"fold{fold}\" / \"training\"\n","    inf = exp_model_dir / f\"fold{fold}\" / \"inference\"\n","    ex = exp_model_dir / f\"fold{fold}\" / \"explain\"\n","    ag = exp_model_dir / \"_aggregate\"\n","    el = exp_dir / \"_experiment_level\"\n","\n","    # Creazione cartelle\n","    for d in (tr, inf, ex, ag, el):\n","        d.mkdir(parents=True, exist_ok=True)\n","\n","    # Ritorna i path assoluti per tutti gli artefatti\n","    return {\n","        'ckpt_dir':       tr.resolve(),\n","        'ckpt_tpl':       (tr / f\"{model}_bestepoch{{epoch:03d}}_fold{fold}.pt\").resolve(),\n","        'features':       (tr / f\"{model}_features_fold{fold}.pt\").resolve(),\n","        'features_train': (tr / f\"{model}_features_train_fold{fold}.pt\").resolve(),\n","        'features_val':   (tr / f\"{model}_features_val_fold{fold}.pt\").resolve(),\n","        'clf':            (tr / f\"{model}_classifier_fold{fold}.joblib\").resolve(),\n","        'scaler':         (tr / f\"{model}_ts_scaler_fold{fold}.joblib\").resolve(),\n","        'log':            (tr / f\"{model}_train_log_fold{fold}.md\").resolve(),\n","        'loss_json':      (tr / f\"{model}_train_valid_loss_fold{fold}.json\").resolve(),\n","\n","        'patch_preds':    (inf / f\"{model}_patch_preds_fold{fold}.pt\").resolve(),\n","        'patient_preds':  (inf / f\"{model}_patient_preds_fold{fold}.csv\").resolve(),\n","        'mc_logits':      (inf / f\"{model}_mc_logits_fold{fold}.npy\").resolve(),\n","        'metrics':        (inf / f\"{model}_metrics_fold{fold}.json\").resolve(),\n","\n","        'gradcam_dir':    ex.resolve(),\n","        'metadata_csv':   (ex / f\"{model}_metadata_gradcam_fold{fold}.csv\").resolve(),\n","\n","        'aggregate_metrics': (ag / f\"{model}_metrics.json\").resolve(),\n","        'aggregate_summary': (ag / f\"{model}_summary_agg.jpg\").resolve(),\n","\n","        'comparison_json':    (el / \"models_comparison.json\").resolve(),\n","        'comparison_img':     (el / \"models_comparison.jpg\").resolve(),\n","\n","        'readme':            (exp_dir / \"README_EXPERIMENT.md\").resolve(),\n","    }\n","\n","\n","\n","\n","def _completed(paths: dict[str, Path], is_ssl: bool) -> bool:\n","    \"\"\"\n","    Verifica se il training + artefatti SSL sono giÃ  stati completati.\n","    \"\"\"\n","    if not get_latest_checkpoint(paths[\"ckpt_dir\"]):\n","        return False\n","    if is_ssl:\n","        return all(paths[k].exists() for k in (\n","            \"features_train\", \"features_val\", \"clf\", \"scaler\", \"loss_json\"\n","        ))\n","    return True\n","\n","\n","def _select_models(cfg: dict) -> dict[str, dict[str, Any]]:\n","    \"\"\"\n","    Seleziona i modelli da eseguire in base a `run_models` o tutti i modelli.\n","    \"\"\"\n","    wanted = cfg.get(\"run_models\") or list(cfg[\"models\"].keys())\n","    return {name: cfg[\"models\"][name] for name in wanted}\n","\n","\n","def _init_trainer(name: str, m_cfg: dict, data_cfg: dict, ckpt_dir: Path):\n","    \"\"\"\n","    Inizializza il trainer registrato per nome.\n","    \"\"\"\n","    tr = TRAINER_REGISTRY[name](m_cfg, data_cfg)\n","    tr.ckpt_dir = ckpt_dir\n","    tr.m_cfg = m_cfg\n","    tr.data_cfg = data_cfg\n","    tr.is_ssl = m_cfg.get(\"type\", \"\").lower() == \"ssl\"\n","    return tr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptdtjmnBBJe8","executionInfo":{"status":"aborted","timestamp":1752630876810,"user_tz":240,"elapsed":69,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"outputs":[],"source":["# %% ----------------------------------------------------------------------- #\n","# Cell 5 â€“ Training loop                                                     #\n","# ----------------------------------------------------------------------- #\n","import math\n","import time\n","import inspect\n","from pathlib import Path\n","\n","import torch\n","import joblib\n","import numpy as np\n","\n","from utils.training_utils.device_io import get_latest_checkpoint, load_checkpoint, save_json\n","from utils.training_utils.data_utils import count_samples\n","from utils.training_utils.metrics import TemperatureScaler\n","from utils.training_utils.data_utils import extract_labels_from_keys\n","from trainers.train_classifier import train_classifier\n","\n","\n","def _run_full_training(trainer, paths: dict[str, Path], epochs: int):\n","    \"\"\"\n","    Full training loop with per-batch logging, ETA estimation,\n","    and unified support for SSL and SL trainers.\n","    \"\"\"\n","    is_ssl = getattr(trainer, \"is_ssl\", False)\n","    batch_size = int(trainer.m_cfg[\"training\"][\"batch_size\"])\n","\n","    # Compute total batches if not set\n","    if getattr(trainer, \"batches_train\", None) is None:\n","        n_samples = count_samples(Path(trainer.data_cfg[\"train\"]))\n","        trainer.batches_train = math.ceil(n_samples / batch_size)\n","    total_batches = trainer.batches_train\n","\n","    history: list[dict] = []\n","    for epoch in range(1, epochs + 1):\n","        start_time = time.time()\n","        loss_sum = 0.0\n","        corr_sum = 0\n","        seen = 0\n","        print(f\"[fold{trainer.cfg_fold}] â”€â”€ Epoch {epoch}/{epochs} â”€â”€\")\n","\n","        for i, batch in enumerate(trainer.train_loader, start=1):\n","            if i == 3:\n","              break\n","            # Support both signatures: train_step(batch) or train_step(x, y)\n","            sig = inspect.signature(trainer.train_step)\n","            result = (\n","                trainer.train_step(*batch)\n","                if len(sig.parameters) > 1\n","                else trainer.train_step(batch)\n","            )\n","            if len(result) == 4:\n","                _, loss, correct, bs = result\n","            else:\n","                loss, bs = result\n","                correct = 0\n","\n","            loss_sum += loss * bs\n","            corr_sum += correct\n","            seen += bs\n","\n","            pct = (i / total_batches) * 100\n","            eta = (time.time() - start_time) / i * (total_batches - i)\n","            msg = (\n","                f\"[fold{trainer.cfg_fold}] Batch {i}/{total_batches} \"\n","                f\"({pct:5.1f}%) | Loss {loss_sum/seen:.4f}\"\n","            )\n","            if not is_ssl:\n","                msg += f\" | Acc {corr_sum/seen:.3f}\"\n","            msg += f\" | ETA {eta:6.1f}s\"\n","            print(msg, flush=True)\n","\n","        train_loss = loss_sum / seen\n","        if not is_ssl:\n","            val_loss, val_acc = trainer.validate_epoch()\n","            trainer.post_epoch(epoch, val_acc)\n","            history.append({\n","                \"epoch\": epoch,\n","                \"train_loss\": train_loss,\n","                \"val_loss\": val_loss,\n","                \"val_acc\": val_acc,\n","            })\n","            print(f\"[fold{trainer.cfg_fold}] Val â†’ Loss {val_loss:.4f} | Acc {val_acc:.3f}\")\n","        else:\n","            trainer.post_epoch(epoch, train_loss)\n","            history.append({\"epoch\": epoch, \"train_loss\": train_loss})\n","            print(f\"[fold{trainer.cfg_fold}] Train â†’ Loss {train_loss:.4f}\")\n","\n","        elapsed = time.time() - start_time\n","        print(f\"[fold{trainer.cfg_fold}] â±  {elapsed:.1f}s\\n\")\n","\n","    # Save training history\n","    save_json(history, paths[\"loss_json\"])\n","\n","\n","def _resume_or_train(trainer, paths: dict[str, Path], epochs: int):\n","    \"\"\"\n","    Resume from last checkpoint if available, otherwise run full training.\n","    \"\"\"\n","    ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","    if ckpt:\n","        print(f\"[fold{trainer.cfg_fold}] â©  Resuming from {ckpt.name}\")\n","        model, optimizer = trainer.get_resume_model_and_optimizer()\n","        load_checkpoint(ckpt, model, optimizer)\n","    _run_full_training(trainer, paths, epochs)\n","\n","\n","def _ensure_ssl_artifacts(trainer, paths: dict[str, Path]):\n","    \"\"\"\n","    For SSL models:\n","      1) Extract train and val features\n","      2) Train a linear probe on train features\n","      3) Calibrate the probe via temperature scaling on val logits\n","    \"\"\"\n","    # 1) Extract train features\n","    if not paths[\"features_train\"].exists():\n","        trainer.extract_features_to(paths[\"features_train\"], split=\"train\")\n","\n","    # 2) Extract val features\n","    if not paths[\"features_val\"].exists():\n","        trainer.extract_features_to(paths[\"features_val\"], split=\"val\")\n","\n","    # 3) Train the linear classifier\n","    if not paths[\"clf\"].exists():\n","        train_classifier(str(paths[\"features_train\"]), str(paths[\"clf\"]))\n","\n","    # 4) Temperature scaling on validation logits\n","    if not paths[\"scaler\"].exists():\n","        # Load validation features\n","        val_data = torch.load(paths[\"features_val\"], map_location=\"cpu\")\n","        X_val = val_data[\"features\"].numpy()\n","        keys_val = val_data[\"keys\"]\n","\n","        # Load classifier and label encoder\n","        bundle = joblib.load(paths[\"clf\"])\n","        clf = bundle[\"model\"]\n","        le = bundle[\"label_encoder\"]\n","\n","        # Obtain logits or convert probs to logits\n","        if hasattr(clf, \"decision_function\"):\n","            logits = clf.decision_function(X_val)\n","        else:\n","            probs = clf.predict_proba(X_val)\n","            logits = np.log(probs + 1e-12)\n","\n","        # Extract labels from keys\n","        labels = extract_labels_from_keys(keys_val, le)\n","\n","        # Fit the TemperatureScaler\n","        scaler = TemperatureScaler().fit(logits, labels)\n","        joblib.dump(scaler, str(paths[\"scaler\"]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMV3BYIyE7FM","executionInfo":{"status":"aborted","timestamp":1752630876849,"user_tz":240,"elapsed":1,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"outputs":[],"source":["# %% -------------------------------------------------------------------- #\n","# Cell 6 â€“ Modular Launch & Auto-Recover (cross-fold)                     #\n","# ------------------------------------------------------------------------#\n","from pathlib import Path\n","import contextlib, sys, torch, os\n","from utils.training_utils.device_io import get_latest_checkpoint, load_checkpoint\n","\n","def launch_training(cfg: dict) -> None:\n","    \"\"\"Esegue training + generazione artefatti per tutti i modelli e fold.\"\"\"\n","    for name, m_cfg in _select_models(cfg).items():\n","        is_ssl = (m_cfg.get(\"type\") == \"ssl\")\n","        epochs = int(m_cfg[\"training\"][\"epochs\"])\n","        print(f\"\\nğŸš€  Modello '{name}'  ({'SSL' if is_ssl else 'SL'}) â€“ epochs={epochs}\")\n","\n","        for fold in cfg[\"folds\"]:\n","            # 1) Configurazione dati\n","            data_cfg = {\n","                \"train\": str((PROJECT_ROOT / cfg[\"data\"][\"train\"]\n","                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"], fold_idx=fold))\n","                             .resolve()),\n","                \"val\":   str((PROJECT_ROOT / cfg[\"data\"][\"val\"]\n","                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"], fold_idx=fold))\n","                             .resolve()),\n","                \"test\":  str((PROJECT_ROOT / cfg[\"data\"][\"test\"]\n","                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"]))\n","                             .resolve())\n","            }\n","            if not Path(data_cfg[\"train\"]).exists():\n","                print(f\"[fold{fold}] âš ï¸  shard train mancante â†’ skip\")\n","                continue\n","\n","            # 2) Path output + trainer\n","            paths   = _paths(cfg, name, fold)\n","            trainer = _init_trainer(name, m_cfg, data_cfg, paths[\"ckpt_dir\"])\n","            trainer.cfg_fold     = fold\n","            trainer.train_loader = trainer.build_loader(\"train\")\n","            if hasattr(trainer, \"validate_epoch\"):\n","                trainer.val_loader = trainer.build_loader(\"val\")\n","\n","            # 3) Logging su stdout + file\n","            with open(paths[\"log\"], \"a\") as logf, \\\n","                 contextlib.redirect_stdout(_Tee(sys.stdout, logf)), \\\n","                 contextlib.redirect_stderr(_Tee(sys.stderr, logf)):\n","\n","                print(f\"[fold{fold}] ğŸ“‚  ckpt dir      â†’ {paths['ckpt_dir'].resolve()}\")\n","                print(f\"[fold{fold}] ğŸ·   train shards  â†’ {Path(data_cfg['train']).resolve()}\")\n","                print(f\"[fold{fold}] ğŸš€  avvio trainer  â†’ '{name}'\")\n","\n","                # 3.1) Se abbiamo giÃ  tutti gli artefatti, salta interamente\n","                if _completed(paths, is_ssl):\n","                    print(f\"[fold{fold}] âš¡  Artefatti giÃ  presenti â†’ skip training + SSL pipeline\")\n","                else:\n","                    # 4) SSL: encoder once su fold0 oppure training completo\n","                    if is_ssl and cfg.get(\"train_encoder_once\", False) and fold > 0:\n","                        fold0_ckpt = get_latest_checkpoint(_paths(cfg, name, 0)[\"ckpt_dir\"])\n","                        if fold0_ckpt:\n","                            mdl, _ = trainer.get_resume_model_and_optimizer()\n","                            load_checkpoint(fold0_ckpt, mdl, None)\n","                            print(f\"[fold{fold}] âœ… encoder da fold0 â†’ {fold0_ckpt.resolve()}\")\n","                        else:\n","                            print(f\"[fold{fold}] âš ï¸  encoder fold0 mancante â†’ train completo\")\n","                            _resume_or_train(trainer, paths, epochs)\n","                    else:\n","                        _resume_or_train(trainer, paths, epochs)\n","\n","                    # 5) SSL â€“ Feature extraction, probe, T-scaling\n","                    if is_ssl:\n","                        _ensure_ssl_artifacts(trainer, paths)\n","\n","                # 6) Append a esperimenti globali (anche in caso di skip)\n","                latest_ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","                if latest_ckpt:\n","                    base_dir = EXP_BASE.parent.parent.resolve()\n","                    try:\n","                        rel_path = os.path.relpath(str(latest_ckpt.resolve()), str(base_dir))\n","                    except Exception:\n","                        rel_path = str(latest_ckpt.resolve())\n","                else:\n","                    rel_path = \"-\"\n","                _global_experiments_append(\n","                    f\"| {cfg['exp_code']} | {name} | fold{fold} | {epochs} | {rel_path} |\"\n","                )\n","\n","                print(f\"[fold{fold}] âœ…  completato\\n\")\n","\n","# â”€â”€â”€ Avvio automatico in Colab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","if IN_COLAB:\n","    launch_training(cfg)\n","else:\n","    print(\"â©  Ambiente locale: esegui manualmente  launch_training(cfg) per partire.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6196183d","executionInfo":{"status":"aborted","timestamp":1752630876851,"user_tz":240,"elapsed":29043,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"outputs":[],"source":["from pathlib import Path\n","\n","# Configura esperimento\n","EXP_CODE = \"20250709142129\"\n","ROOT = Path(\"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\")\n","EXP_DIR = ROOT / \"data/processed/dataset_9f30917e/experiments\" / EXP_CODE\n","\n","# Modelli\n","SSL_MODELS = {\"simclr\", \"rotation\", \"moco_v2\", \"jepa\"}\n","SL_MODELS  = {\"supervised\", \"transfer\"}\n","ALL_MODELS = SSL_MODELS | SL_MODELS\n","N_FOLDS = 2\n","\n","# File attesi per tipo di modello\n","FILES_PER_MODEL = {\n","    \"SSL\": [\n","        \"{model}_bestepoch*_fold{i}.pt\",\n","        \"{model}_train_log_fold{i}.md\",\n","        \"{model}_train_valid_loss_fold{i}.json\",\n","    ],\n","    \"SL\": [\n","        \"{model}_bestepoch*_fold{i}.pt\",\n","        \"{model}_train_log_fold{i}.md\",\n","        \"{model}_train_valid_loss_fold{i}.json\",\n","    ],\n","}\n","\n","missing = []\n","\n","for model in sorted(ALL_MODELS):\n","    model_type = \"SSL\" if model in SSL_MODELS else \"SL\"\n","    folds = [0] if model_type == \"SSL\" else list(range(N_FOLDS))\n","\n","    for i in folds:\n","        train_dir = EXP_DIR / model / f\"fold{i}\" / \"training\"\n","        for pattern in FILES_PER_MODEL[model_type]:\n","            pattern_path = pattern.format(model=model, i=i)\n","            matched = list(train_dir.glob(pattern_path))  # usa direttamente il pattern\n","            if not matched:\n","                missing.append(str(train_dir / pattern_path))\n","\n","# Stampa risultato\n","print(\"ğŸ“‚ Verifica artefatti TRAINING\\n\")\n","if not missing:\n","    print(\"âœ… Tutti i file richiesti sono presenti.\")\n","else:\n","    print(f\"âŒ Mancano {len(missing)} artefatti TRAINING:\\n\")\n","    for m in missing:\n","        print(\" â€¢\", m)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}