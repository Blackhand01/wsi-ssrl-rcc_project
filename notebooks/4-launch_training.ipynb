{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118033,
     "status": "ok",
     "timestamp": 1749305392367,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "8cOZCcsp0RCa",
    "outputId": "e1bb265d-82c7-45dc-a114-5cf52d792200"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49273) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "!pip install --quiet torch torchvision webdataset tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13301,
     "status": "ok",
     "timestamp": 1749305405756,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "CPkMc3ONzi0J",
    "outputId": "b0dee59a-0134-494c-e50a-e2e92b126243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ [DEBUG] Avvio configurazione ambiente...\n",
      "üíª [DEBUG] Ambiente locale rilevato (VSCode o simile).\n",
      "üìÅ [DEBUG] PROJECT_ROOT impostato a: /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\n",
      "üîß [DEBUG] Installazione pacchetti mancanti: ['pillow', 'pyyaml']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49275) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äì Environment Setup & Dependencies\n",
    "import os, sys, subprocess, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì¶ [DEBUG] Avvio configurazione ambiente...\")\n",
    "\n",
    "# --- Colab detection ---------------------------------------------------------#\n",
    "IN_COLAB = Path(\"/content\").exists()\n",
    "if IN_COLAB:\n",
    "    print(\"üìç [DEBUG] Ambiente Google Colab rilevato.\")\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "else:\n",
    "    print(\"üíª [DEBUG] Ambiente locale rilevato (VSCode o simile).\")\n",
    "\n",
    "# --- Project root ------------------------------------------------------------#\n",
    "ENV_PATHS = {\n",
    "    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n",
    "    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n",
    "}\n",
    "PROJECT_ROOT = Path(ENV_PATHS[\"colab\" if IN_COLAB else \"local\"]).resolve()\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "print(f\"üìÅ [DEBUG] PROJECT_ROOT impostato a: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- Dependencies (installa solo se mancano) ---------------------------------#\n",
    "def _pip_install(pkgs):\n",
    "    import importlib.util\n",
    "    missing = [p for p in pkgs if importlib.util.find_spec(p) is None]\n",
    "    if missing:\n",
    "        print(f\"üîß [DEBUG] Installazione pacchetti mancanti: {missing}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n",
    "    else:\n",
    "        print(\"‚úÖ [DEBUG] Tutti i pacchetti richiesti sono gi√† installati.\")\n",
    "\n",
    "_pip_install([\n",
    "    \"torch\", \"torchvision\", \"webdataset\", \"tqdm\",\n",
    "    \"pillow\", \"pyyaml\", \"joblib\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749210491818,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "lIancWXKKuOY",
    "outputId": "bec2e845-d808-445d-f65a-ad46274e2f06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 14\n",
      "python-dotenv could not parse statement starting at line 20\n",
      "python-dotenv could not parse statement starting at line 22\n",
      "python-dotenv could not parse statement starting at line 24\n",
      "python-dotenv could not parse statement starting at line 26\n",
      "python-dotenv could not parse statement starting at line 33\n",
      "python-dotenv could not parse statement starting at line 35\n",
      "python-dotenv could not parse statement starting at line 37\n",
      "python-dotenv could not parse statement starting at line 39\n",
      "python-dotenv could not parse statement starting at line 40\n",
      "python-dotenv could not parse statement starting at line 45\n",
      "python-dotenv could not parse statement starting at line 47\n",
      "python-dotenv could not parse statement starting at line 48\n",
      "python-dotenv could not parse statement starting at line 53\n",
      "python-dotenv could not parse statement starting at line 57\n",
      "python-dotenv could not parse statement starting at line 59\n",
      "python-dotenv could not parse statement starting at line 63\n",
      "python-dotenv could not parse statement starting at line 65\n",
      "python-dotenv could not parse statement starting at line 67\n",
      "python-dotenv could not parse statement starting at line 68\n",
      "python-dotenv could not parse statement starting at line 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Detected Colab=False, VSCode=True\n",
      "üîç Carico .env da /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/.env\n",
      "   ‚Ä¢ SSH target: mla_group_19@hpc-legion.polito.it:/home/mla_group_19/wsi-ssrl-rcc_project\n",
      "üìù Wrote sbatch script: /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/notebooks/hpc_submit.sh\n",
      "‚ùå SLURM submission failed:\n",
      "None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49277) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "ssh: Could not resolve hostname hpc-legion.polito.it: nodename nor servname provided, or not known\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ‚Äì SLURM Submission via SSH per locale VSCode (debug .env + rsync)\n",
    "import os, subprocess, traceback\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "\n",
    "# Detect VSCode vs Colab\n",
    "IN_COLAB  = Path(\"/content\").exists()\n",
    "# Use explicit check for not in Colab for VSCode specific logic\n",
    "IN_VSCODE = not IN_COLAB and bool(os.environ.get(\"VSCODE_PID\"))\n",
    "print(f\"üöÄ Detected Colab={IN_COLAB}, VSCode={IN_VSCODE}\")\n",
    "\n",
    "if IN_VSCODE:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "    # 1) Carica .env (ricerca automatica)\n",
    "    dotenv_path = find_dotenv()\n",
    "    if not dotenv_path:\n",
    "        raise FileNotFoundError(\"‚ùå Non ho trovato alcun .env! Mettilo nella root del progetto.\")\n",
    "    print(f\"üîç Carico .env da {dotenv_path}\")\n",
    "    load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "    # 2) Controlla le env vars\n",
    "    REMOTE_USER      = os.getenv(\"CLUSTER_USER\")\n",
    "    REMOTE_HOST      = os.getenv(\"CLUSTER_HOST\")\n",
    "    REMOTE_BASE_PATH = os.getenv(\"REMOTE_BASE_PATH\")\n",
    "    SBATCH_MODULE    = os.getenv(\"SBATCH_MODULE\", \"python/3.9\")\n",
    "    SBATCH_PARTITION = os.getenv(\"SBATCH_PARTITION\", \"global\")\n",
    "    MAIL_USER        = os.getenv(\"RESPONSABILE_EMAIL\", os.getenv(\"MEMBER_EMAIL\"))\n",
    "\n",
    "    missing = [v for v in (\"CLUSTER_USER\",\"CLUSTER_HOST\",\"REMOTE_BASE_PATH\") if not os.getenv(v)]\n",
    "    if missing:\n",
    "        raise KeyError(f\"üå± Mancano queste env vars: {missing}. Controlla il .env.\")\n",
    "\n",
    "    # 3) Prepara lo script sbatch locale per la sottomissione remota\n",
    "    LOCAL_SCRIPT = Path.cwd() / \"hpc_submit.sh\"\n",
    "    print(f\"   ‚Ä¢ SSH target: {REMOTE_USER}@{REMOTE_HOST}:{REMOTE_BASE_PATH}\")\n",
    "\n",
    "    # 4) Genera sbatch script\n",
    "    header = dedent(f\"\"\"\\\n",
    "        #!/bin/bash\n",
    "        #SBATCH --job-name=rcc_ssrl_launch\n",
    "        #SBATCH --ntasks=1\n",
    "        #SBATCH --cpus-per-task=4\n",
    "        #SBATCH --mem-per-cpu=4G\n",
    "        #SBATCH --time=2:00:00\n",
    "        #SBATCH --gres=gpu:1\n",
    "        #SBATCH --partition={SBATCH_PARTITION}\n",
    "        #SBATCH --output=%x_%j.out\n",
    "        #SBATCH --mail-type=END,FAIL\n",
    "        #SBATCH --mail-user={MAIL_USER}\n",
    "        #SBATCH --workdir={REMOTE_BASE_PATH}\n",
    "\n",
    "        module purge\n",
    "        module load {SBATCH_MODULE}\n",
    "\n",
    "        cd {REMOTE_BASE_PATH}\n",
    "    \"\"\")\n",
    "    header += f\"\\npython {PROJECT_ROOT}/4-launch_training.py --config config/training.yaml\\n\"\n",
    "\n",
    "    LOCAL_SCRIPT.write_text(header)\n",
    "    LOCAL_SCRIPT.chmod(0o755)\n",
    "    print(f\"üìù Wrote sbatch script: {LOCAL_SCRIPT}\")\n",
    "\n",
    "    try:\n",
    "        # 5) Crea cartella remota\n",
    "        subprocess.run(\n",
    "            [\"ssh\", f\"{REMOTE_USER}@{REMOTE_HOST}\", f\"mkdir -p {REMOTE_BASE_PATH}\"],\n",
    "            check=True\n",
    "        )\n",
    "        print(\"üîÑ Remote directory ensured\")\n",
    "\n",
    "        # 6) Sync progetto (esclude dati pesanti)\n",
    "        subprocess.run([\n",
    "            \"rsync\",\"-avz\",\"--delete\",\n",
    "            \"--exclude\",\"data/processed\",\n",
    "            f\"{PROJECT_ROOT}/\",\n",
    "            f\"{REMOTE_USER}@{REMOTE_HOST}:{REMOTE_BASE_PATH}/\"\n",
    "        ], check=True)\n",
    "        print(\"üîÑ Project synchronized via rsync\")\n",
    "\n",
    "        # 7) Sottometti job\n",
    "        res = subprocess.run(\n",
    "            [\"ssh\", f\"{REMOTE_USER}@{REMOTE_HOST}\",\n",
    "             f\"cd {REMOTE_BASE_PATH} && sbatch {LOCAL_SCRIPT.name}\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        print(f\"üîç sbatch stdout: {res.stdout.strip()}\")\n",
    "        print(f\"üì¨ Job submitted: {res.stdout.strip().split()[-1]}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"‚ùå SLURM submission failed:\")\n",
    "        print(e.stdout, e.stderr)\n",
    "    except Exception:\n",
    "        print(\"‚ùå Unexpected error:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SLURM integration skipped: non in locale VSCode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8UIj4XOZP6U9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loaded utils.training_utils from /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/src/utils/training_utils.py\n",
      "[DEBUG] Imported:\n",
      "  ‚Ä¢ TRAINER_REGISTRY keys: []\n",
      "  ‚Ä¢ get_latest_checkpoint ‚Üí <function get_latest_checkpoint at 0x3e80d36d0>\n",
      "  ‚Ä¢ load_checkpoint       ‚Üí <function load_checkpoint at 0x3e80d3640>\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ‚Äì Dynamic import of utils.training_utils\n",
    "import sys\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) locate & load the module file\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\" / \"training_utils.py\"\n",
    "spec       = importlib.util.spec_from_file_location(\"utils.training_utils\", str(utils_path))\n",
    "utils_mod  = importlib.util.module_from_spec(spec)     # type: ignore[arg-type]\n",
    "assert spec and spec.loader, f\"Cannot load spec for {utils_path}\"\n",
    "spec.loader.exec_module(utils_mod)                     # type: ignore[assignment]\n",
    "sys.modules[\"utils.training_utils\"] = utils_mod        # register in sys.modules\n",
    "print(f\"[DEBUG] Loaded utils.training_utils from {utils_path}\")\n",
    "\n",
    "# 2) import what we need\n",
    "from utils.training_utils import (\n",
    "    TRAINER_REGISTRY,\n",
    "    get_latest_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "\n",
    "print(\"[DEBUG] Imported:\")\n",
    "print(\"  ‚Ä¢ TRAINER_REGISTRY keys:\", list(TRAINER_REGISTRY.keys()))\n",
    "print(\"  ‚Ä¢ get_latest_checkpoint ‚Üí\", get_latest_checkpoint)\n",
    "print(\"  ‚Ä¢ load_checkpoint       ‚Üí\", load_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jwx0_Pr9E59j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] TRAIN ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/train/patches-0000.tar\n",
      "[DEBUG] VAL ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/val/patches-0000.tar\n",
      "[DEBUG] TEST ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test/patches-0000.tar\n",
      "[DEBUG] MODELS_DIR ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/models\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 ‚Äì Configuration & Directory Setup (formatted and absolute paths)\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load config file\n",
    "cfg_path   = PROJECT_ROOT / \"config\" / \"training.yaml\"\n",
    "cfg        = yaml.safe_load(cfg_path.read_text())\n",
    "DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n",
    "\n",
    "# 2. Format and resolve absolute paths for train/val/test\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    rel_path = cfg[\"data\"][split].format(dataset_id=DATASET_ID)\n",
    "    abs_path = (PROJECT_ROOT / rel_path).resolve()\n",
    "    cfg[\"data\"][split] = str(abs_path)\n",
    "    print(f\"[DEBUG] {split.upper()} ‚Üí {abs_path}\")\n",
    "\n",
    "# 3. Format and resolve models dir\n",
    "rel_models = cfg[\"output_dir\"].format(dataset_id=DATASET_ID)\n",
    "MODELS_DIR = (PROJECT_ROOT / rel_models).resolve()\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[DEBUG] MODELS_DIR ‚Üí {MODELS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Reloading module: trainers.simclr\n",
      "[DEBUG] Reloading module: trainers.moco_v2\n",
      "[DEBUG] Reloading module: trainers.rotation\n",
      "[DEBUG] Reloading module: trainers.jigsaw\n",
      "[DEBUG] Reloading module: trainers.supervised\n",
      "[DEBUG] Reloading module: trainers.transfer\n",
      "[DEBUG] Registered trainers: ['simclr', 'supervised', 'transfer']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ‚Äì Import all trainer modules with debug prints\n",
    "import importlib, sys\n",
    "from utils.training_utils import TRAINER_REGISTRY\n",
    "\n",
    "trainer_mods = [\n",
    "    \"trainers.simclr\",\n",
    "    \"trainers.moco_v2\",\n",
    "    \"trainers.rotation\",\n",
    "    \"trainers.jigsaw\",\n",
    "    \"trainers.supervised\",\n",
    "    \"trainers.transfer\",\n",
    "]\n",
    "\n",
    "for module_name in trainer_mods:\n",
    "    if module_name in sys.modules:\n",
    "        print(f\"[DEBUG] Reloading module: {module_name}\")\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "    else:\n",
    "        print(f\"[DEBUG] Importing module: {module_name}\")\n",
    "        importlib.import_module(module_name)\n",
    "\n",
    "print(f\"[DEBUG] Registered trainers: {list(TRAINER_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def save_artifact(subdir: str, filename: str) -> Path:\n",
    "    out_dir = MODELS_DIR / subdir\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir / filename\n",
    "\n",
    "def append_report(md_rel: Path, header: str, body: str):\n",
    "    md_abs = MODELS_DIR / md_rel\n",
    "    md_abs.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y‚Äë%m‚Äë%d %H:%M\")\n",
    "    with md_abs.open(\"a\") as f:\n",
    "        f.write(f\"\\n\\n### {header}  \\n*{ts}*\\n\\n{body}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMV3BYIyE7FM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49285) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps üöÄ  Starting training for model 'supervised'\n",
      "‚Üí Model config: {'backbone': 'resnet50', 'pretrained': False, 'training': {'epochs': 2, 'batch_size': 32, 'optimizer': 'adam', 'learning_rate': '1e-4', 'weight_decay': '1e-5'}}\n",
      "Epochs: 2 | Batch size: 32\n",
      "\n",
      "TOTAL BATCHES 47\n",
      "--- Epoch 1/2 ---\n",
      "  Batch 1/47 (2.1%) | Loss: 3.1586 | Acc: 0.000 | Elapsed: 8.2s | ETA: 375.8s\n",
      "  Batch 2/47 (4.3%) | Loss: 2.9020 | Acc: 0.000 | Elapsed: 12.7s | ETA: 285.4s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 146\u001b[0m\n\u001b[1;32m    141\u001b[0m             append_report(Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/classifier/classifier_report.md\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    142\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier addestrato\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    143\u001b[0m                           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_path\u001b[38;5;241m.\u001b[39mrelative_to(MODELS_DIR)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Cell 8 ‚Äì Run!\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[43mlaunch_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 69\u001b[0m, in \u001b[0;36mlaunch_training\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mtrain_loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     68\u001b[0m     sig    \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(trainer\u001b[38;5;241m.\u001b[39mtrain_step)\n\u001b[0;32m---> 69\u001b[0m     result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_step(batch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     72\u001b[0m         _, loss, correct, bs \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/Desktop/MLA/project/wsi-ssrl-rcc_project/src/trainers/supervised.py:134\u001b[0m, in \u001b[0;36mSupervisedTrainer.train_step\u001b[0;34m(self, imgs, labels)\u001b[0m\n\u001b[1;32m    132\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m    133\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    137\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/adam.py:414\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m             grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    417\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 7 ‚Äì launch_training() con logging dettagliato in checkpoint_report.md\n",
    "import torch\n",
    "import inspect\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from utils.training_utils import (\n",
    "    TRAINER_REGISTRY,\n",
    "    load_checkpoint,\n",
    "    get_latest_checkpoint,\n",
    ")\n",
    "from trainers.train_classifier import train_classifier\n",
    "\n",
    "def launch_training(cfg: dict) -> None:\n",
    "    run_model  = cfg.get(\"run_model\", \"all\").lower()\n",
    "    models_cfg = cfg[\"models\"]\n",
    "    tasks = models_cfg.items() if run_model == \"all\" else [(run_model, models_cfg[run_model])]\n",
    "\n",
    "    for name, m_cfg in tasks:\n",
    "        # --- Trainer setup ---------------------------------------------------- #\n",
    "        TrainerCls = TRAINER_REGISTRY.get(name)\n",
    "        if TrainerCls is None:\n",
    "            raise KeyError(f\"Trainer '{name}' non registrato\")\n",
    "        trainer = TrainerCls(m_cfg, cfg[\"data\"])\n",
    "        has_val  = hasattr(trainer, \"validate_epoch\")\n",
    "        epochs   = int(m_cfg[\"training\"][\"epochs\"])\n",
    "        batch_sz = int(m_cfg[\"training\"][\"batch_size\"])\n",
    "        device   = getattr(trainer, \"device\", \"cpu\")\n",
    "\n",
    "        print(f\"Device: {device} üöÄ  Starting training for model '{name}'\")\n",
    "        print(f\"‚Üí Model config: {m_cfg}\")\n",
    "        print(f\"Epochs: {epochs} | Batch size: {batch_sz}\\n\")\n",
    "\n",
    "        # --- Paths per artefatti ---------------------------------------------- #\n",
    "        ckpt_dir  = MODELS_DIR / f\"{name}/checkpoints\"\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ckpt_best = ckpt_dir / f\"{TrainerCls.__name__}_best.pt\"\n",
    "\n",
    "        feat_path = save_artifact(f\"{name}/features\",   f\"{name}_features.pt\")\n",
    "        clf_path  = save_artifact(f\"{name}/classifier\", f\"{name}_classifier.joblib\")\n",
    "        report_md = Path(f\"{name}/checkpoints/checkpoint_report.md\")\n",
    "\n",
    "        # --- Checkpoint gi√† presente? ---------------------------------------- #\n",
    "        latest_ckpt = get_latest_checkpoint(ckpt_dir, prefix=TrainerCls.__name__)\n",
    "        if latest_ckpt is not None:\n",
    "            print(f\"‚è≠Ô∏è  Checkpoint trovato per '{name}': {latest_ckpt.name} ‚Äì skip training\")\n",
    "            model = torch.nn.Sequential(trainer.encoder,\n",
    "                                        getattr(trainer, \"projector\", torch.nn.Identity()))\n",
    "            load_checkpoint(latest_ckpt, model=model)\n",
    "            trainer.encoder = model[0].to(trainer.device)\n",
    "            append_report(report_md, \"Checkpoint ri-usato\",\n",
    "                          f\"`{latest_ckpt.relative_to(MODELS_DIR)}`\")\n",
    "            skip_training = True\n",
    "        else:\n",
    "            skip_training = False\n",
    "\n",
    "        # --- Training loop ---------------------------------------------------- #\n",
    "        if not skip_training:\n",
    "            total_batches = getattr(trainer, \"batches_train\", None) or len(trainer.train_loader)\n",
    "            print(f\"TOTAL BATCHES {total_batches}\")\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                epoch_start = time.time()\n",
    "                running_loss, running_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "                print(f\"--- Epoch {epoch}/{epochs} ---\")\n",
    "                for i, batch in enumerate(trainer.train_loader, start=1):\n",
    "                    sig    = inspect.signature(trainer.train_step)\n",
    "                    result = trainer.train_step(batch) if len(sig.parameters) == 1 else trainer.train_step(*batch)\n",
    "\n",
    "                    if len(result) == 4:\n",
    "                        _, loss, correct, bs = result\n",
    "                    else:\n",
    "                        loss, bs = result\n",
    "                        correct = 0\n",
    "\n",
    "                    running_loss    += loss * bs\n",
    "                    running_correct += correct\n",
    "                    total_samples   += bs\n",
    "\n",
    "                    avg_loss = running_loss / total_samples\n",
    "                    avg_acc  = (running_correct / total_samples) if has_val else 0.0\n",
    "                    elapsed  = time.time() - epoch_start\n",
    "                    pct      = (i / total_batches) * 100\n",
    "                    eta      = (elapsed / i) * (total_batches - i)\n",
    "\n",
    "                    msg = f\"  Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {avg_loss:.4f}\"\n",
    "                    if has_val:\n",
    "                        msg += f\" | Acc: {avg_acc:.3f}\"\n",
    "                    msg += f\" | Elapsed: {elapsed:.1f}s | ETA: {eta:.1f}s\"\n",
    "                    print(msg)\n",
    "\n",
    "                if has_val:\n",
    "                    val_loss, val_acc = trainer.validate_epoch()\n",
    "                    print(f\"Val -> Loss: {val_loss:.4f} | Acc: {val_acc:.3f}\")\n",
    "                    metric = val_acc\n",
    "                else:\n",
    "                    val_loss = val_acc = None\n",
    "                    metric = running_loss / total_samples\n",
    "\n",
    "                trainer.post_epoch(epoch, metric)\n",
    "                epoch_time = time.time() - epoch_start\n",
    "                train_loss = running_loss / total_samples\n",
    "                train_acc  = running_correct / total_samples if has_val else None\n",
    "\n",
    "                print(f\"Epoch {epoch} completed in {epoch_time:.1f}s\\n\")\n",
    "\n",
    "                # --- Scrittura metriche in Markdown ---------------------------- #\n",
    "                metrics_md = (\n",
    "                    f\"| Epoch | Train Loss | Train Acc | Val Loss | Val Acc | Duration |\\n\"\n",
    "                    f\"|-------|------------|-----------|----------|---------|----------|\\n\"\n",
    "                    f\"| {epoch} \"\n",
    "                    f\"| {train_loss:.4f} \"\n",
    "                    f\"| {train_acc:.3f} \" if train_acc is not None else \"| n/a \"\n",
    "                )\n",
    "                metrics_md += (\n",
    "                    f\"| {val_loss:.4f} | {val_acc:.3f} \" if val_loss is not None else \"| n/a | n/a \"\n",
    "                )\n",
    "                metrics_md += f\"| {epoch_time:.1f}s |\"\n",
    "\n",
    "                append_report(report_md, f\"Epoch {epoch} summary\", metrics_md)\n",
    "\n",
    "            # Salva checkpoint finale\n",
    "            new_best = get_latest_checkpoint(ckpt_dir, prefix=TrainerCls.__name__)\n",
    "            if new_best and new_best != ckpt_best:\n",
    "                new_best.replace(ckpt_best)\n",
    "            append_report(report_md, \"Checkpoint salvato\",\n",
    "                          f\"`{ckpt_best.relative_to(MODELS_DIR)}`\")\n",
    "\n",
    "        # --- SSL: estrazione feature + classificatore -------------------------- #\n",
    "        if not has_val:\n",
    "            if not feat_path.exists():\n",
    "                trainer.extract_features_to(str(feat_path))\n",
    "            feat_shape = tuple(torch.load(feat_path)[\"features\"].shape)\n",
    "            append_report(Path(f\"{name}/features/features_report.md\"),\n",
    "                          \"Features estratte\",\n",
    "                          f\"`{feat_path.relative_to(MODELS_DIR)}` shape = {feat_shape}\")\n",
    "\n",
    "            print(f\"üîç Extracting & training classifier for '{name}'\")\n",
    "            train_classifier(str(feat_path), str(clf_path))\n",
    "            append_report(Path(f\"{name}/classifier/classifier_report.md\"),\n",
    "                          \"Classifier addestrato\",\n",
    "                          f\"`{clf_path.relative_to(MODELS_DIR)}`\")\n",
    "\n",
    "# Cell 8 ‚Äì Run!\n",
    "launch_training(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMUn+/uyACrBWAZGivnsuX",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wsi-ssrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
