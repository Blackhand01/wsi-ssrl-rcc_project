{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118033,
     "status": "ok",
     "timestamp": 1749305392367,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "8cOZCcsp0RCa",
    "outputId": "e1bb265d-82c7-45dc-a114-5cf52d792200"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(18973) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "!pip install --quiet torch torchvision webdataset tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13301,
     "status": "ok",
     "timestamp": 1749305405756,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "CPkMc3ONzi0J",
    "outputId": "b0dee59a-0134-494c-e50a-e2e92b126243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ [DEBUG] Avvio configurazione ambiente...\n",
      "üíª [DEBUG] Ambiente locale rilevato (VSCode o simile).\n",
      "üìÅ [DEBUG] PROJECT_ROOT impostato a: /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\n",
      "üîß [DEBUG] Installazione pacchetti mancanti: ['pillow', 'pyyaml']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(18975) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äì Environment Setup & Dependencies\n",
    "import os, sys, subprocess, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì¶ [DEBUG] Avvio configurazione ambiente...\")\n",
    "\n",
    "# --- Colab detection ---------------------------------------------------------#\n",
    "IN_COLAB = Path(\"/content\").exists()\n",
    "if IN_COLAB:\n",
    "    print(\"üìç [DEBUG] Ambiente Google Colab rilevato.\")\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "else:\n",
    "    print(\"üíª [DEBUG] Ambiente locale rilevato (VSCode o simile).\")\n",
    "\n",
    "# --- Project root ------------------------------------------------------------#\n",
    "ENV_PATHS = {\n",
    "    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n",
    "    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n",
    "}\n",
    "PROJECT_ROOT = Path(ENV_PATHS[\"colab\" if IN_COLAB else \"local\"]).resolve()\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "print(f\"üìÅ [DEBUG] PROJECT_ROOT impostato a: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- Dependencies (installa solo se mancano) ---------------------------------#\n",
    "def _pip_install(pkgs):\n",
    "    import importlib.util\n",
    "    missing = [p for p in pkgs if importlib.util.find_spec(p) is None]\n",
    "    if missing:\n",
    "        print(f\"üîß [DEBUG] Installazione pacchetti mancanti: {missing}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n",
    "    else:\n",
    "        print(\"‚úÖ [DEBUG] Tutti i pacchetti richiesti sono gi√† installati.\")\n",
    "\n",
    "_pip_install([\n",
    "    \"torch\", \"torchvision\", \"webdataset\", \"tqdm\",\n",
    "    \"pillow\", \"pyyaml\", \"joblib\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749210491818,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "lIancWXKKuOY",
    "outputId": "bec2e845-d808-445d-f65a-ad46274e2f06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 14\n",
      "python-dotenv could not parse statement starting at line 20\n",
      "python-dotenv could not parse statement starting at line 22\n",
      "python-dotenv could not parse statement starting at line 24\n",
      "python-dotenv could not parse statement starting at line 26\n",
      "python-dotenv could not parse statement starting at line 33\n",
      "python-dotenv could not parse statement starting at line 35\n",
      "python-dotenv could not parse statement starting at line 37\n",
      "python-dotenv could not parse statement starting at line 39\n",
      "python-dotenv could not parse statement starting at line 40\n",
      "python-dotenv could not parse statement starting at line 45\n",
      "python-dotenv could not parse statement starting at line 47\n",
      "python-dotenv could not parse statement starting at line 48\n",
      "python-dotenv could not parse statement starting at line 53\n",
      "python-dotenv could not parse statement starting at line 57\n",
      "python-dotenv could not parse statement starting at line 59\n",
      "python-dotenv could not parse statement starting at line 63\n",
      "python-dotenv could not parse statement starting at line 65\n",
      "python-dotenv could not parse statement starting at line 67\n",
      "python-dotenv could not parse statement starting at line 68\n",
      "python-dotenv could not parse statement starting at line 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Detected Colab=False, VSCode=True\n",
      "üîç Carico .env da /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/.env\n",
      "   ‚Ä¢ SSH target: mla_group_19@hpc-legion.polito.it:/home/mla_group_19/wsi-ssrl-rcc_project\n",
      "üìù Wrote sbatch script: /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/notebooks/hpc_submit.sh\n",
      "‚ùå SLURM submission failed:\n",
      "None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(18977) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "ssh: Could not resolve hostname hpc-legion.polito.it: nodename nor servname provided, or not known\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 ‚Äì SLURM Submission via SSH per locale VSCode (debug .env + rsync)\n",
    "import os, subprocess, traceback\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "\n",
    "# Detect VSCode vs Colab\n",
    "IN_COLAB  = Path(\"/content\").exists()\n",
    "# Use explicit check for not in Colab for VSCode specific logic\n",
    "IN_VSCODE = not IN_COLAB and bool(os.environ.get(\"VSCODE_PID\"))\n",
    "print(f\"üöÄ Detected Colab={IN_COLAB}, VSCode={IN_VSCODE}\")\n",
    "\n",
    "if IN_VSCODE:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "    # 1) Carica .env (ricerca automatica)\n",
    "    dotenv_path = find_dotenv()\n",
    "    if not dotenv_path:\n",
    "        raise FileNotFoundError(\"‚ùå Non ho trovato alcun .env! Mettilo nella root del progetto.\")\n",
    "    print(f\"üîç Carico .env da {dotenv_path}\")\n",
    "    load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "    # 2) Controlla le env vars\n",
    "    REMOTE_USER      = os.getenv(\"CLUSTER_USER\")\n",
    "    REMOTE_HOST      = os.getenv(\"CLUSTER_HOST\")\n",
    "    REMOTE_BASE_PATH = os.getenv(\"REMOTE_BASE_PATH\")\n",
    "    SBATCH_MODULE    = os.getenv(\"SBATCH_MODULE\", \"python/3.9\")\n",
    "    SBATCH_PARTITION = os.getenv(\"SBATCH_PARTITION\", \"global\")\n",
    "    MAIL_USER        = os.getenv(\"RESPONSABILE_EMAIL\", os.getenv(\"MEMBER_EMAIL\"))\n",
    "\n",
    "    missing = [v for v in (\"CLUSTER_USER\",\"CLUSTER_HOST\",\"REMOTE_BASE_PATH\") if not os.getenv(v)]\n",
    "    if missing:\n",
    "        raise KeyError(f\"üå± Mancano queste env vars: {missing}. Controlla il .env.\")\n",
    "\n",
    "    # 3) Prepara lo script sbatch locale per la sottomissione remota\n",
    "    LOCAL_SCRIPT = Path.cwd() / \"hpc_submit.sh\"\n",
    "    print(f\"   ‚Ä¢ SSH target: {REMOTE_USER}@{REMOTE_HOST}:{REMOTE_BASE_PATH}\")\n",
    "\n",
    "    # 4) Genera sbatch script\n",
    "    header = dedent(f\"\"\"\\\n",
    "        #!/bin/bash\n",
    "        #SBATCH --job-name=rcc_ssrl_launch\n",
    "        #SBATCH --ntasks=1\n",
    "        #SBATCH --cpus-per-task=4\n",
    "        #SBATCH --mem-per-cpu=4G\n",
    "        #SBATCH --time=2:00:00\n",
    "        #SBATCH --gres=gpu:1\n",
    "        #SBATCH --partition={SBATCH_PARTITION}\n",
    "        #SBATCH --output=%x_%j.out\n",
    "        #SBATCH --mail-type=END,FAIL\n",
    "        #SBATCH --mail-user={MAIL_USER}\n",
    "        #SBATCH --workdir={REMOTE_BASE_PATH}\n",
    "\n",
    "        module purge\n",
    "        module load {SBATCH_MODULE}\n",
    "\n",
    "        cd {REMOTE_BASE_PATH}\n",
    "    \"\"\")\n",
    "    header += f\"\\npython {PROJECT_ROOT}/4-launch_training.py --config config/training.yaml\\n\"\n",
    "\n",
    "    LOCAL_SCRIPT.write_text(header)\n",
    "    LOCAL_SCRIPT.chmod(0o755)\n",
    "    print(f\"üìù Wrote sbatch script: {LOCAL_SCRIPT}\")\n",
    "\n",
    "    try:\n",
    "        # 5) Crea cartella remota\n",
    "        subprocess.run(\n",
    "            [\"ssh\", f\"{REMOTE_USER}@{REMOTE_HOST}\", f\"mkdir -p {REMOTE_BASE_PATH}\"],\n",
    "            check=True\n",
    "        )\n",
    "        print(\"üîÑ Remote directory ensured\")\n",
    "\n",
    "        # 6) Sync progetto (esclude dati pesanti)\n",
    "        subprocess.run([\n",
    "            \"rsync\",\"-avz\",\"--delete\",\n",
    "            \"--exclude\",\"data/processed\",\n",
    "            f\"{PROJECT_ROOT}/\",\n",
    "            f\"{REMOTE_USER}@{REMOTE_HOST}:{REMOTE_BASE_PATH}/\"\n",
    "        ], check=True)\n",
    "        print(\"üîÑ Project synchronized via rsync\")\n",
    "\n",
    "        # 7) Sottometti job\n",
    "        res = subprocess.run(\n",
    "            [\"ssh\", f\"{REMOTE_USER}@{REMOTE_HOST}\",\n",
    "             f\"cd {REMOTE_BASE_PATH} && sbatch {LOCAL_SCRIPT.name}\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        print(f\"üîç sbatch stdout: {res.stdout.strip()}\")\n",
    "        print(f\"üì¨ Job submitted: {res.stdout.strip().split()[-1]}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"‚ùå SLURM submission failed:\")\n",
    "        print(e.stdout, e.stderr)\n",
    "    except Exception:\n",
    "        print(\"‚ùå Unexpected error:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SLURM integration skipped: non in locale VSCode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8UIj4XOZP6U9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loaded utils.training_utils from /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/src/utils/training_utils.py\n",
      "[DEBUG] Imported:\n",
      "  ‚Ä¢ TRAINER_REGISTRY keys: []\n",
      "  ‚Ä¢ get_latest_checkpoint ‚Üí <function get_latest_checkpoint at 0x3021fa0e0>\n",
      "  ‚Ä¢ load_checkpoint       ‚Üí <function load_checkpoint at 0x3021fa050>\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ‚Äì Dynamic import of utils.training_utils\n",
    "import sys\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) locate & load the module file\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\" / \"training_utils.py\"\n",
    "spec       = importlib.util.spec_from_file_location(\"utils.training_utils\", str(utils_path))\n",
    "utils_mod  = importlib.util.module_from_spec(spec)     # type: ignore[arg-type]\n",
    "assert spec and spec.loader, f\"Cannot load spec for {utils_path}\"\n",
    "spec.loader.exec_module(utils_mod)                     # type: ignore[assignment]\n",
    "sys.modules[\"utils.training_utils\"] = utils_mod        # register in sys.modules\n",
    "print(f\"[DEBUG] Loaded utils.training_utils from {utils_path}\")\n",
    "\n",
    "# 2) import what we need\n",
    "from utils.training_utils import (\n",
    "    TRAINER_REGISTRY,\n",
    "    get_latest_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "\n",
    "print(\"[DEBUG] Imported:\")\n",
    "print(\"  ‚Ä¢ TRAINER_REGISTRY keys:\", list(TRAINER_REGISTRY.keys()))\n",
    "print(\"  ‚Ä¢ get_latest_checkpoint ‚Üí\", get_latest_checkpoint)\n",
    "print(\"  ‚Ä¢ load_checkpoint       ‚Üí\", load_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jwx0_Pr9E59j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] EXP_CODE ‚Üí 20250624121016\n",
      "[DEBUG] TRAIN ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/train/patches-0000.tar\n",
      "[DEBUG] VAL ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/val/patches-0000.tar\n",
      "[DEBUG] TEST ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test/patches-0000.tar\n",
      "[DEBUG] EXP_BASE ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250624121016\n",
      "[DEBUG] Config gi√† presente ‚Üí /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250624121016/training_20250624121016.yaml\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 4 ‚Äì Configuration & Directory Setup (formatted and absolute paths)\n",
    "import yaml, datetime, os\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 0) EXP_CODE: ri-usa se esiste, altrimenti crealo e memorizzalo     #\n",
    "# ------------------------------------------------------------------ #\n",
    "if \"EXP_CODE\" in os.environ:          # questa variabile resta valida finch√© dura il kernel\n",
    "    EXP_CODE = os.environ[\"EXP_CODE\"]\n",
    "else:\n",
    "    EXP_CODE = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    os.environ[\"EXP_CODE\"] = EXP_CODE            # rende il codice ri-utilizzabile\n",
    "\n",
    "print(f\"[DEBUG] EXP_CODE ‚Üí {EXP_CODE}\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1) Carica configurazione generale                                  #\n",
    "# ------------------------------------------------------------------ #\n",
    "cfg_path  = PROJECT_ROOT / \"config\" / \"training.yaml\"\n",
    "cfg       = yaml.safe_load(cfg_path.read_text())\n",
    "DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n",
    "cfg[\"experiment_code\"] = EXP_CODE     # lo inseriamo nel dict per eventuali usi downstream\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2) Percorsi assoluti (train / val / test)                          #\n",
    "# ------------------------------------------------------------------ #\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    rel = cfg[\"data\"][split].format(dataset_id=DATASET_ID)\n",
    "    abs_ = (PROJECT_ROOT / rel).resolve()\n",
    "    cfg[\"data\"][split] = str(abs_)\n",
    "    print(f\"[DEBUG] {split.upper()} ‚Üí {abs_}\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3) Struttura directory esperimento                                 #\n",
    "# ------------------------------------------------------------------ #\n",
    "EXP_ROOT = PROJECT_ROOT / \"data\" / \"processed\" / str(DATASET_ID)\n",
    "EXP_BASE = EXP_ROOT / \"experiments\" / EXP_CODE                   # unica per tutta la run\n",
    "EXP_BASE.mkdir(parents=True, exist_ok=True)\n",
    "(EXP_ROOT / \"experiments.md\").touch(exist_ok=True)               # indice globale\n",
    "\n",
    "print(f\"[DEBUG] EXP_BASE ‚Üí {EXP_BASE}\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 4) Salva la **copia YAML** solo se non esiste gi√†                  #\n",
    "# ------------------------------------------------------------------ #\n",
    "exp_yaml = EXP_BASE / f\"training_{EXP_CODE}.yaml\"\n",
    "if not exp_yaml.exists():                          # evita duplicati\n",
    "    exp_yaml.write_text(yaml.dump(cfg, sort_keys=False))\n",
    "    print(f\"[DEBUG] Scritto   {exp_yaml}\")\n",
    "else:\n",
    "    print(f\"[DEBUG] Config gi√† presente ‚Üí {exp_yaml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Reloading module: trainers.simclr\n",
      "[DEBUG] Reloading module: trainers.moco_v2\n",
      "[DEBUG] Reloading module: trainers.rotation\n",
      "[DEBUG] Reloading module: trainers.jigsaw\n",
      "[DEBUG] Reloading module: trainers.supervised\n",
      "[DEBUG] Reloading module: trainers.transfer\n",
      "[DEBUG] Registered trainers: ['simclr', 'moco_v2', 'rotation', 'jigsaw', 'supervised', 'transfer']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ‚Äì Import all trainer modules with debug prints\n",
    "import importlib, sys\n",
    "from utils.training_utils import TRAINER_REGISTRY\n",
    "\n",
    "trainer_mods = [\n",
    "    \"trainers.simclr\",\n",
    "    \"trainers.moco_v2\",\n",
    "    \"trainers.rotation\",\n",
    "    \"trainers.jigsaw\",\n",
    "    \"trainers.supervised\",\n",
    "    \"trainers.transfer\",\n",
    "]\n",
    "\n",
    "for module_name in trainer_mods:\n",
    "    if module_name in sys.modules:\n",
    "        print(f\"[DEBUG] Reloading module: {module_name}\")\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "    else:\n",
    "        print(f\"[DEBUG] Importing module: {module_name}\")\n",
    "        importlib.import_module(module_name)\n",
    "\n",
    "print(f\"[DEBUG] Registered trainers: {list(TRAINER_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 6 ‚Äì Helpers (tee-logger + path)\n",
    "import contextlib, sys\n",
    "\n",
    "class _Tee:\n",
    "    \"\"\"Duplica stdout/stderr su console e file.\"\"\"\n",
    "    def __init__(self, *targets): self.targets = targets\n",
    "    def write(self, data):  [t.write(data) and t.flush() for t in self.targets]\n",
    "    def flush(self):        [t.flush() for t in self.targets]\n",
    "\n",
    "def _prepare_model_dir(model_name: str):\n",
    "    \"\"\"\n",
    "    Crea   data/processed/<dataset_id>/experiments/<EXP_CODE>/<model_name>/ \n",
    "    e restituisce (model_dir, log_file_path).\n",
    "    \"\"\"\n",
    "    model_dir = EXP_BASE / model_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_file  = model_dir / f\"log_{EXP_CODE}.md\"\n",
    "    return model_dir, log_file\n",
    "\n",
    "def _global_experiments_append(line: str):\n",
    "    \"\"\"Aggiunge una riga al file   data/processed/<dataset_id>/experiments.md\"\"\"\n",
    "    exp_md = EXP_ROOT / \"experiments.md\"\n",
    "    with exp_md.open(\"a\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rMV3BYIyE7FM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefanoroybisignano/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps üöÄ  Starting training for model 'simclr'\n",
      "‚Üí Model config: {'backbone': 'resnet18', 'proj_dim': 128, 'augmentation': {'enabled': True, 'horizontal_flip': True, 'rotation': [0, 90, 180, 270], 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}}, 'training': {'epochs': 2, 'batch_size': 64, 'optimizer': 'adam', 'learning_rate': '1e-3', 'weight_decay': '1e-5', 'temperature': 0.5}}\n",
      "Epochs: 2 | Batch size: 64\n",
      "\n",
      "TOTAL BATCHES 24\n",
      "--- Epoch 1/2 ---\n",
      "  Batch 1/24 (4.2%) | Loss: 4.8470 | Elapsed: 9.1s | ETA: 210.0s\n",
      "  Batch 2/24 (8.3%) | Loss: 4.9179 | Elapsed: 11.8s | ETA: 130.0s\n",
      "  Batch 3/24 (12.5%) | Loss: 4.8866 | Elapsed: 16.4s | ETA: 114.6s\n",
      "  Batch 4/24 (16.7%) | Loss: 4.8858 | Elapsed: 21.6s | ETA: 108.0s\n",
      "  Batch 5/24 (20.8%) | Loss: 4.8779 | Elapsed: 26.2s | ETA: 99.6s\n",
      "  Batch 6/24 (25.0%) | Loss: 4.8593 | Elapsed: 29.2s | ETA: 87.5s\n",
      "  Batch 7/24 (29.2%) | Loss: 4.8455 | Elapsed: 32.0s | ETA: 77.7s\n",
      "  Batch 8/24 (33.3%) | Loss: 4.8336 | Elapsed: 34.6s | ETA: 69.2s\n",
      "  Batch 9/24 (37.5%) | Loss: 4.8148 | Elapsed: 37.1s | ETA: 61.8s\n",
      "  Batch 10/24 (41.7%) | Loss: 4.8108 | Elapsed: 39.6s | ETA: 55.4s\n",
      "  Batch 11/24 (45.8%) | Loss: 4.8082 | Elapsed: 42.5s | ETA: 50.2s\n",
      "  Batch 12/24 (50.0%) | Loss: 4.8253 | Elapsed: 45.3s | ETA: 45.3s\n",
      "  Batch 13/24 (54.2%) | Loss: 4.8297 | Elapsed: 49.8s | ETA: 42.2s\n",
      "  Batch 14/24 (58.3%) | Loss: 4.8300 | Elapsed: 53.0s | ETA: 37.8s\n",
      "  Batch 15/24 (62.5%) | Loss: 4.8335 | Elapsed: 55.9s | ETA: 33.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 136\u001b[0m\n\u001b[1;32m    131\u001b[0m             _global_experiments_append(\n\u001b[1;32m    132\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXP_CODE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# avvia\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[43mlaunch_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m, in \u001b[0;36mlaunch_training\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mtrain_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     70\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(trainer\u001b[38;5;241m.\u001b[39mtrain_step)\n\u001b[0;32m---> 71\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \\\n\u001b[1;32m     72\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtrain_step(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     75\u001b[0m         _, loss, corr, bs \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/Desktop/MLA/project/wsi-ssrl-rcc_project/src/trainers/simclr.py:181\u001b[0m, in \u001b[0;36mSimCLRTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(z1, z2)\n\u001b[1;32m    180\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem()), x1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/site-packages/torch/optim/adam.py:414\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m             grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    417\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 7 ‚Äì launch_training() con gestione WebDataset e nuova struttura\n",
    "import torch, inspect, time, contextlib, sys\n",
    "from utils.training_utils import TRAINER_REGISTRY, load_checkpoint, get_latest_checkpoint\n",
    "from trainers.train_classifier import train_classifier\n",
    "\n",
    "def launch_training(cfg: dict) -> None:\n",
    "    sel = cfg.get(\"run_model\", \"all\").lower()\n",
    "    todo = {n: c for n, c in cfg[\"models\"].items()\n",
    "            if sel in (\"all\", n)\n",
    "            or (sel == \"supervised\"     and c.get(\"type\") == \"supervised\")\n",
    "            or (sel == \"selfsupervised\" and c.get(\"type\") == \"selfsupervised\")}\n",
    "\n",
    "    for name, m_cfg in todo.items():\n",
    "        # ---------- directory & log ---------------------------------------- #\n",
    "        model_dir, log_file = _prepare_model_dir(name)\n",
    "        with open(log_file, \"w\") as lf, \\\n",
    "             contextlib.redirect_stdout(_Tee(sys.stdout, lf)), \\\n",
    "             contextlib.redirect_stderr(_Tee(sys.stderr, lf)):\n",
    "\n",
    "            # ---- trainer init --------------------------------------------- #\n",
    "            TrainerCls = TRAINER_REGISTRY.get(name)\n",
    "            if TrainerCls is None:\n",
    "                raise KeyError(f\"Trainer '{name}' non registrato\")\n",
    "\n",
    "            trainer   = TrainerCls(m_cfg, cfg[\"data\"])\n",
    "            has_val   = hasattr(trainer, \"validate_epoch\")\n",
    "            epochs    = int(m_cfg[\"training\"][\"epochs\"])\n",
    "            batch_sz  = int(m_cfg[\"training\"][\"batch_size\"])\n",
    "            device    = getattr(trainer, \"device\", \"cpu\")\n",
    "\n",
    "            print(f\"Device: {device} üöÄ  Starting training for model '{name}'\")\n",
    "            print(f\"‚Üí Model config: {m_cfg}\")\n",
    "            print(f\"Epochs: {epochs} | Batch size: {batch_sz}\\n\")\n",
    "\n",
    "            # ---- artefatti -------------------------------------------------- #\n",
    "            ckpt_best = model_dir / f\"{name}_best.pt\"\n",
    "            feat_path = model_dir / f\"{name}_features.pt\"\n",
    "            clf_path  = model_dir / f\"{name}_classifier.joblib\"\n",
    "\n",
    "            # ---- resume ----------------------------------------------------- #\n",
    "            latest_ckpt = get_latest_checkpoint(model_dir, prefix=name)\n",
    "            if latest_ckpt:\n",
    "                print(f\"‚è≠Ô∏è  Checkpoint trovato: {latest_ckpt.name} ‚Äì skip training\")\n",
    "                model = torch.nn.Sequential(\n",
    "                    trainer.encoder,\n",
    "                    getattr(trainer, \"projector\", torch.nn.Identity())\n",
    "                )\n",
    "                load_checkpoint(latest_ckpt, model=model)\n",
    "                trainer.encoder = model[0].to(trainer.device)\n",
    "            else:\n",
    "                # ---------- training loop ----------------------------------- #\n",
    "                total_batches = getattr(trainer, \"batches_train\", None)\n",
    "                if total_batches is None:\n",
    "                    try:\n",
    "                        total_batches = len(trainer.train_loader)\n",
    "                    except TypeError:\n",
    "                        total_batches = None  # WebDataset / IterableDataset\n",
    "                if total_batches:\n",
    "                    print(f\"TOTAL BATCHES {total_batches}\")\n",
    "                else:\n",
    "                    print(\"TOTAL BATCHES unknown (IterableDataset)\")\n",
    "\n",
    "                for epoch in range(1, epochs + 1):\n",
    "                    t0 = time.time()\n",
    "                    run_loss = run_corr = seen = 0\n",
    "\n",
    "                    print(f\"--- Epoch {epoch}/{epochs} ---\")\n",
    "                    for i, batch in enumerate(trainer.train_loader, 1):\n",
    "                        sig = inspect.signature(trainer.train_step)\n",
    "                        res = trainer.train_step(batch) if len(sig.parameters) == 1 \\\n",
    "                              else trainer.train_step(*batch)\n",
    "\n",
    "                        if len(res) == 4:\n",
    "                            _, loss, corr, bs = res\n",
    "                        else:\n",
    "                            loss, bs = res; corr = 0\n",
    "\n",
    "                        run_loss += loss * bs\n",
    "                        run_corr += corr\n",
    "                        seen     += bs\n",
    "\n",
    "                        # --- progress bar only if tot batch noto ------------- #\n",
    "                        if total_batches:\n",
    "                            pct = (i / total_batches) * 100\n",
    "                            eta = ((time.time() - t0) / i) * (total_batches - i)\n",
    "                            msg = (f\"  Batch {i}/{total_batches} ({pct:.1f}%) | \"\n",
    "                                   f\"Loss: {run_loss/seen:.4f}\")\n",
    "                            if has_val:\n",
    "                                msg += f\" | Acc: {run_corr/seen:.3f}\"\n",
    "                            msg += f\" | Elapsed: {time.time()-t0:.1f}s | ETA: {eta:.1f}s\"\n",
    "                        else:  # no len()\n",
    "                            msg = (f\"  Batch {i} | Loss: {run_loss/seen:.4f}\")\n",
    "                            if has_val:\n",
    "                                msg += f\" | Acc: {run_corr/seen:.3f}\"\n",
    "                            msg += f\" | Elapsed: {time.time()-t0:.1f}s\"\n",
    "                        print(msg)\n",
    "\n",
    "                    # ---------- validation / metric ------------------------- #\n",
    "                    val_loss = val_acc = None\n",
    "                    if has_val:\n",
    "                        val_loss, val_acc = trainer.validate_epoch()\n",
    "                        metric = val_acc\n",
    "                        print(f\"Val -> Loss: {val_loss:.4f} | Acc: {val_acc:.3f}\")\n",
    "                    else:\n",
    "                        metric = run_loss / seen\n",
    "\n",
    "                    trainer.post_epoch(epoch, metric)\n",
    "                    print(f\"Epoch {epoch} completed in {time.time()-t0:.1f}s\\n\")\n",
    "\n",
    "                # ---------- save best checkpoint ---------------------------- #\n",
    "                best = get_latest_checkpoint(model_dir, prefix=name)\n",
    "                if best and best != ckpt_best:\n",
    "                    best.replace(ckpt_best)\n",
    "\n",
    "            # ---------- SSL: feature & classifier --------------------------- #\n",
    "            if m_cfg.get(\"type\") == \"selfsupervised\":\n",
    "                if not feat_path.exists():\n",
    "                    trainer.extract_features_to(str(feat_path))\n",
    "                print(f\"üîç Extracting & training classifier for '{name}'\")\n",
    "                train_classifier(str(feat_path), str(clf_path))\n",
    "\n",
    "            # ---------- aggiorna experiments.md globale ------------------------- #\n",
    "            if ckpt_best.exists():\n",
    "                rel = ckpt_best.relative_to(EXP_ROOT)\n",
    "            elif latest_ckpt:\n",
    "                rel = latest_ckpt.relative_to(EXP_ROOT)\n",
    "            else:\n",
    "                rel = \"-\"  # nessun checkpoint trovato\n",
    "\n",
    "            _global_experiments_append(\n",
    "                f\"| {EXP_CODE} | {name} | {epochs} | {rel} |\"\n",
    "            )\n",
    "\n",
    "# avvia\n",
    "launch_training(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMUn+/uyACrBWAZGivnsuX",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wsi-ssrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
