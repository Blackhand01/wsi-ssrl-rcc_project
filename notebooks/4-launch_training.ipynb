{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1004,"status":"ok","timestamp":1753293959113,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"},"user_tz":-120},"id":"2GFLbAUPJoOq","outputId":"c5366694-7a77-4675-b6c2-040300d74ce9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"bi1bbemBZE99"},"source":["## 1. ğŸ§  Self-Supervised Models (SimCLR, MoCo, Rotation, JEPA)\n","\n","### 1. Pre-training\n","\n","* Allena lâ€™**encoder una sola volta**, su tutti i pazienti di `train_fold0`.\n","* Salva i checkpoint (encoder + projector).\n","\n","### 2. Estrazione delle feature\n","\n","* Carica il **checkpoint fisso** del pretraining.\n","* Estrai le feature su tutti i fold (`train_fold{i}`, `val_fold{i}`, `test_holdout`).\n","* Salva i file come `{model}_features_fold{i}.pt`.\n","\n","### 3. Linear-Probe per ogni fold `i âˆˆ [0, ..., N-1]`\n","\n","* **Train**: allena la testa lineare (LogReg, MLP) sulle feature di `train_fold{i}`.\n","* **Validation**: usa `val_fold{i}` per:\n","\n","  * early stopping,\n","  * salvare il miglior probe,\n","  * applicare **Temperature Scaling**.\n","* **Test**: valuta il probe calibrato su `test_holdout`, usando anche **MC-Dropout**.\n","\n","### 4. Aggregazione finale\n","\n","* Raccogli i risultati su `test_holdout` per ogni fold.\n","* Calcola **media Â± deviazione standard** per ogni metrica (Accuracy, F1, AUC, ECE, incertezza).\n","* Questo Ã¨ il risultato finale del tuo modello SSL + probe.\n","\n","> ğŸ”’ **Non riaddestrare lâ€™encoder nei fold 1â€“N.**\n","> Lo scopo Ã¨ testarne la **capacitÃ  di generalizzazione task-agnostica**, non adattarlo a ogni fold.\n","\n","---\n","\n","## 2. ğŸ§ª Supervised & Transfer Learning\n","\n","Qui **non hai un encoder fisso**. Ogni fold ha il proprio training da zero.\n","\n","### Per ciascun fold `i âˆˆ [0, ..., N-1]`\n","\n","* **Train**: allena lâ€™intero modello (es. ResNet-50) su `train_fold{i}`.\n","* **Validation**: usa `val_fold{i}` per early-stopping e calibrazione.\n","* **Test**: valuta sempre su `test_holdout` (o sul val-fold se non esiste un holdout).\n","* Salva un checkpoint per ogni fold (`supervised_fold{i}.pt`, `transfer_fold{i}.pt`, ecc.).\n","\n","### Aggregazione finale\n","\n","* Come per gli SSL, calcola la **media Â± deviazione** delle metriche su `test_holdout` per i vari fold.\n","* Non scegli il modello col miglior punteggio, ma riporti la **media aggregata**.\n","\n","> â„¹ï¸ **Facoltativo**: puoi evidenziare il fold piÃ¹ vicino alla media, **solo a scopo illustrativo**.\n","\n","---\n","\n","## 3. ğŸ” Confronto tra pipeline SSL e SL\n","\n","| Step                   | SSL models (SimCLR, MoCo, ...)  | SL/Transfer models              |\n","| ---------------------- | ------------------------------- | ------------------------------- |\n","| **Encoder training**   | Solo su fold0                   | Uno per ogni fold               |\n","| **Feature extraction** | Uno per ogni fold               | â€“                               |\n","| **Probe/classifier**   | Uno per ogni fold               | Uno per ogni fold               |\n","| **Inference**          | Su `test_holdout` per ogni fold | Su `test_holdout` per ogni fold |\n","| **Checkpoint**         | Encoder 1Ã—                      | 1Ã— per fold                     |\n","| **Output finale**      | Media Â± std su tutti i fold     | Media Â± std su tutti i fold     |\n","\n","---\n","\n","## ğŸ¯ PerchÃ© questa architettura?\n","\n","* Nei **modelli SSL**, vogliamo dimostrare che un encoder **generalizza** a nuovi pazienti come **feature extractor**, senza mai essere fine-tuned.\n","* Nei **modelli supervisionati**, alleniamo da capo su ogni fold per valutare una baseline comparabile (ma meno task-agnostica).\n","\n","In entrambi i casi:\n","\n","âœ… **Non scegli il miglior fold**,\n","âœ… **Riporti solo le medie cross-fold**,\n","âœ… **Dimostri affidabilitÃ , generalizzazione e riproducibilitÃ **.\n","\n","---\n","\n","## ğŸ“Š Output finale\n","\n","Le metriche aggregate sono presentate in una tabella riassuntiva:\n","\n","| Model             | Accuracy (Î¼Â±Ïƒ) | Macro-F1 (Î¼Â±Ïƒ) | ECE (Î¼Â±Ïƒ)   | Uncertainty (Î¼Â±Ïƒ) |\n","| ----------------- | -------------- | -------------- | ----------- | ----------------- |\n","| SimCLR + probe    | 0.83 Â± 0.04    | 0.79 Â± 0.06    | 0.03 Â± 0.01 | 0.12 Â± 0.03       |\n","| MoCo-v2 + probe   | 0.78 Â± 0.05    | â€¦              | â€¦           | â€¦                 |\n","| Rotation + probe  | 0.74 Â± 0.08    | â€¦              | â€¦           | â€¦                 |\n","| Supervised        | 0.80 Â± 0.03    | â€¦              | â€¦           | â€¦                 |\n","| Transfer learning | 0.82 Â± 0.04    | â€¦              | â€¦           | â€¦                 |\n","\n","> Ogni riga rappresenta una valutazione **completa e aggregata** del modello, utile per confronti clinici e accademici.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5960,"status":"ok","timestamp":1753293965061,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"},"user_tz":-120},"id":"CPkMc3ONzi0J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e6e6772-a272-46d8-a59c-664566c604a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¦ [DEBUG] Avvio configurazione ambienteâ€¦\n","ğŸ“ [DEBUG] Google Colab rilevato.\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","ğŸ“ [DEBUG] PROJECT_ROOT â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\n","âœ… [DEBUG] PyTorch giÃ  presente (2.6.0+cu124)\n","ğŸ”§ [DEBUG] Installazione pacchetti ausiliari mancanti: ['pillow', 'pyyaml']\n","ğŸ–¥ï¸ [DEBUG] Torch device disponibile â†’ cuda\n","ğŸ“¦ [DEBUG] DATA_TARBALL â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed\n"]}],"source":["# ## Cell 1 â€“ Environment Setup & Dependencies\n","# Compatibile con Google Colab (GPU/CPU) e ambiente locale (VS Code).\n","\n","# %%\n","import os, sys, subprocess, importlib.util\n","from pathlib import Path\n","\n","print(\"ğŸ“¦ [DEBUG] Avvio configurazione ambienteâ€¦\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 1) Rileva ambiente (Colab vs locale)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","IN_COLAB = Path(\"/content\").exists()\n","if IN_COLAB:\n","    print(\"ğŸ“ [DEBUG] Google Colab rilevato.\")\n","    from google.colab import drive  # type: ignore\n","    drive.mount(\"/content/drive\", force_remount=False)\n","else:\n","    print(\"ğŸ’» [DEBUG] Ambiente locale (VS Code / CLI) rilevato.\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 2) Definisci PROJECT_ROOT (ENV > default mapping)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DEFAULT_ENV_PATHS = {\n","    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n","    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n","}\n","PROJECT_ROOT = Path(os.getenv(\n","    \"PROJECT_ROOT\",\n","    DEFAULT_ENV_PATHS[\"colab\" if IN_COLAB else \"local\"])\n",").resolve()\n","\n","sys.path.append(str(PROJECT_ROOT / \"src\"))\n","print(f\"ğŸ“ [DEBUG] PROJECT_ROOT â†’ {PROJECT_ROOT}\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 3) Utility per installare pacchetti mancanti\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def _missing(pkgs):\n","    return [p for p in pkgs if importlib.util.find_spec(p) is None]\n","\n","def _install(pkgs, idx_url=None):\n","    if not pkgs:\n","        return\n","    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"]\n","    if idx_url:\n","        cmd += [\"--index-url\", idx_url]\n","    subprocess.check_call(cmd + pkgs)\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 4) Verifica / installa PyTorch (se non presente)\n","#    â€¢ In Colab non sovrascrive la versione pre-installata\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","TORCH_PKGS = [\"torch\", \"torchvision\", \"torchaudio\"]\n","\n","if _missing([\"torch\"]):\n","    print(\"ğŸ”§ [DEBUG] PyTorch non trovato â†’ installazione in corsoâ€¦\")\n","    if IN_COLAB:\n","        GPU = Path(\"/usr/local/cuda\").exists()\n","        INDEX = \"https://download.pytorch.org/whl/cu121\" if GPU else \"https://download.pytorch.org/whl/cpu\"\n","        _install(TORCH_PKGS, INDEX)\n","    else:  # locale: lascia scegliere all'utente il build corretto\n","        _install(TORCH_PKGS)\n","else:\n","    import torch\n","    print(f\"âœ… [DEBUG] PyTorch giÃ  presente ({torch.__version__})\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 5) Installazione pacchetti ausiliari (sempre sicura)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","AUX_PKGS = [\"webdataset\", \"tqdm\", \"pillow\", \"pyyaml\", \"joblib\"]\n","missing_aux = _missing(AUX_PKGS)\n","if missing_aux:\n","    print(f\"ğŸ”§ [DEBUG] Installazione pacchetti ausiliari mancanti: {missing_aux}\")\n","    _install(missing_aux)\n","else:\n","    print(\"âœ… [DEBUG] Pacchetti ausiliari giÃ  presenti.\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 6) Info dispositivo\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","import torch\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"ğŸ–¥ï¸ [DEBUG] Torch device disponibile â†’ {DEVICE}\")\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 7) Costante path per Data Tarball (utilizzata negli step successivi)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DATA_TARBALL = PROJECT_ROOT / \"data\" / \"processed\"\n","print(f\"ğŸ“¦ [DEBUG] DATA_TARBALL â†’ {DATA_TARBALL}\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1753293965068,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"},"user_tz":-120},"id":"jwx0_Pr9E59j","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e21ab0a-9d60-4d57-f84f-948e6262831e"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“„ [DEBUG] Config caricata da: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/config/training.yaml\n","ğŸ”‘ [DEBUG] EXP_CODE â†’ 20250723180604 (fonte: TIMESTAMP)\n","ğŸ§¬ [DEBUG] DATASET_ID         = dataset_9f30917e\n","ğŸ” [DEBUG] Folds              = [0, 1, 2, 3]\n","ğŸ”’ [DEBUG] train_encoder_once = True\n","ğŸ“‚ [DEBUG] EXP_BASE â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604\n","ğŸ’¾ [DEBUG] Salvato snapshot â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/training_20250723180604.yaml\n","ğŸ² [DEBUG] Seed globale impostato a: 42\n"]}],"source":["# %% -------------------------------------------------------------------- #\n","# Cell 2 â€“ Configurazione & Setup Esperimento                            #\n","# ----------------------------------------------------------------------- #\n","import yaml\n","import datetime\n","import os\n","import pprint\n","import random\n","import numpy as np\n","import torch\n","from pathlib import Path\n","\n","# â”€â”€â”€ 1) Carica il file di configurazione â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","cfg_path = PROJECT_ROOT / \"config\" / \"training.yaml\"\n","assert cfg_path.exists(), f\"âŒ File mancante: {cfg_path}\"\n","cfg = yaml.safe_load(cfg_path.read_text())\n","print(f\"ğŸ“„ [DEBUG] Config caricata da: {cfg_path}\")\n","\n","# â”€â”€â”€ 2) Genera EXP_CODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","yaml_exp = cfg.get(\"exp_code\", \"\")\n","env_exp  = os.getenv(\"EXP_CODE\", \"\")\n","if yaml_exp:\n","    EXP_CODE, src = yaml_exp, \"YAML\"\n","elif env_exp:\n","    EXP_CODE, src = env_exp, \"ENV\"\n","else:\n","    EXP_CODE, src = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"), \"TIMESTAMP\"\n","os.environ[\"EXP_CODE\"] = EXP_CODE\n","cfg[\"exp_code\"] = EXP_CODE\n","print(f\"ğŸ”‘ [DEBUG] EXP_CODE â†’ {EXP_CODE} (fonte: {src})\")\n","\n","# â”€â”€â”€ 3) Parametri dataset & folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n","FOLDS = cfg.get(\"folds\", [0])\n","TRAIN_ENCODER_ONCE = cfg.get(\"train_encoder_once\", False)\n","\n","print(f\"ğŸ§¬ [DEBUG] DATASET_ID         = {DATASET_ID}\")\n","print(f\"ğŸ” [DEBUG] Folds              = {FOLDS}\")\n","print(f\"ğŸ”’ [DEBUG] train_encoder_once = {TRAIN_ENCODER_ONCE}\")\n","\n","# â”€â”€â”€ 4) Crea la directory dellâ€™esperimento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","exp_dir_rel = cfg[\"output\"][\"exp_dir\"].format(\n","    dataset_id=DATASET_ID,\n","    exp_code=EXP_CODE\n",")\n","# Use PROJECT_ROOT without early resolve to ensure Drive path\n","EXP_BASE = PROJECT_ROOT / exp_dir_rel\n","# Safety check: ensure EXP_BASE is under PROJECT_ROOT\n","if not str(EXP_BASE.resolve()).startswith(str(PROJECT_ROOT.resolve())):\n","    raise RuntimeError(f\"ğŸš¨ EXP_BASE NON sotto PROJECT_ROOT! â†’ {EXP_BASE}\")\n","# Create experiment directory\n","EXP_BASE.mkdir(parents=True, exist_ok=True)\n","print(f\"ğŸ“‚ [DEBUG] EXP_BASE â†’ {EXP_BASE.resolve()}\")\n","\n","# â”€â”€â”€ 5) Salva snapshot del file YAML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","snap = EXP_BASE / f\"training_{EXP_CODE}.yaml\"\n","if not snap.exists():\n","    snap.write_text(yaml.dump(cfg, sort_keys=False))\n","    print(f\"ğŸ’¾ [DEBUG] Salvato snapshot â†’ {snap.resolve()}\")\n","else:\n","    print(f\"â„¹ï¸  [DEBUG] Snapshot giÃ  presente â†’ {snap.resolve()}\")\n","\n","# â”€â”€â”€ 6) Imposta seed per la riproducibilitÃ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","SEED = cfg.get(\"seed\", 42)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","print(f\"ğŸ² [DEBUG] Seed globale impostato a: {SEED}\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"aVDklAZ1Yurj","executionInfo":{"status":"ok","timestamp":1753293968436,"user_tz":-120,"elapsed":3366,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"584672fd-6191-4839-b5f4-8e78e4886c5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ [DEBUG] Modelli configurati in YAML: ['simclr', 'moco_v2', 'rotation', 'jepa', 'supervised', 'transfer']\n","âœ… [DEBUG] Importato trainers.simclr\n","âœ… [DEBUG] Importato trainers.moco_v2\n","âœ… [DEBUG] Importato trainers.rotation\n","âœ… [DEBUG] Importato trainers.jepa\n","âœ… [DEBUG] Importato trainers.supervised\n","âœ… [DEBUG] Importato trainers.transfer\n","ğŸ“š [DEBUG] Trainer registry contiene e corrispondenti tipi:\n","  â€¢ simclr       â†’ SSL\n","  â€¢ moco_v2      â†’ SSL\n","  â€¢ rotation     â†’ SSL\n","  â€¢ jepa         â†’ SSL\n","  â€¢ supervised   â†’ SL\n","  â€¢ transfer     â†’ SL\n","ğŸ” [DEBUG] Sotto-moduli caricati da utils.training_utils:\n","  â€¢ registry     â†’ functions: ['register_trainer']\n","  â€¢ device_io    â†’ functions: ['choose_device', 'get_latest_checkpoint', 'load_checkpoint', 'load_json', 'save_checkpoint', 'save_joblib', 'save_json']\n","  â€¢ data_utils   â†’ functions: ['build_loader', 'count_samples', 'default_transforms', 'discover_classes', 'extract_labels_from_keys', 'load_classifier', 'load_features', 'parse_label_from_filename', 'save_classifier', 'save_features']\n","  â€¢ model_utils  â†’ functions: ['create_backbone', 'mc_dropout_predictions']\n","  â€¢ metrics      â†’ functions: ['accuracy_score', 'aggregate_fold_metrics', 'apply_temperature_scaling', 'classification_report', 'compute_classification_metrics', 'confusion_matrix', 'expected_calibration_error', 'f1_score', 'mc_dropout_statistics', 'minimize', 'roc_auc_score', 'softmax']\n"]}],"source":["# ## Cell 3 â€“ Import & Registrazione dei Trainer\n","# Import dinamico dei modelli definiti in YAML e verifica del registry + tipo (SSL/SL).\n","import sys\n","import importlib\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","\n","# 1) Leggi i nomi dei modelli e i loro tipi da YAML\n","model_cfgs = cfg[\"models\"]\n","model_names = list(model_cfgs.keys())\n","print(f\"ğŸ”„ [DEBUG] Modelli configurati in YAML: {model_names}\")\n","\n","# 2) Import dinamico di ciascun modulo trainers.{model}\n","for name in model_names:\n","    module_name = f\"trainers.{name}\"\n","    try:\n","        if module_name in sys.modules:\n","            importlib.reload(sys.modules[module_name])\n","            print(f\"âœ… [DEBUG] Ricaricato {module_name}\")\n","        else:\n","            importlib.import_module(module_name)\n","            print(f\"âœ… [DEBUG] Importato {module_name}\")\n","    except ImportError as e:\n","        print(f\"âŒ [DEBUG] Errore importazione {module_name}: {e}\")\n","\n","# 3) Verifica che tutti i modelli siano registrati e mostra il loro tipo\n","missing = [n for n in model_names if n not in TRAINER_REGISTRY]\n","if missing:\n","    print(f\"âŒ [DEBUG] Trainer mancanti nel registry: {missing}\")\n","else:\n","    print(\"ğŸ“š [DEBUG] Trainer registry contiene e corrispondenti tipi:\")\n","    for name in model_names:\n","        # Usa solo il type dal file YAML per ogni modello\n","        ttype = model_cfgs[name].get(\"type\", \"unknown\").upper()\n","        print(f\"  â€¢ {name:<12} â†’ {ttype}\")\n","\n","# 3) Verifica funzioni in sottomoduli\n","import importlib, pkgutil, inspect\n","\n","print(\"ğŸ” [DEBUG] Sotto-moduli caricati da utils.training_utils:\")\n","for mod in (\"registry\", \"device_io\", \"data_utils\", \"model_utils\", \"metrics\"):\n","    m = importlib.import_module(f\"utils.training_utils.{mod}\")\n","    print(f\"  â€¢ {mod:<12} â†’ functions:\", [n for n, o in inspect.getmembers(m) if inspect.isfunction(o)])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Zie12NaWYurk","executionInfo":{"status":"ok","timestamp":1753293968516,"user_tz":-120,"elapsed":77,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"outputs":[],"source":["# %% -------------------------------------------------------------------- #\n","# Cell 4 â€“ Helper utilities (Tee, paths, selezione, â€¦)                    #\n","# ----------------------------------------------------------------------- #\n","import contextlib\n","import sys\n","import time\n","import math\n","import inspect\n","from pathlib import Path\n","from typing import Any\n","\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.device_io import (\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json,\n","    save_joblib,\n",")\n","from utils.training_utils.data_utils import count_samples\n","from trainers.train_classifier import train_classifier\n","from utils.training_utils.metrics import apply_temperature_scaling\n","\n","\n","class _Tee:\n","    \"\"\"\n","    Classe per duplicare lo stdout/stderr su piÃ¹ target (es. console + file).\n","    \"\"\"\n","    def __init__(self, *tgts):\n","        self.tgts = tgts\n","\n","    def write(self, data: str):\n","        for t in self.tgts:\n","            t.write(data)\n","            t.flush()\n","\n","    def flush(self):\n","        for t in self.tgts:\n","            t.flush()\n","\n","\n","def _global_experiments_append(line: str):\n","    \"\"\"\n","    Appende una riga al file esperimenti globale (esperiments.md).\n","    \"\"\"\n","    global_file = EXP_BASE.parent.parent / \"experiments.md\"\n","    with open(global_file, \"a\") as f:\n","        f.write(line.rstrip() + \"\\n\")\n","\n","\n","# %% ----------------------------------------------------------------------- #\n","# Cell 4 â€“ Helper utilities (Tee, paths, selezione, â€¦)                    #\n","# ------------------------------------------------------------------------ #\n","import os\n","from pathlib import Path\n","from typing import Any\n","\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.device_io import (\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json,\n","    save_joblib,\n",")\n","from utils.training_utils.data_utils import count_samples\n","from trainers.train_classifier import train_classifier\n","from utils.training_utils.metrics import apply_temperature_scaling\n","\n","\n","def _paths(cfg: dict, model: str, fold: int) -> dict[str, Path]:\n","    \"\"\"\n","    Costruisce tutti i path di output (training, inference, explain, aggregate, ecc.)\n","    per uno specifico modello e fold, sempre ancorati a PROJECT_ROOT.\n","    \"\"\"\n","    # Parametri di formattazione iniziali\n","    ph = {\n","        'dataset_id': cfg['data']['dataset_id'],\n","        'exp_code': cfg['exp_code'],\n","        'model_name': model,\n","        'fold_idx': fold,\n","    }\n","    # 1) exp_dir relativo\n","    rel_exp_dir = cfg['output']['exp_dir'].format(**ph)\n","    ph['exp_dir'] = rel_exp_dir\n","    # 2) exp_model_dir relativo (usa ph['exp_dir'])\n","    rel_exp_model_dir = cfg['output']['exp_model_dir'].format(**ph)\n","\n","    # Costruisci path assoluti sotto PROJECT_ROOT\n","    exp_dir       = (PROJECT_ROOT / rel_exp_dir)\n","    exp_model_dir = (PROJECT_ROOT / rel_exp_model_dir)\n","\n","    # Directory per training, inference, explain, aggregate, experiment level\n","    tr = exp_model_dir / f\"fold{fold}\" / \"training\"\n","    inf = exp_model_dir / f\"fold{fold}\" / \"inference\"\n","    ex = exp_model_dir / f\"fold{fold}\" / \"explain\"\n","    ag = exp_model_dir / \"_aggregate\"\n","    el = exp_dir / \"_experiment_level\"\n","\n","    # Creazione cartelle\n","    for d in (tr, inf, ex, ag, el):\n","        d.mkdir(parents=True, exist_ok=True)\n","\n","    # Ritorna i path assoluti per tutti gli artefatti\n","    return {\n","        'ckpt_dir':       tr.resolve(),\n","        'ckpt_tpl':       (tr / f\"{model}_bestepoch{{epoch:03d}}_fold{fold}.pt\").resolve(),\n","        'features':       (tr / f\"{model}_features_fold{fold}.pt\").resolve(),\n","        'features_train': (tr / f\"{model}_features_train_fold{fold}.pt\").resolve(),\n","        'features_val':   (tr / f\"{model}_features_val_fold{fold}.pt\").resolve(),\n","        'clf':            (tr / f\"{model}_classifier_fold{fold}.joblib\").resolve(),\n","        'scaler':         (tr / f\"{model}_ts_scaler_fold{fold}.joblib\").resolve(),\n","        'log':            (tr / f\"{model}_train_log_fold{fold}.md\").resolve(),\n","        'loss_json':      (tr / f\"{model}_train_valid_loss_fold{fold}.json\").resolve(),\n","\n","        'patch_preds':    (inf / f\"{model}_patch_preds_fold{fold}.pt\").resolve(),\n","        'patient_preds':  (inf / f\"{model}_patient_preds_fold{fold}.csv\").resolve(),\n","        'mc_logits':      (inf / f\"{model}_mc_logits_fold{fold}.npy\").resolve(),\n","        'metrics':        (inf / f\"{model}_metrics_fold{fold}.json\").resolve(),\n","\n","        'gradcam_dir':    ex.resolve(),\n","        'metadata_csv':   (ex / f\"{model}_metadata_gradcam_fold{fold}.csv\").resolve(),\n","\n","        'aggregate_metrics': (ag / f\"{model}_metrics.json\").resolve(),\n","        'aggregate_summary': (ag / f\"{model}_summary_agg.jpg\").resolve(),\n","\n","        'comparison_json':    (el / \"models_comparison.json\").resolve(),\n","        'comparison_img':     (el / \"models_comparison.jpg\").resolve(),\n","\n","        'readme':            (exp_dir / \"README_EXPERIMENT.md\").resolve(),\n","    }\n","\n","\n","\n","\n","def _completed(paths: dict[str, Path], is_ssl: bool) -> bool:\n","    \"\"\"\n","    Verifica se il training + artefatti SSL sono giÃ  stati completati.\n","    \"\"\"\n","    if not get_latest_checkpoint(paths[\"ckpt_dir\"]):\n","        return False\n","    if is_ssl:\n","        return all(paths[k].exists() for k in (\n","            \"features_train\", \"features_val\", \"clf\", \"scaler\", \"loss_json\"\n","        ))\n","    return True\n","\n","\n","def _select_models(cfg: dict) -> dict[str, dict[str, Any]]:\n","    \"\"\"\n","    Seleziona i modelli da eseguire in base a `run_models` o tutti i modelli.\n","    \"\"\"\n","    wanted = cfg.get(\"run_models\") or list(cfg[\"models\"].keys())\n","    return {name: cfg[\"models\"][name] for name in wanted}\n","\n","\n","def _init_trainer(name: str, m_cfg: dict, data_cfg: dict, ckpt_dir: Path):\n","    \"\"\"\n","    Inizializza il trainer registrato per nome.\n","    \"\"\"\n","    tr = TRAINER_REGISTRY[name](m_cfg, data_cfg)\n","    tr.ckpt_dir = ckpt_dir\n","    tr.m_cfg = m_cfg\n","    tr.data_cfg = data_cfg\n","    tr.is_ssl = m_cfg.get(\"type\", \"\").lower() == \"ssl\"\n","    return tr"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ptdtjmnBBJe8","executionInfo":{"status":"ok","timestamp":1753293968555,"user_tz":-120,"elapsed":36,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"outputs":[],"source":["# %% ----------------------------------------------------------------------- #\n","# Cell 5 â€“ Training loop                                                     #\n","# ----------------------------------------------------------------------- #\n","import math\n","import time\n","import inspect\n","from pathlib import Path\n","\n","import torch\n","import joblib\n","import numpy as np\n","\n","from utils.training_utils.device_io import get_latest_checkpoint, load_checkpoint, save_json\n","from utils.training_utils.data_utils import count_samples\n","from utils.training_utils.metrics import TemperatureScaler\n","from utils.training_utils.data_utils import extract_labels_from_keys\n","from trainers.train_classifier import train_classifier\n","\n","\n","import time\n","import inspect\n","from pathlib import Path\n","from typing import Optional\n","\n","import torch\n","import joblib\n","import numpy as np\n","\n","from utils.training_utils.device_io import get_latest_checkpoint, load_checkpoint, save_json\n","from utils.training_utils.data_utils import count_samples\n","from utils.training_utils.metrics import TemperatureScaler\n","from utils.training_utils.data_utils import extract_labels_from_keys\n","from trainers.train_classifier import train_classifier\n","\n","\n","def _get_total_batches(loader) -> Optional[int]:\n","    \"\"\"\n","    Try to infer the number of batches from the loader.\n","    Returns ``None`` if the loader/dataset has no valid length (e.g. WebDataset).\n","    \"\"\"\n","    try:\n","        return len(loader)  # may raise TypeError if undefined\n","    except TypeError:\n","        return None\n","\n","\n","def _run_full_training(trainer, paths: dict[str, Path], epochs: int) -> None:\n","    \"\"\"\n","    Full training loop with per-batch logging.\n","\n","    * If ``len(trainer.train_loader)`` is available, a percentage and ETA are shown.\n","    * Otherwise, only the current batch index is logged, avoiding wrong totals.\n","    \"\"\"\n","    is_ssl = getattr(trainer, \"is_ssl\", False)\n","    history: list[dict] = []\n","\n","    for epoch in range(1, epochs + 1):\n","        start_time = time.time()\n","        loss_sum = 0.0\n","        corr_sum = 0\n","        seen = 0\n","\n","        # â”€â”€ try to compute total batches on-the-fly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","        total_batches = _get_total_batches(trainer.train_loader)\n","        print(f\"[fold{trainer.cfg_fold}] â”€â”€ Epoch {epoch}/{epochs} â”€â”€\")\n","\n","        for i, batch in enumerate(trainer.train_loader, start=1):\n","            # if i == 3:\n","            #   break\n","            sig = inspect.signature(trainer.train_step)\n","            result = (\n","                trainer.train_step(*batch)\n","                if len(sig.parameters) > 1\n","                else trainer.train_step(batch)\n","            )\n","\n","            if len(result) == 4:\n","                _, loss, correct, bs = result\n","            else:\n","                loss, bs = result\n","                correct = 0\n","\n","            loss_sum += loss * bs\n","            corr_sum += correct\n","            seen += bs\n","\n","            # â”€â”€ logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","            msg = f\"[fold{trainer.cfg_fold}] Batch {i}\"\n","            if total_batches:\n","                pct = (i / total_batches) * 100\n","                eta = (time.time() - start_time) / i * (total_batches - i)\n","                msg += f\"/{total_batches} ({pct:5.1f}%) | ETA {eta:6.1f}s\"\n","            msg += f\" | Loss {loss_sum/seen:.4f}\"\n","            if not is_ssl:\n","                msg += f\" | Acc {corr_sum/seen:.3f}\"\n","            print(msg, flush=True)\n","\n","        # â”€â”€ end-of-epoch bookkeeping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","        train_loss = loss_sum / seen\n","        if not is_ssl:\n","            val_loss, val_acc = trainer.validate_epoch()\n","            trainer.post_epoch(epoch, val_acc)\n","            history.append(\n","                {\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": val_acc}\n","            )\n","            print(f\"[fold{trainer.cfg_fold}] Val â†’ Loss {val_loss:.4f} | Acc {val_acc:.3f}\")\n","        else:\n","            trainer.post_epoch(epoch, train_loss)\n","            history.append({\"epoch\": epoch, \"train_loss\": train_loss})\n","            print(f\"[fold{trainer.cfg_fold}] Train â†’ Loss {train_loss:.4f}\")\n","\n","        print(f\"[fold{trainer.cfg_fold}] â±  {time.time() - start_time:.1f}s\\n\")\n","\n","    save_json(history, paths[\"loss_json\"])\n","\n","\n","def _resume_or_train(trainer, paths: dict[str, Path], epochs: int):\n","    \"\"\"\n","    Resume from last checkpoint if available, otherwise run full training.\n","    \"\"\"\n","    ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","    if ckpt:\n","        print(f\"[fold{trainer.cfg_fold}] â©  Resuming from {ckpt.name}\")\n","        model, optimizer = trainer.get_resume_model_and_optimizer()\n","        load_checkpoint(ckpt, model, optimizer)\n","    _run_full_training(trainer, paths, epochs)\n","\n","\n","def _ensure_ssl_artifacts(trainer, paths: dict[str, Path]):\n","    \"\"\"\n","    For SSL models:\n","      1) Extract train and val features\n","      2) Train a linear probe on train features\n","      3) Calibrate the probe via temperature scaling on val logits\n","    \"\"\"\n","    # 1) Extract train features\n","    if not paths[\"features_train\"].exists():\n","        trainer.extract_features_to(paths[\"features_train\"], split=\"train\")\n","\n","    # 2) Extract val features\n","    if not paths[\"features_val\"].exists():\n","        trainer.extract_features_to(paths[\"features_val\"], split=\"val\")\n","\n","    # 3) Train the linear classifier\n","    if not paths[\"clf\"].exists():\n","        train_classifier(str(paths[\"features_train\"]), str(paths[\"clf\"]))\n","\n","    # 4) Temperature scaling on validation logits\n","    if not paths[\"scaler\"].exists():\n","        # Load validation features\n","        val_data = torch.load(paths[\"features_val\"], map_location=\"cpu\")\n","        X_val = val_data[\"features\"].numpy()\n","        keys_val = val_data[\"keys\"]\n","\n","        # Load classifier and label encoder\n","        bundle = joblib.load(paths[\"clf\"])\n","        clf = bundle[\"model\"]\n","        le = bundle[\"label_encoder\"]\n","\n","        # Obtain logits or convert probs to logits\n","        if hasattr(clf, \"decision_function\"):\n","            logits = clf.decision_function(X_val)\n","        else:\n","            probs = clf.predict_proba(X_val)\n","            logits = np.log(probs + 1e-12)\n","\n","        # Extract labels from keys\n","        labels = extract_labels_from_keys(keys_val, le)\n","\n","        # Fit the TemperatureScaler\n","        scaler = TemperatureScaler().fit(logits, labels)\n","        joblib.dump(scaler, str(paths[\"scaler\"]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMV3BYIyE7FM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2dd56956-8989-4dfd-cf06-2f1162ba62e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸš€  Modello 'rotation'  (SSL) â€“ epochs=25\n","[fold0] ğŸ“‚  ckpt dir      â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold0/training\n","[fold0] ğŸ·   train shards  â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n","[fold0] ğŸš€  avvio trainer  â†’ 'rotation'\n","[fold0] â”€â”€ Epoch 1/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3691\n","[fold0] Batch 2 | Loss 1.3961\n","[fold0] Batch 3 | Loss 1.4439\n","[fold0] Batch 4 | Loss 1.4594\n","[fold0] Batch 5 | Loss 1.4723\n","[fold0] Batch 6 | Loss 1.4619\n","[fold0] Batch 7 | Loss 1.4791\n","[fold0] Batch 8 | Loss 1.4834\n","[fold0] Batch 9 | Loss 1.4735\n","[fold0] Batch 10 | Loss 1.4779\n","[fold0] Batch 11 | Loss 1.4835\n","[fold0] Batch 12 | Loss 1.4779\n","[fold0] Batch 13 | Loss 1.4768\n","[fold0] Batch 14 | Loss 1.4763\n","[fold0] Batch 15 | Loss 1.4715\n","[fold0] Batch 16 | Loss 1.4715\n","[fold0] Batch 17 | Loss 1.4695\n","[fold0] Batch 18 | Loss 1.4693\n","[fold0] Batch 19 | Loss 1.4659\n","[fold0] Batch 20 | Loss 1.4683\n","[fold0] Batch 21 | Loss 1.4700\n","[fold0] Batch 22 | Loss 1.4679\n","[fold0] Batch 23 | Loss 1.4713\n","[fold0] Train â†’ Loss 1.4713\n","[fold0] â±  72.3s\n","\n","[fold0] â”€â”€ Epoch 2/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.5365\n","[fold0] Batch 2 | Loss 1.5892\n","[fold0] Batch 3 | Loss 1.5604\n","[fold0] Batch 4 | Loss 1.5435\n","[fold0] Batch 5 | Loss 1.5736\n","[fold0] Batch 6 | Loss 1.6316\n","[fold0] Batch 7 | Loss 1.6197\n","[fold0] Batch 8 | Loss 1.6561\n","[fold0] Batch 9 | Loss 1.6719\n","[fold0] Batch 10 | Loss 1.6459\n","[fold0] Batch 11 | Loss 1.6652\n","[fold0] Batch 12 | Loss 1.6848\n","[fold0] Batch 13 | Loss 1.6747\n","[fold0] Batch 14 | Loss 1.6578\n","[fold0] Batch 15 | Loss 1.6483\n","[fold0] Batch 16 | Loss 1.6486\n","[fold0] Batch 17 | Loss 1.6598\n","[fold0] Batch 18 | Loss 1.6436\n","[fold0] Batch 19 | Loss 1.6386\n","[fold0] Batch 20 | Loss 1.6470\n","[fold0] Batch 21 | Loss 1.6428\n","[fold0] Batch 22 | Loss 1.6325\n","[fold0] Batch 23 | Loss 1.6295\n","[fold0] Train â†’ Loss 1.6295\n","[fold0] â±  65.9s\n","\n","[fold0] â”€â”€ Epoch 3/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4848\n","[fold0] Batch 2 | Loss 1.4733\n","[fold0] Batch 3 | Loss 1.4702\n","[fold0] Batch 4 | Loss 1.4771\n","[fold0] Batch 5 | Loss 1.4821\n","[fold0] Batch 6 | Loss 1.4802\n","[fold0] Batch 7 | Loss 1.4743\n","[fold0] Batch 8 | Loss 1.4646\n","[fold0] Batch 9 | Loss 1.4646\n","[fold0] Batch 10 | Loss 1.4606\n","[fold0] Batch 11 | Loss 1.4558\n","[fold0] Batch 12 | Loss 1.4494\n","[fold0] Batch 13 | Loss 1.4529\n","[fold0] Batch 14 | Loss 1.4556\n","[fold0] Batch 15 | Loss 1.4518\n","[fold0] Batch 16 | Loss 1.4628\n","[fold0] Batch 17 | Loss 1.4622\n","[fold0] Batch 18 | Loss 1.4620\n","[fold0] Batch 19 | Loss 1.4650\n","[fold0] Batch 20 | Loss 1.4689\n","[fold0] Batch 21 | Loss 1.4738\n","[fold0] Batch 22 | Loss 1.4717\n","[fold0] Batch 23 | Loss 1.4743\n","[fold0] Train â†’ Loss 1.4743\n","[fold0] â±  64.4s\n","\n","[fold0] â”€â”€ Epoch 4/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4789\n","[fold0] Batch 2 | Loss 1.4228\n","[fold0] Batch 3 | Loss 1.4753\n","[fold0] Batch 4 | Loss 1.4999\n","[fold0] Batch 5 | Loss 1.4832\n","[fold0] Batch 6 | Loss 1.4995\n","[fold0] Batch 7 | Loss 1.5142\n","[fold0] Batch 8 | Loss 1.5062\n","[fold0] Batch 9 | Loss 1.5193\n","[fold0] Batch 10 | Loss 1.5257\n","[fold0] Batch 11 | Loss 1.5283\n","[fold0] Batch 12 | Loss 1.5227\n","[fold0] Batch 13 | Loss 1.5284\n","[fold0] Batch 14 | Loss 1.5237\n","[fold0] Batch 15 | Loss 1.5276\n","[fold0] Batch 16 | Loss 1.5300\n","[fold0] Batch 17 | Loss 1.5262\n","[fold0] Batch 18 | Loss 1.5278\n","[fold0] Batch 19 | Loss 1.5236\n","[fold0] Batch 20 | Loss 1.5222\n","[fold0] Batch 21 | Loss 1.5166\n","[fold0] Batch 22 | Loss 1.5131\n","[fold0] Batch 23 | Loss 1.5083\n","[fold0] Train â†’ Loss 1.5083\n","[fold0] â±  64.6s\n","\n","[fold0] â”€â”€ Epoch 5/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4931\n","[fold0] Batch 2 | Loss 1.4251\n","[fold0] Batch 3 | Loss 1.4531\n","[fold0] Batch 4 | Loss 1.4451\n","[fold0] Batch 5 | Loss 1.4358\n","[fold0] Batch 6 | Loss 1.4339\n","[fold0] Batch 7 | Loss 1.4301\n","[fold0] Batch 8 | Loss 1.4369\n","[fold0] Batch 9 | Loss 1.4540\n","[fold0] Batch 10 | Loss 1.4651\n","[fold0] Batch 11 | Loss 1.4613\n","[fold0] Batch 12 | Loss 1.4530\n","[fold0] Batch 13 | Loss 1.4616\n","[fold0] Batch 14 | Loss 1.4587\n","[fold0] Batch 15 | Loss 1.4688\n","[fold0] Batch 16 | Loss 1.4680\n","[fold0] Batch 17 | Loss 1.4721\n","[fold0] Batch 18 | Loss 1.4928\n","[fold0] Batch 19 | Loss 1.4909\n","[fold0] Batch 20 | Loss 1.4907\n","[fold0] Batch 21 | Loss 1.4909\n","[fold0] Batch 22 | Loss 1.4900\n","[fold0] Batch 23 | Loss 1.4842\n","[fold0] Train â†’ Loss 1.4842\n","[fold0] â±  66.0s\n","\n","[fold0] â”€â”€ Epoch 6/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.5089\n","[fold0] Batch 2 | Loss 1.4655\n","[fold0] Batch 3 | Loss 1.5069\n","[fold0] Batch 4 | Loss 1.5021\n","[fold0] Batch 5 | Loss 1.5002\n","[fold0] Batch 6 | Loss 1.5126\n","[fold0] Batch 7 | Loss 1.5082\n","[fold0] Batch 8 | Loss 1.4981\n","[fold0] Batch 9 | Loss 1.5050\n","[fold0] Batch 10 | Loss 1.5351\n","[fold0] Batch 11 | Loss 1.5254\n","[fold0] Batch 12 | Loss 1.5170\n","[fold0] Batch 13 | Loss 1.5116\n","[fold0] Batch 14 | Loss 1.5094\n","[fold0] Batch 15 | Loss 1.5050\n","[fold0] Batch 16 | Loss 1.5078\n","[fold0] Batch 17 | Loss 1.5046\n","[fold0] Batch 18 | Loss 1.5027\n","[fold0] Batch 19 | Loss 1.5006\n","[fold0] Batch 20 | Loss 1.4972\n","[fold0] Batch 21 | Loss 1.4913\n","[fold0] Batch 22 | Loss 1.4886\n","[fold0] Batch 23 | Loss 1.4877\n","[fold0] Train â†’ Loss 1.4877\n","[fold0] â±  65.9s\n","\n","[fold0] â”€â”€ Epoch 7/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3836\n","[fold0] Batch 2 | Loss 1.4050\n","[fold0] Batch 3 | Loss 1.4649\n","[fold0] Batch 4 | Loss 1.4437\n","[fold0] Batch 5 | Loss 1.4330\n","[fold0] Batch 6 | Loss 1.4411\n","[fold0] Batch 7 | Loss 1.4499\n","[fold0] Batch 8 | Loss 1.4465\n","[fold0] Batch 9 | Loss 1.4485\n","[fold0] Batch 10 | Loss 1.4569\n","[fold0] Batch 11 | Loss 1.4575\n","[fold0] Batch 12 | Loss 1.4571\n","[fold0] Batch 13 | Loss 1.4622\n","[fold0] Batch 14 | Loss 1.4681\n","[fold0] Batch 15 | Loss 1.4668\n","[fold0] Batch 16 | Loss 1.4675\n","[fold0] Batch 17 | Loss 1.4657\n","[fold0] Batch 18 | Loss 1.4638\n","[fold0] Batch 19 | Loss 1.4670\n","[fold0] Batch 20 | Loss 1.4676\n","[fold0] Batch 21 | Loss 1.4697\n","[fold0] Batch 22 | Loss 1.4752\n","[fold0] Batch 23 | Loss 1.4766\n","[fold0] Train â†’ Loss 1.4766\n","[fold0] â±  65.0s\n","\n","[fold0] â”€â”€ Epoch 8/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3712\n","[fold0] Batch 2 | Loss 1.5572\n","[fold0] Batch 3 | Loss 1.6096\n","[fold0] Batch 4 | Loss 1.5681\n","[fold0] Batch 5 | Loss 1.5301\n","[fold0] Batch 6 | Loss 1.5454\n","[fold0] Batch 7 | Loss 1.5546\n","[fold0] Batch 8 | Loss 1.5528\n","[fold0] Batch 9 | Loss 1.5431\n","[fold0] Batch 10 | Loss 1.5315\n","[fold0] Batch 11 | Loss 1.5234\n","[fold0] Batch 12 | Loss 1.5182\n","[fold0] Batch 13 | Loss 1.5170\n","[fold0] Batch 14 | Loss 1.5179\n","[fold0] Batch 15 | Loss 1.5089\n","[fold0] Batch 16 | Loss 1.5038\n","[fold0] Batch 17 | Loss 1.4974\n","[fold0] Batch 18 | Loss 1.4926\n","[fold0] Batch 19 | Loss 1.4980\n","[fold0] Batch 20 | Loss 1.4934\n","[fold0] Batch 21 | Loss 1.4899\n","[fold0] Batch 22 | Loss 1.4887\n","[fold0] Batch 23 | Loss 1.4875\n","[fold0] Train â†’ Loss 1.4875\n","[fold0] â±  64.2s\n","\n","[fold0] â”€â”€ Epoch 9/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4112\n","[fold0] Batch 2 | Loss 1.4170\n","[fold0] Batch 3 | Loss 1.4143\n","[fold0] Batch 4 | Loss 1.4135\n","[fold0] Batch 5 | Loss 1.4163\n","[fold0] Batch 6 | Loss 1.4131\n","[fold0] Batch 7 | Loss 1.4201\n","[fold0] Batch 8 | Loss 1.4149\n","[fold0] Batch 9 | Loss 1.4160\n","[fold0] Batch 10 | Loss 1.4148\n","[fold0] Batch 11 | Loss 1.4092\n","[fold0] Batch 12 | Loss 1.4175\n","[fold0] Batch 13 | Loss 1.4175\n","[fold0] Batch 14 | Loss 1.4201\n","[fold0] Batch 15 | Loss 1.4211\n","[fold0] Batch 16 | Loss 1.4190\n","[fold0] Batch 17 | Loss 1.4191\n","[fold0] Batch 18 | Loss 1.4239\n","[fold0] Batch 19 | Loss 1.4262\n","[fold0] Batch 20 | Loss 1.4252\n","[fold0] Batch 21 | Loss 1.4250\n","[fold0] Batch 22 | Loss 1.4232\n","[fold0] Batch 23 | Loss 1.4260\n","[fold0] Train â†’ Loss 1.4260\n","[fold0] â±  65.4s\n","\n","[fold0] â”€â”€ Epoch 10/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4935\n","[fold0] Batch 2 | Loss 1.4390\n","[fold0] Batch 3 | Loss 1.4330\n","[fold0] Batch 4 | Loss 1.4647\n","[fold0] Batch 5 | Loss 1.4729\n","[fold0] Batch 6 | Loss 1.4742\n","[fold0] Batch 7 | Loss 1.4747\n","[fold0] Batch 8 | Loss 1.4687\n","[fold0] Batch 9 | Loss 1.4710\n","[fold0] Batch 10 | Loss 1.4739\n","[fold0] Batch 11 | Loss 1.4638\n","[fold0] Batch 12 | Loss 1.4628\n","[fold0] Batch 13 | Loss 1.4559\n","[fold0] Batch 14 | Loss 1.4525\n","[fold0] Batch 15 | Loss 1.4532\n","[fold0] Batch 16 | Loss 1.4488\n","[fold0] Batch 17 | Loss 1.4467\n","[fold0] Batch 18 | Loss 1.4494\n","[fold0] Batch 19 | Loss 1.4463\n","[fold0] Batch 20 | Loss 1.4433\n","[fold0] Batch 21 | Loss 1.4426\n","[fold0] Batch 22 | Loss 1.4415\n","[fold0] Batch 23 | Loss 1.4391\n","[fold0] Train â†’ Loss 1.4391\n","[fold0] â±  65.4s\n","\n","[fold0] â”€â”€ Epoch 11/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3855\n","[fold0] Batch 2 | Loss 1.3881\n","[fold0] Batch 3 | Loss 1.3893\n","[fold0] Batch 4 | Loss 1.3920\n","[fold0] Batch 5 | Loss 1.4023\n","[fold0] Batch 6 | Loss 1.4255\n","[fold0] Batch 7 | Loss 1.4226\n","[fold0] Batch 8 | Loss 1.4223\n","[fold0] Batch 9 | Loss 1.4181\n","[fold0] Batch 10 | Loss 1.4281\n","[fold0] Batch 11 | Loss 1.4356\n","[fold0] Batch 12 | Loss 1.4367\n","[fold0] Batch 13 | Loss 1.4331\n","[fold0] Batch 14 | Loss 1.4307\n","[fold0] Batch 15 | Loss 1.4302\n","[fold0] Batch 16 | Loss 1.4282\n","[fold0] Batch 17 | Loss 1.4311\n","[fold0] Batch 18 | Loss 1.4281\n","[fold0] Batch 19 | Loss 1.4263\n","[fold0] Batch 20 | Loss 1.4240\n","[fold0] Batch 21 | Loss 1.4222\n","[fold0] Batch 22 | Loss 1.4208\n","[fold0] Batch 23 | Loss 1.4202\n","[fold0] Train â†’ Loss 1.4202\n","[fold0] â±  65.3s\n","\n","[fold0] â”€â”€ Epoch 12/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4201\n","[fold0] Batch 2 | Loss 1.4108\n","[fold0] Batch 3 | Loss 1.3959\n","[fold0] Batch 4 | Loss 1.4212\n","[fold0] Batch 5 | Loss 1.4289\n","[fold0] Batch 6 | Loss 1.4266\n","[fold0] Batch 7 | Loss 1.4217\n","[fold0] Batch 8 | Loss 1.4236\n","[fold0] Batch 9 | Loss 1.4251\n","[fold0] Batch 10 | Loss 1.4243\n","[fold0] Batch 11 | Loss 1.4269\n","[fold0] Batch 12 | Loss 1.4234\n","[fold0] Batch 13 | Loss 1.4274\n","[fold0] Batch 14 | Loss 1.4256\n","[fold0] Batch 15 | Loss 1.4243\n","[fold0] Batch 16 | Loss 1.4197\n","[fold0] Batch 17 | Loss 1.4165\n","[fold0] Batch 18 | Loss 1.4188\n","[fold0] Batch 19 | Loss 1.4258\n","[fold0] Batch 20 | Loss 1.4277\n","[fold0] Batch 21 | Loss 1.4250\n","[fold0] Batch 22 | Loss 1.4230\n","[fold0] Batch 23 | Loss 1.4224\n","[fold0] Train â†’ Loss 1.4224\n","[fold0] â±  65.4s\n","\n","[fold0] â”€â”€ Epoch 13/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4950\n","[fold0] Batch 2 | Loss 1.4860\n","[fold0] Batch 3 | Loss 1.4449\n","[fold0] Batch 4 | Loss 1.4323\n","[fold0] Batch 5 | Loss 1.4281\n","[fold0] Batch 6 | Loss 1.4218\n","[fold0] Batch 7 | Loss 1.4174\n","[fold0] Batch 8 | Loss 1.4135\n","[fold0] Batch 9 | Loss 1.4199\n","[fold0] Batch 10 | Loss 1.4229\n","[fold0] Batch 11 | Loss 1.4187\n","[fold0] Batch 12 | Loss 1.4202\n","[fold0] Batch 13 | Loss 1.4213\n","[fold0] Batch 14 | Loss 1.4183\n","[fold0] Batch 15 | Loss 1.4198\n","[fold0] Batch 16 | Loss 1.4211\n","[fold0] Batch 17 | Loss 1.4189\n","[fold0] Batch 18 | Loss 1.4185\n","[fold0] Batch 19 | Loss 1.4172\n","[fold0] Batch 20 | Loss 1.4176\n","[fold0] Batch 21 | Loss 1.4151\n","[fold0] Batch 22 | Loss 1.4136\n","[fold0] Batch 23 | Loss 1.4128\n","[fold0] Train â†’ Loss 1.4128\n","[fold0] â±  64.7s\n","\n","[fold0] â”€â”€ Epoch 14/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3904\n","[fold0] Batch 2 | Loss 1.4236\n","[fold0] Batch 3 | Loss 1.4200\n","[fold0] Batch 4 | Loss 1.4269\n","[fold0] Batch 5 | Loss 1.4340\n","[fold0] Batch 6 | Loss 1.4393\n","[fold0] Batch 7 | Loss 1.4336\n","[fold0] Batch 8 | Loss 1.4336\n","[fold0] Batch 9 | Loss 1.4322\n","[fold0] Batch 10 | Loss 1.4297\n","[fold0] Batch 11 | Loss 1.4315\n","[fold0] Batch 12 | Loss 1.4271\n","[fold0] Batch 13 | Loss 1.4261\n","[fold0] Batch 14 | Loss 1.4319\n","[fold0] Batch 15 | Loss 1.4284\n","[fold0] Batch 16 | Loss 1.4251\n","[fold0] Batch 17 | Loss 1.4241\n","[fold0] Batch 18 | Loss 1.4221\n","[fold0] Batch 19 | Loss 1.4210\n","[fold0] Batch 20 | Loss 1.4198\n","[fold0] Batch 21 | Loss 1.4187\n","[fold0] Batch 22 | Loss 1.4169\n","[fold0] Batch 23 | Loss 1.4161\n","[fold0] Train â†’ Loss 1.4161\n","[fold0] â±  65.1s\n","\n","[fold0] â”€â”€ Epoch 15/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3673\n","[fold0] Batch 2 | Loss 1.3895\n","[fold0] Batch 3 | Loss 1.3906\n","[fold0] Batch 4 | Loss 1.3974\n","[fold0] Batch 5 | Loss 1.3987\n","[fold0] Batch 6 | Loss 1.3974\n","[fold0] Batch 7 | Loss 1.3976\n","[fold0] Batch 8 | Loss 1.3975\n","[fold0] Batch 9 | Loss 1.3985\n","[fold0] Batch 10 | Loss 1.3984\n","[fold0] Batch 11 | Loss 1.3963\n","[fold0] Batch 12 | Loss 1.3951\n","[fold0] Batch 13 | Loss 1.3966\n","[fold0] Batch 14 | Loss 1.4007\n","[fold0] Batch 15 | Loss 1.4015\n","[fold0] Batch 16 | Loss 1.4019\n","[fold0] Batch 17 | Loss 1.4015\n","[fold0] Batch 18 | Loss 1.4007\n","[fold0] Batch 19 | Loss 1.4029\n","[fold0] Batch 20 | Loss 1.4076\n","[fold0] Batch 21 | Loss 1.4079\n","[fold0] Batch 22 | Loss 1.4072\n","[fold0] Batch 23 | Loss 1.4073\n","[fold0] Train â†’ Loss 1.4073\n","[fold0] â±  65.8s\n","\n","[fold0] â”€â”€ Epoch 16/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4596\n","[fold0] Batch 2 | Loss 1.4569\n","[fold0] Batch 3 | Loss 1.4367\n","[fold0] Batch 4 | Loss 1.4252\n","[fold0] Batch 5 | Loss 1.4261\n","[fold0] Batch 6 | Loss 1.4209\n","[fold0] Batch 7 | Loss 1.4170\n","[fold0] Batch 8 | Loss 1.4202\n","[fold0] Batch 9 | Loss 1.4216\n","[fold0] Batch 10 | Loss 1.4171\n","[fold0] Batch 11 | Loss 1.4165\n","[fold0] Batch 12 | Loss 1.4163\n","[fold0] Batch 13 | Loss 1.4156\n","[fold0] Batch 14 | Loss 1.4146\n","[fold0] Batch 15 | Loss 1.4137\n","[fold0] Batch 16 | Loss 1.4143\n","[fold0] Batch 17 | Loss 1.4173\n","[fold0] Batch 18 | Loss 1.4191\n","[fold0] Batch 19 | Loss 1.4207\n","[fold0] Batch 20 | Loss 1.4200\n","[fold0] Batch 21 | Loss 1.4200\n","[fold0] Batch 22 | Loss 1.4207\n","[fold0] Batch 23 | Loss 1.4202\n","[fold0] Train â†’ Loss 1.4202\n","[fold0] â±  66.1s\n","\n","[fold0] â”€â”€ Epoch 17/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3992\n","[fold0] Batch 2 | Loss 1.3876\n","[fold0] Batch 3 | Loss 1.3962\n","[fold0] Batch 4 | Loss 1.3970\n","[fold0] Batch 5 | Loss 1.3945\n","[fold0] Batch 6 | Loss 1.4095\n","[fold0] Batch 7 | Loss 1.4016\n","[fold0] Batch 8 | Loss 1.4060\n","[fold0] Batch 9 | Loss 1.4020\n","[fold0] Batch 10 | Loss 1.4035\n","[fold0] Batch 11 | Loss 1.4021\n","[fold0] Batch 12 | Loss 1.4055\n","[fold0] Batch 13 | Loss 1.4185\n","[fold0] Batch 14 | Loss 1.4189\n","[fold0] Batch 15 | Loss 1.4149\n","[fold0] Batch 16 | Loss 1.4135\n","[fold0] Batch 17 | Loss 1.4137\n","[fold0] Batch 18 | Loss 1.4135\n","[fold0] Batch 19 | Loss 1.4144\n","[fold0] Batch 20 | Loss 1.4140\n","[fold0] Batch 21 | Loss 1.4131\n","[fold0] Batch 22 | Loss 1.4114\n","[fold0] Batch 23 | Loss 1.4101\n","[fold0] Train â†’ Loss 1.4101\n","[fold0] â±  64.8s\n","\n","[fold0] â”€â”€ Epoch 18/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4100\n","[fold0] Batch 2 | Loss 1.4020\n","[fold0] Batch 3 | Loss 1.4004\n","[fold0] Batch 4 | Loss 1.4002\n","[fold0] Batch 5 | Loss 1.3993\n","[fold0] Batch 6 | Loss 1.3976\n","[fold0] Batch 7 | Loss 1.3968\n","[fold0] Batch 8 | Loss 1.3936\n","[fold0] Batch 9 | Loss 1.3996\n","[fold0] Batch 10 | Loss 1.3995\n","[fold0] Batch 11 | Loss 1.4037\n","[fold0] Batch 12 | Loss 1.4019\n","[fold0] Batch 13 | Loss 1.4036\n","[fold0] Batch 14 | Loss 1.4063\n","[fold0] Batch 15 | Loss 1.4069\n","[fold0] Batch 16 | Loss 1.4073\n","[fold0] Batch 17 | Loss 1.4069\n","[fold0] Batch 18 | Loss 1.4062\n","[fold0] Batch 19 | Loss 1.4059\n","[fold0] Batch 20 | Loss 1.4054\n","[fold0] Batch 21 | Loss 1.4049\n","[fold0] Batch 22 | Loss 1.4059\n","[fold0] Batch 23 | Loss 1.4049\n","[fold0] Train â†’ Loss 1.4049\n","[fold0] â±  64.0s\n","\n","[fold0] â”€â”€ Epoch 19/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3813\n","[fold0] Batch 2 | Loss 1.3894\n","[fold0] Batch 3 | Loss 1.3845\n","[fold0] Batch 4 | Loss 1.3940\n","[fold0] Batch 5 | Loss 1.3982\n","[fold0] Batch 6 | Loss 1.3999\n","[fold0] Batch 7 | Loss 1.3983\n","[fold0] Batch 8 | Loss 1.3982\n","[fold0] Batch 9 | Loss 1.3996\n","[fold0] Batch 10 | Loss 1.4044\n","[fold0] Batch 11 | Loss 1.4078\n","[fold0] Batch 12 | Loss 1.4143\n","[fold0] Batch 13 | Loss 1.4147\n","[fold0] Batch 14 | Loss 1.4129\n","[fold0] Batch 15 | Loss 1.4103\n","[fold0] Batch 16 | Loss 1.4109\n","[fold0] Batch 17 | Loss 1.4128\n","[fold0] Batch 18 | Loss 1.4095\n","[fold0] Batch 19 | Loss 1.4126\n","[fold0] Batch 20 | Loss 1.4128\n","[fold0] Batch 21 | Loss 1.4120\n","[fold0] Batch 22 | Loss 1.4109\n","[fold0] Batch 23 | Loss 1.4106\n","[fold0] Train â†’ Loss 1.4106\n","[fold0] â±  65.1s\n","\n","[fold0] â”€â”€ Epoch 20/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3992\n","[fold0] Batch 2 | Loss 1.4126\n","[fold0] Batch 3 | Loss 1.4402\n","[fold0] Batch 4 | Loss 1.4316\n","[fold0] Batch 5 | Loss 1.4220\n","[fold0] Batch 6 | Loss 1.4158\n","[fold0] Batch 7 | Loss 1.4124\n","[fold0] Batch 8 | Loss 1.4108\n","[fold0] Batch 9 | Loss 1.4094\n","[fold0] Batch 10 | Loss 1.4117\n","[fold0] Batch 11 | Loss 1.4184\n","[fold0] Batch 12 | Loss 1.4167\n","[fold0] Batch 13 | Loss 1.4160\n","[fold0] Batch 14 | Loss 1.4159\n","[fold0] Batch 15 | Loss 1.4176\n","[fold0] Batch 16 | Loss 1.4137\n","[fold0] Batch 17 | Loss 1.4126\n","[fold0] Batch 18 | Loss 1.4131\n","[fold0] Batch 19 | Loss 1.4128\n","[fold0] Batch 20 | Loss 1.4120\n","[fold0] Batch 21 | Loss 1.4100\n","[fold0] Batch 22 | Loss 1.4087\n","[fold0] Batch 23 | Loss 1.4093\n","[fold0] Train â†’ Loss 1.4093\n","[fold0] â±  65.6s\n","\n","[fold0] â”€â”€ Epoch 21/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4170\n","[fold0] Batch 2 | Loss 1.4153\n","[fold0] Batch 3 | Loss 1.4078\n","[fold0] Batch 4 | Loss 1.4058\n","[fold0] Batch 5 | Loss 1.4051\n","[fold0] Batch 6 | Loss 1.4109\n","[fold0] Batch 7 | Loss 1.4110\n","[fold0] Batch 8 | Loss 1.4058\n","[fold0] Batch 9 | Loss 1.4066\n","[fold0] Batch 10 | Loss 1.4054\n","[fold0] Batch 11 | Loss 1.4044\n","[fold0] Batch 12 | Loss 1.4032\n","[fold0] Batch 13 | Loss 1.4027\n","[fold0] Batch 14 | Loss 1.4016\n","[fold0] Batch 15 | Loss 1.4026\n","[fold0] Batch 16 | Loss 1.4016\n","[fold0] Batch 17 | Loss 1.4004\n","[fold0] Batch 18 | Loss 1.4005\n","[fold0] Batch 19 | Loss 1.4000\n","[fold0] Batch 20 | Loss 1.3998\n","[fold0] Batch 21 | Loss 1.3995\n","[fold0] Batch 22 | Loss 1.4019\n","[fold0] Batch 23 | Loss 1.4018\n","[fold0] Train â†’ Loss 1.4018\n","[fold0] â±  65.1s\n","\n","[fold0] â”€â”€ Epoch 22/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3818\n","[fold0] Batch 2 | Loss 1.3763\n","[fold0] Batch 3 | Loss 1.3865\n","[fold0] Batch 4 | Loss 1.3997\n","[fold0] Batch 5 | Loss 1.4080\n","[fold0] Batch 6 | Loss 1.4064\n","[fold0] Batch 7 | Loss 1.4067\n","[fold0] Batch 8 | Loss 1.4073\n","[fold0] Batch 9 | Loss 1.4074\n","[fold0] Batch 10 | Loss 1.4030\n","[fold0] Batch 11 | Loss 1.4026\n","[fold0] Batch 12 | Loss 1.4079\n","[fold0] Batch 13 | Loss 1.4096\n","[fold0] Batch 14 | Loss 1.4059\n","[fold0] Batch 15 | Loss 1.4031\n","[fold0] Batch 16 | Loss 1.4032\n","[fold0] Batch 17 | Loss 1.4042\n","[fold0] Batch 18 | Loss 1.4039\n","[fold0] Batch 19 | Loss 1.4037\n","[fold0] Batch 20 | Loss 1.4022\n","[fold0] Batch 21 | Loss 1.4016\n","[fold0] Batch 22 | Loss 1.4007\n","[fold0] Batch 23 | Loss 1.4006\n","[fold0] Train â†’ Loss 1.4006\n","[fold0] â±  65.2s\n","\n","[fold0] â”€â”€ Epoch 23/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.3915\n","[fold0] Batch 2 | Loss 1.3918\n","[fold0] Batch 3 | Loss 1.3972\n","[fold0] Batch 4 | Loss 1.3930\n","[fold0] Batch 5 | Loss 1.3909\n","[fold0] Batch 6 | Loss 1.3880\n","[fold0] Batch 7 | Loss 1.3865\n","[fold0] Batch 8 | Loss 1.3877\n","[fold0] Batch 9 | Loss 1.3896\n","[fold0] Batch 10 | Loss 1.3927\n","[fold0] Batch 11 | Loss 1.3910\n","[fold0] Batch 12 | Loss 1.3929\n","[fold0] Batch 13 | Loss 1.3937\n","[fold0] Batch 14 | Loss 1.3946\n","[fold0] Batch 15 | Loss 1.3945\n","[fold0] Batch 16 | Loss 1.3938\n","[fold0] Batch 17 | Loss 1.3957\n","[fold0] Batch 18 | Loss 1.3956\n","[fold0] Batch 19 | Loss 1.3972\n","[fold0] Batch 20 | Loss 1.3974\n","[fold0] Batch 21 | Loss 1.3983\n","[fold0] Batch 22 | Loss 1.3987\n","[fold0] Batch 23 | Loss 1.3983\n","[fold0] Train â†’ Loss 1.3983\n","[fold0] â±  64.5s\n","\n","[fold0] â”€â”€ Epoch 24/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4011\n","[fold0] Batch 2 | Loss 1.3975\n","[fold0] Batch 3 | Loss 1.4088\n","[fold0] Batch 4 | Loss 1.4003\n","[fold0] Batch 5 | Loss 1.3952\n","[fold0] Batch 6 | Loss 1.3947\n","[fold0] Batch 7 | Loss 1.3942\n","[fold0] Batch 8 | Loss 1.3940\n","[fold0] Batch 9 | Loss 1.3941\n","[fold0] Batch 10 | Loss 1.3923\n","[fold0] Batch 11 | Loss 1.3927\n","[fold0] Batch 12 | Loss 1.3934\n","[fold0] Batch 13 | Loss 1.3943\n","[fold0] Batch 14 | Loss 1.3934\n","[fold0] Batch 15 | Loss 1.3934\n","[fold0] Batch 16 | Loss 1.3956\n","[fold0] Batch 17 | Loss 1.3977\n","[fold0] Batch 18 | Loss 1.3982\n","[fold0] Batch 19 | Loss 1.3987\n","[fold0] Batch 20 | Loss 1.3976\n","[fold0] Batch 21 | Loss 1.3981\n","[fold0] Batch 22 | Loss 1.3974\n","[fold0] Batch 23 | Loss 1.3969\n","[fold0] Train â†’ Loss 1.3969\n","[fold0] â±  64.8s\n","\n","[fold0] â”€â”€ Epoch 25/25 â”€â”€\n","[fold0] Batch 1 | Loss 1.4265\n","[fold0] Batch 2 | Loss 1.3992\n","[fold0] Batch 3 | Loss 1.3948\n","[fold0] Batch 4 | Loss 1.4059\n","[fold0] Batch 5 | Loss 1.3891\n","[fold0] Batch 6 | Loss 1.3938\n","[fold0] Batch 7 | Loss 1.3979\n","[fold0] Batch 8 | Loss 1.4048\n","[fold0] Batch 9 | Loss 1.4059\n","[fold0] Batch 10 | Loss 1.4053\n","[fold0] Batch 11 | Loss 1.4071\n","[fold0] Batch 12 | Loss 1.4069\n","[fold0] Batch 13 | Loss 1.4077\n","[fold0] Batch 14 | Loss 1.4079\n","[fold0] Batch 15 | Loss 1.4063\n","[fold0] Batch 16 | Loss 1.4052\n","[fold0] Batch 17 | Loss 1.4065\n","[fold0] Batch 18 | Loss 1.4031\n","[fold0] Batch 19 | Loss 1.4036\n","[fold0] Batch 20 | Loss 1.4062\n","[fold0] Batch 21 | Loss 1.4071\n","[fold0] Batch 22 | Loss 1.4060\n","[fold0] Batch 23 | Loss 1.4046\n","[fold0] Train â†’ Loss 1.4046\n","[fold0] â±  64.8s\n","\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 24it [00:12,  1.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (train) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold0/training/rotation_features_train_fold0.pt\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 8it [00:04,  1.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (val) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold0/training/rotation_features_val_fold0.pt\n","âœ… Loaded 1473 keys and (1473, 4) features\n","ğŸ“Š Class distribution:\n","Counter({np.str_('CHROMO'): 300, np.str_('ONCO'): 298, np.str_('ccRCC'): 297, np.str_('pRCC'): 294, np.str_('not_tumor'): 284})\n","âœ… Filtered dataset: 1473 samples\n","              precision    recall  f1-score   support\n","\n","      CHROMO       0.53      0.90      0.67        60\n","        ONCO       0.74      0.33      0.46        60\n","       ccRCC       0.56      0.25      0.35        59\n","   not_tumor       0.45      0.42      0.44        57\n","        pRCC       0.54      0.80      0.64        59\n","\n","    accuracy                           0.54       295\n","   macro avg       0.56      0.54      0.51       295\n","weighted avg       0.57      0.54      0.51       295\n","\n","Confusion Matrix:\n"," [[54  4  0  0  2]\n"," [29 20  3  1  7]\n"," [13  2 15 25  4]\n"," [ 2  0  4 24 27]\n"," [ 3  1  5  3 47]]\n","ğŸ’¾ Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold0/training/rotation_classifier_fold0.joblib\n","[fold0] âœ…  completato\n","\n","[fold1] ğŸ“‚  ckpt dir      â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold1/training\n","[fold1] ğŸ·   train shards  â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold1/train/patches-0000.tar\n","[fold1] ğŸš€  avvio trainer  â†’ 'rotation'\n","[fold1] âœ… encoder da fold0 â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold0/training/RotationTrainer_bestepoch024.pt\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 24it [00:12,  1.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (train) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold1/training/rotation_features_train_fold1.pt\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 8it [00:05,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (val) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold1/training/rotation_features_val_fold1.pt\n","âœ… Loaded 1506 keys and (1506, 4) features\n","ğŸ“Š Class distribution:\n","Counter({np.str_('pRCC'): 309, np.str_('ONCO'): 308, np.str_('CHROMO'): 303, np.str_('ccRCC'): 293, np.str_('not_tumor'): 293})\n","âœ… Filtered dataset: 1506 samples\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      CHROMO       0.68      0.90      0.77        61\n","        ONCO       0.50      0.44      0.47        62\n","       ccRCC       0.00      0.00      0.00        59\n","   not_tumor       0.40      0.90      0.55        58\n","        pRCC       0.50      0.29      0.37        62\n","\n","    accuracy                           0.50       302\n","   macro avg       0.42      0.50      0.43       302\n","weighted avg       0.42      0.50      0.43       302\n","\n","Confusion Matrix:\n"," [[55  3  0  2  1]\n"," [19 27  0 14  2]\n"," [ 5 12  0 31 11]\n"," [ 2  0  0 52  4]\n"," [ 0 12  0 32 18]]\n","ğŸ’¾ Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold1/training/rotation_classifier_fold1.joblib\n","[fold1] âœ…  completato\n","\n","[fold2] ğŸ“‚  ckpt dir      â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold2/training\n","[fold2] ğŸ·   train shards  â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold2/train/patches-0000.tar\n","[fold2] ğŸš€  avvio trainer  â†’ 'rotation'\n","[fold2] âœ… encoder da fold0 â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold0/training/RotationTrainer_bestepoch024.pt\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 23it [00:12,  1.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (train) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold2/training/rotation_features_train_fold2.pt\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 9it [00:04,  2.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (val) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold2/training/rotation_features_val_fold2.pt\n","âœ… Loaded 1452 keys and (1452, 4) features\n","ğŸ“Š Class distribution:\n","Counter({np.str_('CHROMO'): 298, np.str_('ccRCC'): 297, np.str_('ONCO'): 291, np.str_('pRCC'): 283, np.str_('not_tumor'): 283})\n","âœ… Filtered dataset: 1452 samples\n","              precision    recall  f1-score   support\n","\n","      CHROMO       0.58      0.88      0.70        60\n","        ONCO       0.47      0.14      0.21        58\n","       ccRCC       0.00      0.00      0.00        59\n","   not_tumor       0.34      0.68      0.45        57\n","        pRCC       0.52      0.60      0.55        57\n","\n","    accuracy                           0.46       291\n","   macro avg       0.38      0.46      0.38       291\n","weighted avg       0.38      0.46      0.38       291\n","\n","Confusion Matrix:\n"," [[53  0  1  3  3]\n"," [26  8  0 20  4]\n"," [ 8  5  0 34 12]\n"," [ 5  0  0 39 13]\n"," [ 0  4  0 19 34]]\n","ğŸ’¾ Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold2/training/rotation_classifier_fold2.joblib\n","[fold2] âœ…  completato\n","\n","[fold3] ğŸ“‚  ckpt dir      â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold3/training\n","[fold3] ğŸ·   train shards  â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold3/train/patches-0000.tar\n","[fold3] ğŸš€  avvio trainer  â†’ 'rotation'\n","[fold3] âœ… encoder da fold0 â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold0/training/RotationTrainer_bestepoch024.pt\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 23it [00:12,  1.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (train) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold3/training/rotation_features_train_fold3.pt\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features: 8it [00:04,  1.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Rotation features (val) saved â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold3/training/rotation_features_val_fold3.pt\n","âœ… Loaded 1466 keys and (1466, 4) features\n","ğŸ“Š Class distribution:\n","Counter({np.str_('CHROMO'): 300, np.str_('ONCO'): 298, np.str_('pRCC'): 294, np.str_('ccRCC'): 290, np.str_('not_tumor'): 284})\n","âœ… Filtered dataset: 1466 samples\n","              precision    recall  f1-score   support\n","\n","      CHROMO       0.54      0.95      0.69        60\n","        ONCO       0.12      0.02      0.03        60\n","       ccRCC       0.51      0.36      0.42        58\n","   not_tumor       0.39      0.53      0.45        57\n","        pRCC       0.52      0.56      0.54        59\n","\n","    accuracy                           0.48       294\n","   macro avg       0.42      0.48      0.43       294\n","weighted avg       0.42      0.48      0.43       294\n","\n","Confusion Matrix:\n"," [[57  0  1  2  0]\n"," [32  1  6 12  9]\n"," [ 9  5 21 15  8]\n"," [ 7  0  6 30 14]\n"," [ 0  2  7 17 33]]\n","ğŸ’¾ Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/rotation/fold3/training/rotation_classifier_fold3.joblib\n","[fold3] âœ…  completato\n","\n","\n","ğŸš€  Modello 'simclr'  (SSL) â€“ epochs=25\n","[fold0] ğŸ“‚  ckpt dir      â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723180604/simclr/fold0/training\n","[fold0] ğŸ·   train shards  â†’ /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n","[fold0] ğŸš€  avvio trainer  â†’ 'simclr'\n","[fold0] â”€â”€ Epoch 1/25 â”€â”€\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"stream","name":"stdout","text":["[fold0] Batch 1 | Loss 4.8367\n","[fold0] Batch 2 | Loss 4.8761\n","[fold0] Batch 3 | Loss 4.8965\n","[fold0] Batch 4 | Loss 4.8769\n","[fold0] Batch 5 | Loss 4.8648\n","[fold0] Batch 6 | Loss 4.8552\n","[fold0] Batch 7 | Loss 4.8368\n","[fold0] Batch 8 | Loss 4.8172\n","[fold0] Batch 9 | Loss 4.7884\n","[fold0] Batch 10 | Loss 4.7861\n","[fold0] Batch 11 | Loss 4.7845\n","[fold0] Batch 12 | Loss 4.7922\n","[fold0] Batch 13 | Loss 4.7979\n","[fold0] Batch 14 | Loss 4.8059\n","[fold0] Batch 15 | Loss 4.8073\n","[fold0] Batch 16 | Loss 4.8070\n","[fold0] Batch 17 | Loss 4.8037\n","[fold0] Batch 18 | Loss 4.8101\n","[fold0] Batch 19 | Loss 4.8181\n","[fold0] Batch 20 | Loss 4.8185\n","[fold0] Batch 21 | Loss 4.8181\n","[fold0] Batch 22 | Loss 4.8177\n","[fold0] Batch 23 | Loss 4.8164\n","[fold0] Train â†’ Loss 4.8164\n","[fold0] â±  173.8s\n","\n","[fold0] â”€â”€ Epoch 2/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.7789\n","[fold0] Batch 2 | Loss 4.8016\n","[fold0] Batch 3 | Loss 4.8104\n","[fold0] Batch 4 | Loss 4.8027\n","[fold0] Batch 5 | Loss 4.7956\n","[fold0] Batch 6 | Loss 4.7885\n","[fold0] Batch 7 | Loss 4.7795\n","[fold0] Batch 8 | Loss 4.7705\n","[fold0] Batch 9 | Loss 4.7662\n","[fold0] Batch 10 | Loss 4.7558\n","[fold0] Batch 11 | Loss 4.7582\n","[fold0] Batch 12 | Loss 4.7680\n","[fold0] Batch 13 | Loss 4.7726\n","[fold0] Batch 14 | Loss 4.7762\n","[fold0] Batch 15 | Loss 4.7737\n","[fold0] Batch 16 | Loss 4.7740\n","[fold0] Batch 17 | Loss 4.7704\n","[fold0] Batch 18 | Loss 4.7637\n","[fold0] Batch 19 | Loss 4.7594\n","[fold0] Batch 20 | Loss 4.7574\n","[fold0] Batch 21 | Loss 4.7531\n","[fold0] Batch 22 | Loss 4.7483\n","[fold0] Batch 23 | Loss 4.7436\n","[fold0] Train â†’ Loss 4.7436\n","[fold0] â±  173.4s\n","\n","[fold0] â”€â”€ Epoch 3/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.6919\n","[fold0] Batch 2 | Loss 4.7244\n","[fold0] Batch 3 | Loss 4.7254\n","[fold0] Batch 4 | Loss 4.7578\n","[fold0] Batch 5 | Loss 4.7462\n","[fold0] Batch 6 | Loss 4.7215\n","[fold0] Batch 7 | Loss 4.6908\n","[fold0] Batch 8 | Loss 4.6742\n","[fold0] Batch 9 | Loss 4.6773\n","[fold0] Batch 10 | Loss 4.6743\n","[fold0] Batch 11 | Loss 4.6758\n","[fold0] Batch 12 | Loss 4.6833\n","[fold0] Batch 13 | Loss 4.6916\n","[fold0] Batch 14 | Loss 4.6885\n","[fold0] Batch 15 | Loss 4.6847\n","[fold0] Batch 16 | Loss 4.6912\n","[fold0] Batch 17 | Loss 4.6916\n","[fold0] Batch 18 | Loss 4.6869\n","[fold0] Batch 19 | Loss 4.6842\n","[fold0] Batch 20 | Loss 4.6802\n","[fold0] Batch 21 | Loss 4.6720\n","[fold0] Batch 22 | Loss 4.6730\n","[fold0] Batch 23 | Loss 4.6713\n","[fold0] Train â†’ Loss 4.6713\n","[fold0] â±  194.2s\n","\n","[fold0] â”€â”€ Epoch 4/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.7915\n","[fold0] Batch 2 | Loss 4.8250\n","[fold0] Batch 3 | Loss 4.7705\n","[fold0] Batch 4 | Loss 4.7494\n","[fold0] Batch 5 | Loss 4.7387\n","[fold0] Batch 6 | Loss 4.7118\n","[fold0] Batch 7 | Loss 4.6893\n","[fold0] Batch 8 | Loss 4.6695\n","[fold0] Batch 9 | Loss 4.6638\n","[fold0] Batch 10 | Loss 4.6533\n","[fold0] Batch 11 | Loss 4.6633\n","[fold0] Batch 12 | Loss 4.6662\n","[fold0] Batch 13 | Loss 4.6698\n","[fold0] Batch 14 | Loss 4.6808\n","[fold0] Batch 15 | Loss 4.6855\n","[fold0] Batch 16 | Loss 4.6898\n","[fold0] Batch 17 | Loss 4.6884\n","[fold0] Batch 18 | Loss 4.6825\n","[fold0] Batch 19 | Loss 4.6770\n","[fold0] Batch 20 | Loss 4.6736\n","[fold0] Batch 21 | Loss 4.6693\n","[fold0] Batch 22 | Loss 4.6619\n","[fold0] Batch 23 | Loss 4.6586\n","[fold0] Train â†’ Loss 4.6586\n","[fold0] â±  212.9s\n","\n","[fold0] â”€â”€ Epoch 5/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.7935\n","[fold0] Batch 2 | Loss 4.8178\n","[fold0] Batch 3 | Loss 4.8036\n","[fold0] Batch 4 | Loss 4.8114\n","[fold0] Batch 5 | Loss 4.7945\n","[fold0] Batch 6 | Loss 4.7487\n","[fold0] Batch 7 | Loss 4.7072\n","[fold0] Batch 8 | Loss 4.7017\n","[fold0] Batch 9 | Loss 4.6909\n","[fold0] Batch 10 | Loss 4.6840\n","[fold0] Batch 11 | Loss 4.6789\n","[fold0] Batch 12 | Loss 4.6834\n","[fold0] Batch 13 | Loss 4.6909\n","[fold0] Batch 14 | Loss 4.6889\n","[fold0] Batch 15 | Loss 4.7090\n","[fold0] Batch 16 | Loss 4.7152\n","[fold0] Batch 17 | Loss 4.7065\n","[fold0] Batch 18 | Loss 4.6996\n","[fold0] Batch 19 | Loss 4.6947\n","[fold0] Batch 20 | Loss 4.6888\n","[fold0] Batch 21 | Loss 4.6824\n","[fold0] Batch 22 | Loss 4.6768\n","[fold0] Batch 23 | Loss 4.6706\n","[fold0] Train â†’ Loss 4.6706\n","[fold0] â±  203.6s\n","\n","[fold0] â”€â”€ Epoch 6/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.6928\n","[fold0] Batch 2 | Loss 4.6165\n","[fold0] Batch 3 | Loss 4.6304\n","[fold0] Batch 4 | Loss 4.6987\n","[fold0] Batch 5 | Loss 4.6764\n","[fold0] Batch 6 | Loss 4.6221\n","[fold0] Batch 7 | Loss 4.6008\n","[fold0] Batch 8 | Loss 4.5890\n","[fold0] Batch 9 | Loss 4.5878\n","[fold0] Batch 10 | Loss 4.5789\n","[fold0] Batch 11 | Loss 4.5922\n","[fold0] Batch 12 | Loss 4.6037\n","[fold0] Batch 13 | Loss 4.6033\n","[fold0] Batch 14 | Loss 4.5965\n","[fold0] Batch 15 | Loss 4.5999\n","[fold0] Batch 16 | Loss 4.6014\n","[fold0] Batch 17 | Loss 4.6031\n","[fold0] Batch 18 | Loss 4.5946\n","[fold0] Batch 19 | Loss 4.5968\n","[fold0] Batch 20 | Loss 4.5956\n","[fold0] Batch 21 | Loss 4.5873\n","[fold0] Batch 22 | Loss 4.5784\n","[fold0] Batch 23 | Loss 4.5708\n","[fold0] Train â†’ Loss 4.5708\n","[fold0] â±  210.5s\n","\n","[fold0] â”€â”€ Epoch 7/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.5084\n","[fold0] Batch 2 | Loss 4.5785\n","[fold0] Batch 3 | Loss 4.5941\n","[fold0] Batch 4 | Loss 4.6005\n","[fold0] Batch 5 | Loss 4.6175\n","[fold0] Batch 6 | Loss 4.5782\n","[fold0] Batch 7 | Loss 4.5609\n","[fold0] Batch 8 | Loss 4.5687\n","[fold0] Batch 9 | Loss 4.5825\n","[fold0] Batch 10 | Loss 4.5782\n","[fold0] Batch 11 | Loss 4.5918\n","[fold0] Batch 12 | Loss 4.6002\n","[fold0] Batch 13 | Loss 4.6077\n","[fold0] Batch 14 | Loss 4.6068\n","[fold0] Batch 15 | Loss 4.6200\n","[fold0] Batch 16 | Loss 4.6181\n","[fold0] Batch 17 | Loss 4.6143\n","[fold0] Batch 18 | Loss 4.6027\n","[fold0] Batch 19 | Loss 4.5967\n","[fold0] Batch 20 | Loss 4.5900\n","[fold0] Batch 21 | Loss 4.5852\n","[fold0] Batch 22 | Loss 4.5791\n","[fold0] Batch 23 | Loss 4.5766\n","[fold0] Train â†’ Loss 4.5766\n","[fold0] â±  201.2s\n","\n","[fold0] â”€â”€ Epoch 8/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.6703\n","[fold0] Batch 2 | Loss 4.6262\n","[fold0] Batch 3 | Loss 4.6467\n","[fold0] Batch 4 | Loss 4.6826\n","[fold0] Batch 5 | Loss 4.6635\n","[fold0] Batch 6 | Loss 4.6063\n","[fold0] Batch 7 | Loss 4.5928\n","[fold0] Batch 8 | Loss 4.5824\n","[fold0] Batch 9 | Loss 4.5680\n","[fold0] Batch 10 | Loss 4.5639\n","[fold0] Batch 11 | Loss 4.5739\n","[fold0] Batch 12 | Loss 4.5904\n","[fold0] Batch 13 | Loss 4.5876\n","[fold0] Batch 14 | Loss 4.5858\n","[fold0] Batch 15 | Loss 4.6002\n","[fold0] Batch 16 | Loss 4.6013\n","[fold0] Batch 17 | Loss 4.5952\n","[fold0] Batch 18 | Loss 4.6081\n","[fold0] Batch 19 | Loss 4.6104\n","[fold0] Batch 20 | Loss 4.6083\n","[fold0] Batch 21 | Loss 4.6138\n","[fold0] Batch 22 | Loss 4.6132\n","[fold0] Batch 23 | Loss 4.6116\n","[fold0] Train â†’ Loss 4.6116\n","[fold0] â±  184.0s\n","\n","[fold0] â”€â”€ Epoch 9/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.6606\n","[fold0] Batch 2 | Loss 4.6294\n","[fold0] Batch 3 | Loss 4.5986\n","[fold0] Batch 4 | Loss 4.6354\n","[fold0] Batch 5 | Loss 4.6161\n","[fold0] Batch 6 | Loss 4.5817\n","[fold0] Batch 7 | Loss 4.5688\n","[fold0] Batch 8 | Loss 4.5711\n","[fold0] Batch 9 | Loss 4.5618\n","[fold0] Batch 10 | Loss 4.5551\n","[fold0] Batch 11 | Loss 4.5493\n","[fold0] Batch 12 | Loss 4.5461\n","[fold0] Batch 13 | Loss 4.5256\n","[fold0] Batch 14 | Loss 4.5251\n","[fold0] Batch 15 | Loss 4.5279\n","[fold0] Batch 16 | Loss 4.5379\n","[fold0] Batch 17 | Loss 4.5422\n","[fold0] Batch 18 | Loss 4.5204\n","[fold0] Batch 19 | Loss 4.5235\n","[fold0] Batch 20 | Loss 4.5157\n","[fold0] Batch 21 | Loss 4.5061\n","[fold0] Batch 22 | Loss 4.5056\n","[fold0] Batch 23 | Loss 4.5002\n","[fold0] Train â†’ Loss 4.5002\n","[fold0] â±  204.3s\n","\n","[fold0] â”€â”€ Epoch 10/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.8079\n","[fold0] Batch 2 | Loss 4.6569\n","[fold0] Batch 3 | Loss 4.6143\n","[fold0] Batch 4 | Loss 4.6516\n","[fold0] Batch 5 | Loss 4.6390\n","[fold0] Batch 6 | Loss 4.6002\n","[fold0] Batch 7 | Loss 4.5896\n","[fold0] Batch 8 | Loss 4.5727\n","[fold0] Batch 9 | Loss 4.5543\n","[fold0] Batch 10 | Loss 4.5323\n","[fold0] Batch 11 | Loss 4.5319\n","[fold0] Batch 12 | Loss 4.5385\n","[fold0] Batch 13 | Loss 4.5413\n","[fold0] Batch 14 | Loss 4.5312\n","[fold0] Batch 15 | Loss 4.5480\n","[fold0] Batch 16 | Loss 4.5529\n","[fold0] Batch 17 | Loss 4.5538\n","[fold0] Batch 18 | Loss 4.5380\n","[fold0] Batch 19 | Loss 4.5316\n","[fold0] Batch 20 | Loss 4.5274\n","[fold0] Batch 21 | Loss 4.5247\n","[fold0] Batch 22 | Loss 4.5178\n","[fold0] Batch 23 | Loss 4.5093\n","[fold0] Train â†’ Loss 4.5093\n","[fold0] â±  193.5s\n","\n","[fold0] â”€â”€ Epoch 11/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.5671\n","[fold0] Batch 2 | Loss 4.5443\n","[fold0] Batch 3 | Loss 4.4675\n","[fold0] Batch 4 | Loss 4.5026\n","[fold0] Batch 5 | Loss 4.4970\n","[fold0] Batch 6 | Loss 4.4855\n","[fold0] Batch 7 | Loss 4.4638\n","[fold0] Batch 8 | Loss 4.4590\n","[fold0] Batch 9 | Loss 4.4643\n","[fold0] Batch 10 | Loss 4.4536\n","[fold0] Batch 11 | Loss 4.4489\n","[fold0] Batch 12 | Loss 4.4548\n","[fold0] Batch 13 | Loss 4.4594\n","[fold0] Batch 14 | Loss 4.4579\n","[fold0] Batch 15 | Loss 4.4524\n","[fold0] Batch 16 | Loss 4.4545\n","[fold0] Batch 17 | Loss 4.4470\n","[fold0] Batch 18 | Loss 4.4250\n","[fold0] Batch 19 | Loss 4.4170\n","[fold0] Batch 20 | Loss 4.4061\n","[fold0] Batch 21 | Loss 4.3950\n","[fold0] Batch 22 | Loss 4.3831\n","[fold0] Batch 23 | Loss 4.3778\n","[fold0] Train â†’ Loss 4.3778\n","[fold0] â±  203.8s\n","\n","[fold0] â”€â”€ Epoch 12/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.6158\n","[fold0] Batch 2 | Loss 4.4988\n","[fold0] Batch 3 | Loss 4.4865\n","[fold0] Batch 4 | Loss 4.5923\n","[fold0] Batch 5 | Loss 4.5842\n","[fold0] Batch 6 | Loss 4.5073\n","[fold0] Batch 7 | Loss 4.4763\n","[fold0] Batch 8 | Loss 4.4479\n","[fold0] Batch 9 | Loss 4.4586\n","[fold0] Batch 10 | Loss 4.4547\n","[fold0] Batch 11 | Loss 4.4714\n","[fold0] Batch 12 | Loss 4.4717\n","[fold0] Batch 13 | Loss 4.4762\n","[fold0] Batch 14 | Loss 4.4685\n","[fold0] Batch 15 | Loss 4.4709\n","[fold0] Batch 16 | Loss 4.4705\n","[fold0] Batch 17 | Loss 4.4649\n","[fold0] Batch 18 | Loss 4.4551\n","[fold0] Batch 19 | Loss 4.4376\n","[fold0] Batch 20 | Loss 4.4375\n","[fold0] Batch 21 | Loss 4.4257\n","[fold0] Batch 22 | Loss 4.4096\n","[fold0] Batch 23 | Loss 4.4042\n","[fold0] Train â†’ Loss 4.4042\n","[fold0] â±  174.7s\n","\n","[fold0] â”€â”€ Epoch 13/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.5008\n","[fold0] Batch 2 | Loss 4.4978\n","[fold0] Batch 3 | Loss 4.4649\n","[fold0] Batch 4 | Loss 4.5015\n","[fold0] Batch 5 | Loss 4.4763\n","[fold0] Batch 6 | Loss 4.4353\n","[fold0] Batch 7 | Loss 4.4248\n","[fold0] Batch 8 | Loss 4.4290\n","[fold0] Batch 9 | Loss 4.4210\n","[fold0] Batch 10 | Loss 4.4054\n","[fold0] Batch 11 | Loss 4.3902\n","[fold0] Batch 12 | Loss 4.3912\n","[fold0] Batch 13 | Loss 4.3850\n","[fold0] Batch 14 | Loss 4.3648\n","[fold0] Batch 15 | Loss 4.3622\n","[fold0] Batch 16 | Loss 4.3617\n","[fold0] Batch 17 | Loss 4.3701\n","[fold0] Batch 18 | Loss 4.3749\n","[fold0] Batch 19 | Loss 4.3663\n","[fold0] Batch 20 | Loss 4.3569\n","[fold0] Batch 21 | Loss 4.3536\n","[fold0] Batch 22 | Loss 4.3438\n","[fold0] Batch 23 | Loss 4.3313\n","[fold0] Train â†’ Loss 4.3313\n","[fold0] â±  205.8s\n","\n","[fold0] â”€â”€ Epoch 14/25 â”€â”€\n","[fold0] Batch 1 | Loss 4.5258\n","[fold0] Batch 2 | Loss 4.4973\n","[fold0] Batch 3 | Loss 4.5047\n","[fold0] Batch 4 | Loss 4.5432\n","[fold0] Batch 5 | Loss 4.5006\n","[fold0] Batch 6 | Loss 4.4618\n","[fold0] Batch 7 | Loss 4.4188\n","[fold0] Batch 8 | Loss 4.3838\n"]}],"source":["# %% -------------------------------------------------------------------- #\n","# Cell 6 â€“ Modular Launch & Auto-Recover (cross-fold)                     #\n","# ------------------------------------------------------------------------#\n","from pathlib import Path\n","import contextlib, sys, torch, os\n","from utils.training_utils.device_io import get_latest_checkpoint, load_checkpoint\n","\n","def launch_training(cfg: dict) -> None:\n","    \"\"\"Esegue training + generazione artefatti per tutti i modelli e fold.\"\"\"\n","    for name, m_cfg in _select_models(cfg).items():\n","        is_ssl = (m_cfg.get(\"type\") == \"ssl\")\n","        epochs = int(m_cfg[\"training\"][\"epochs\"])\n","        print(f\"\\nğŸš€  Modello '{name}'  ({'SSL' if is_ssl else 'SL'}) â€“ epochs={epochs}\")\n","\n","        for fold in cfg[\"folds\"]:\n","            # 1) Configurazione dati\n","            data_cfg = {\n","                \"train\": str((PROJECT_ROOT / cfg[\"data\"][\"train\"]\n","                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"], fold_idx=fold))\n","                             .resolve()),\n","                \"val\":   str((PROJECT_ROOT / cfg[\"data\"][\"val\"]\n","                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"], fold_idx=fold))\n","                             .resolve()),\n","                \"test\":  str((PROJECT_ROOT / cfg[\"data\"][\"test\"]\n","                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"]))\n","                             .resolve())\n","            }\n","            if not Path(data_cfg[\"train\"]).exists():\n","                print(f\"[fold{fold}] âš ï¸  shard train mancante â†’ skip\")\n","                continue\n","\n","            # 2) Path output + trainer\n","            paths   = _paths(cfg, name, fold)\n","            trainer = _init_trainer(name, m_cfg, data_cfg, paths[\"ckpt_dir\"])\n","            trainer.cfg_fold     = fold\n","            trainer.train_loader = trainer.build_loader(\"train\")\n","            if hasattr(trainer, \"validate_epoch\"):\n","                trainer.val_loader = trainer.build_loader(\"val\")\n","\n","            # 3) Logging su stdout + file\n","            with open(paths[\"log\"], \"a\") as logf, \\\n","                 contextlib.redirect_stdout(_Tee(sys.stdout, logf)), \\\n","                 contextlib.redirect_stderr(_Tee(sys.stderr, logf)):\n","\n","                print(f\"[fold{fold}] ğŸ“‚  ckpt dir      â†’ {paths['ckpt_dir'].resolve()}\")\n","                print(f\"[fold{fold}] ğŸ·   train shards  â†’ {Path(data_cfg['train']).resolve()}\")\n","                print(f\"[fold{fold}] ğŸš€  avvio trainer  â†’ '{name}'\")\n","\n","                # 3.1) Se abbiamo giÃ  tutti gli artefatti, salta interamente\n","                if _completed(paths, is_ssl):\n","                    print(f\"[fold{fold}] âš¡  Artefatti giÃ  presenti â†’ skip training + SSL pipeline\")\n","                else:\n","                    # 4) SSL: encoder once su fold0 oppure training completo\n","                    if is_ssl and cfg.get(\"train_encoder_once\", False) and fold > 0:\n","                        fold0_ckpt = get_latest_checkpoint(_paths(cfg, name, 0)[\"ckpt_dir\"])\n","                        if fold0_ckpt:\n","                            mdl, _ = trainer.get_resume_model_and_optimizer()\n","                            load_checkpoint(fold0_ckpt, mdl, None)\n","                            print(f\"[fold{fold}] âœ… encoder da fold0 â†’ {fold0_ckpt.resolve()}\")\n","                        else:\n","                            print(f\"[fold{fold}] âš ï¸  encoder fold0 mancante â†’ train completo\")\n","                            _resume_or_train(trainer, paths, epochs)\n","                    else:\n","                        _resume_or_train(trainer, paths, epochs)\n","\n","                    # 5) SSL â€“ Feature extraction, probe, T-scaling\n","                    if is_ssl:\n","                        _ensure_ssl_artifacts(trainer, paths)\n","\n","                # 6) Append a esperimenti globali (anche in caso di skip)\n","                latest_ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","                if latest_ckpt:\n","                    base_dir = EXP_BASE.parent.parent.resolve()\n","                    try:\n","                        rel_path = os.path.relpath(str(latest_ckpt.resolve()), str(base_dir))\n","                    except Exception:\n","                        rel_path = str(latest_ckpt.resolve())\n","                else:\n","                    rel_path = \"-\"\n","                _global_experiments_append(\n","                    f\"| {cfg['exp_code']} | {name} | fold{fold} | {epochs} | {rel_path} |\"\n","                )\n","\n","                print(f\"[fold{fold}] âœ…  completato\\n\")\n","\n","# â”€â”€â”€ Avvio automatico in Colab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","if IN_COLAB:\n","    launch_training(cfg)\n","else:\n","    print(\"â©  Ambiente locale: esegui manualmente  launch_training(cfg) per partire.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6196183d"},"outputs":[],"source":["from pathlib import Path\n","\n","# Configura esperimento\n","EXP_CODE = \"\"\n","ROOT = Path(\"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\")\n","EXP_DIR = ROOT / \"data/processed/dataset_9f30917e/experiments\" / EXP_CODE\n","\n","# Modelli\n","SSL_MODELS = {\"simclr\", \"rotation\", \"moco_v2\", \"jepa\"}\n","SL_MODELS  = {\"supervised\", \"transfer\"}\n","ALL_MODELS = SSL_MODELS | SL_MODELS\n","N_FOLDS = 2\n","\n","# File attesi per tipo di modello\n","FILES_PER_MODEL = {\n","    \"SSL\": [\n","        \"{model}_bestepoch*_fold{i}.pt\",\n","        \"{model}_train_log_fold{i}.md\",\n","        \"{model}_train_valid_loss_fold{i}.json\",\n","    ],\n","    \"SL\": [\n","        \"{model}_bestepoch*_fold{i}.pt\",\n","        \"{model}_train_log_fold{i}.md\",\n","        \"{model}_train_valid_loss_fold{i}.json\",\n","    ],\n","}\n","\n","missing = []\n","\n","for model in sorted(ALL_MODELS):\n","    model_type = \"SSL\" if model in SSL_MODELS else \"SL\"\n","    folds = [0] if model_type == \"SSL\" else list(range(N_FOLDS))\n","\n","    for i in folds:\n","        train_dir = EXP_DIR / model / f\"fold{i}\" / \"training\"\n","        for pattern in FILES_PER_MODEL[model_type]:\n","            pattern_path = pattern.format(model=model, i=i)\n","            matched = list(train_dir.glob(pattern_path))  # usa direttamente il pattern\n","            if not matched:\n","                missing.append(str(train_dir / pattern_path))\n","\n","# Stampa risultato\n","print(\"ğŸ“‚ Verifica artefatti TRAINING\\n\")\n","if not missing:\n","    print(\"âœ… Tutti i file richiesti sono presenti.\")\n","else:\n","    print(f\"âŒ Mancano {len(missing)} artefatti TRAINING:\\n\")\n","    for m in missing:\n","        print(\" â€¢\", m)\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}