{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118033,
     "status": "ok",
     "timestamp": 1749305392367,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "8cOZCcsp0RCa",
    "outputId": "e1bb265d-82c7-45dc-a114-5cf52d792200"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "!pip install --quiet torch torchvision webdataset tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13301,
     "status": "ok",
     "timestamp": 1749305405756,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "CPkMc3ONzi0J",
    "outputId": "b0dee59a-0134-494c-e50a-e2e92b126243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 [DEBUG] Avvio configurazione ambiente...\n",
      "💻 [DEBUG] Ambiente locale rilevato (VSCode o simile).\n",
      "📁 [DEBUG] PROJECT_ROOT → /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\n",
      "🔧 [DEBUG] Installazione pacchetti mancanti: ['pillow', 'pyyaml']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1 – Environment Setup & Dependencies\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"📦 [DEBUG] Avvio configurazione ambiente...\")\n",
    "\n",
    "# --- Colab detection ---------------------------------------------------------#\n",
    "IN_COLAB = Path(\"/content\").exists()\n",
    "if IN_COLAB:\n",
    "    print(\"📍 [DEBUG] Ambiente Google Colab rilevato.\")\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "else:\n",
    "    print(\"💻 [DEBUG] Ambiente locale rilevato (VSCode o simile).\")\n",
    "\n",
    "# --- Project root ------------------------------------------------------------#\n",
    "ENV_PATHS = {\n",
    "    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n",
    "    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n",
    "}\n",
    "PROJECT_ROOT = Path(ENV_PATHS[\"colab\" if IN_COLAB else \"local\"]).resolve()\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "print(f\"📁 [DEBUG] PROJECT_ROOT → {PROJECT_ROOT}\")\n",
    "\n",
    "# --- Dependencies (installa solo se mancano) ---------------------------------#\n",
    "def _pip_install(pkgs):\n",
    "    import importlib.util, sys, subprocess\n",
    "    missing = [p for p in pkgs if importlib.util.find_spec(p) is None]\n",
    "    if missing:\n",
    "        print(f\"🔧 [DEBUG] Installazione pacchetti mancanti: {missing}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n",
    "    else:\n",
    "        print(\"✅ [DEBUG] Tutti i pacchetti richiesti sono già installati.\")\n",
    "\n",
    "_pip_install([\"torch\", \"torchvision\", \"webdataset\", \"tqdm\", \"pillow\", \"pyyaml\", \"joblib\"])\n",
    "\n",
    "# helper per il tarball del dataset (usato in Cell 2)\n",
    "DATA_TARBALL = PROJECT_ROOT / \"data\" / \"processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749210491818,
     "user": {
      "displayName": "Stefano Bisignano",
      "userId": "12037847569875379268"
     },
     "user_tz": -120
    },
    "id": "lIancWXKKuOY",
    "outputId": "bec2e845-d808-445d-f65a-ad46274e2f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 In Colab=False, VSCode=True\n",
      "🔄 Sync codice progetto → cluster\n",
      "Transfer starting: 109 files\n",
      "hpc_submit.sh\n",
      "config/training.yaml\n",
      "notebooks/4-launch_training.ipynb\n",
      "src/trainers/moco_v2.py\n",
      "\n",
      "sent 13077 bytes  received 534 bytes  79087 bytes/sec\n",
      "total size is 29072991  speedup is 2135.99\n",
      "📦 Dataset già presente sul cluster, skip upload\n",
      "📬 Job submit: Submitted batch job 1201556\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 2 – SLURM Submission (modulare) ─────────────────────────────────────────\n",
    "import os, subprocess, traceback, shutil, yaml\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# ─────────────────────────────── Helpers di basso livello ─────────────────────\n",
    "def _detect_env() -> tuple[bool, bool]:\n",
    "    \"\"\"Rileva se siamo in Colab o VSCode (con VSCODE_PID).\"\"\"\n",
    "    in_colab  = Path(\"/content\").exists()\n",
    "    in_vscode = not in_colab and bool(os.environ.get(\"VSCODE_PID\"))\n",
    "    print(f\"🚀 In Colab={in_colab}, VSCode={in_vscode}\")\n",
    "    return in_colab, in_vscode\n",
    "\n",
    "\n",
    "def _load_env_vars() -> dict:\n",
    "    \"\"\"Carica .env e restituisce le variabili essenziali in un dict.\"\"\"\n",
    "    dotenv_path = find_dotenv()\n",
    "    if not dotenv_path:\n",
    "        raise FileNotFoundError(\"❌ .env non trovato nella root del progetto.\")\n",
    "    load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "    env = {\n",
    "        \"USER\"   : os.getenv(\"CLUSTER_USER\"),\n",
    "        \"HOST\"   : os.getenv(\"CLUSTER_HOST\"),\n",
    "        \"BASE\"   : os.getenv(\"REMOTE_BASE_PATH\"),\n",
    "        \"PW\"     : os.getenv(\"CLUSTER_PASSWORD\", \"\"),\n",
    "        \"MAIL\"   : os.getenv(\"RESPONSABILE_EMAIL\", os.getenv(\"MEMBER_EMAIL\")),\n",
    "        \"PART\"   : os.getenv(\"SBATCH_PARTITION\", \"global\"),\n",
    "        \"MOD\"    : os.getenv(\"SBATCH_MODULE\", \"intel/python/3/2019.4.088\"),\n",
    "    }\n",
    "    missing = [k for k, v in env.items() if k in (\"USER\",\"HOST\",\"BASE\") and not v]\n",
    "    if missing:\n",
    "        raise KeyError(f\"🌱 Mancano variabili nel .env: {missing}\")\n",
    "    return env\n",
    "\n",
    "\n",
    "def _ssh_cmd(env: dict, cmd: str) -> list[str]:\n",
    "    \"\"\"Costruisce il comando SSH (con o senza sshpass).\"\"\"\n",
    "    base = []\n",
    "    if env[\"PW\"] and shutil.which(\"sshpass\"):\n",
    "        base += [\"sshpass\", \"-p\", env[\"PW\"]]\n",
    "    base += [\"ssh\", f\"{env['USER']}@{env['HOST']}\", cmd]\n",
    "    return base\n",
    "\n",
    "\n",
    "def _rsync_cmd(env: dict, src: str, dst: str, extra: list[str] | None = None) -> list[str]:\n",
    "    \"\"\"Costruisce il comando rsync con eventuale sshpass.\"\"\"\n",
    "    cmd = [\"rsync\", \"-avz\"]\n",
    "    if extra:\n",
    "        cmd += extra\n",
    "    if env[\"PW\"] and shutil.which(\"sshpass\"):\n",
    "        cmd += [\"-e\", f\"sshpass -p {env['PW']} ssh -o StrictHostKeyChecking=no\"]\n",
    "    cmd += [src, dst]\n",
    "    return cmd\n",
    "\n",
    "\n",
    "# ─────────────────────────────── Funzioni operative ───────────────────────────\n",
    "def build_sbatch_script(env: dict, script_path: Path):\n",
    "    body = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=rcc_ssrl_launch\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=16G              # puoi alzare la RAM in un colpo solo\n",
    "#SBATCH --time=2:00:00\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --partition={env['PART']}\n",
    "#SBATCH --output=%x_%j.out\n",
    "#SBATCH --mail-type=END,FAIL\n",
    "#SBATCH --mail-user={env['MAIL']}\n",
    "#SBATCH --workdir={env['BASE']}\n",
    "\n",
    "module purge\n",
    "module load {env['MOD']}      # intel/python/3/2019.4.088\n",
    "\n",
    "cd {env['BASE']}\n",
    "python notebooks/4-launch_training_cluster.py --config config/training.yaml\n",
    "\"\"\"\n",
    "    script_path.write_text(body)\n",
    "    script_path.chmod(0o755)\n",
    "\n",
    "\n",
    "def sync_project_code(env: dict):\n",
    "    print(\"🔄 Sync codice progetto → cluster\")\n",
    "    subprocess.run(\n",
    "        _rsync_cmd(env,\n",
    "                   f\"{PROJECT_ROOT}/\",\n",
    "                   f\"{env['USER']}@{env['HOST']}:{env['BASE']}/\",\n",
    "                   [\"--delete\", \"--exclude\", \".git\", \"--exclude\", \"data/processed\"]),\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "\n",
    "def upload_dataset_tar(env: dict, dataset_id: str):\n",
    "    \"\"\"Upload robusto del dataset come tar.gz (crea dir remota prima di rsync).\"\"\"\n",
    "    local_tar   = PROJECT_ROOT / \"data\" / \"processed\" / f\"{dataset_id}.tar.gz\"\n",
    "    remote_dir  = f\"{env['BASE']}/data/processed\"\n",
    "    remote_tar  = f\"{remote_dir}/{dataset_id}.tar.gz\"\n",
    "    remote_ds   = f\"{remote_dir}/{dataset_id}\"\n",
    "\n",
    "    # già presente il dataset estratto?\n",
    "    if subprocess.run(_ssh_cmd(env, f\"test -d {remote_ds}\")).returncode == 0:\n",
    "        print(\"📦 Dataset già presente sul cluster, skip upload\")\n",
    "        return\n",
    "\n",
    "    # crea tarball solo se assente\n",
    "    if not local_tar.exists():\n",
    "        print(\"🗜️  Creo tarball locale del dataset …\")\n",
    "        subprocess.run(\n",
    "            [\"tar\", \"-czf\", str(local_tar), \"-C\",\n",
    "             str(PROJECT_ROOT / \"data\" / \"processed\"), dataset_id],\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "    # assicura la cartella remota data/processed\n",
    "    subprocess.run(_ssh_cmd(env, f\"mkdir -p {remote_dir}\"), check=True)\n",
    "\n",
    "    # rsync del tar.gz\n",
    "    print(\"🚚  Upload tarball …\")\n",
    "    subprocess.run(\n",
    "        _rsync_cmd(env, str(local_tar),\n",
    "                   f\"{env['USER']}@{env['HOST']}:{remote_tar}\"),\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "    # estrazione + (facoltativo) rimozione tar\n",
    "    print(\"📂  Estrazione sul cluster …\")\n",
    "    subprocess.run(_ssh_cmd(env,\n",
    "        f\"tar -xzf {remote_tar} -C {remote_dir} && rm -f {remote_tar}\"),\n",
    "        check=True\n",
    "    )\n",
    "\n",
    "def submit_job(env: dict, script_name: str):\n",
    "    cmd = f\"cd {env['BASE']} && sbatch {script_name}\"\n",
    "    if env[\"PW\"] and shutil.which(\"sshpass\"):\n",
    "        output = subprocess.check_output(_ssh_cmd(env, cmd))\n",
    "        print(\"📬 Job submit:\", output.decode().strip())\n",
    "    else:\n",
    "        subprocess.run(_ssh_cmd(env, cmd), check=True)\n",
    "\n",
    "\n",
    "# ─────────────────────────────── MAIN per VSCode ──────────────────────────────\n",
    "IN_COLAB, IN_VSCODE = _detect_env()\n",
    "if IN_VSCODE:\n",
    "    try:\n",
    "        env = _load_env_vars()\n",
    "\n",
    "        # dataset_id dal training.yaml\n",
    "        cfg = yaml.safe_load((PROJECT_ROOT / \"config\" / \"training.yaml\").read_text())\n",
    "        DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n",
    "\n",
    "        # assicura dir base remoto\n",
    "        subprocess.run(_ssh_cmd(env, f\"mkdir -p {env['BASE']}\"), check=True)\n",
    "\n",
    "        # build e sync\n",
    "        SBATCH_LOCAL = PROJECT_ROOT / \"hpc_submit.sh\"   # era Path.cwd()/…\n",
    "        build_sbatch_script(env, SBATCH_LOCAL)\n",
    "        sync_project_code(env)\n",
    "        upload_dataset_tar(env, DATASET_ID)\n",
    "        submit_job(env, SBATCH_LOCAL.name)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"❌ Submission fallita:\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"⚠️  Cell 2: skip (non siamo in VSCode).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8UIj4XOZP6U9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loaded utils.training_utils from /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/src/utils/training_utils.py\n",
      "[DEBUG] Imported:\n",
      "  • TRAINER_REGISTRY keys: []\n",
      "  • get_latest_checkpoint → <function get_latest_checkpoint at 0x14a0bb7f0>\n",
      "  • load_checkpoint       → <function load_checkpoint at 0x14a0bb760>\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 – Dynamic import of utils.training_utils\n",
    "import sys\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) locate & load the module file\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\" / \"training_utils.py\"\n",
    "spec       = importlib.util.spec_from_file_location(\"utils.training_utils\", str(utils_path))\n",
    "utils_mod  = importlib.util.module_from_spec(spec)     # type: ignore[arg-type]\n",
    "assert spec and spec.loader, f\"Cannot load spec for {utils_path}\"\n",
    "spec.loader.exec_module(utils_mod)                     # type: ignore[assignment]\n",
    "sys.modules[\"utils.training_utils\"] = utils_mod        # register in sys.modules\n",
    "print(f\"[DEBUG] Loaded utils.training_utils from {utils_path}\")\n",
    "\n",
    "# 2) import what we need\n",
    "from utils.training_utils import (\n",
    "    TRAINER_REGISTRY,\n",
    "    get_latest_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "\n",
    "print(\"[DEBUG] Imported:\")\n",
    "print(\"  • TRAINER_REGISTRY keys:\", list(TRAINER_REGISTRY.keys()))\n",
    "print(\"  • get_latest_checkpoint →\", get_latest_checkpoint)\n",
    "print(\"  • load_checkpoint       →\", load_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jwx0_Pr9E59j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] EXP_CODE → 20250626011748\n",
      "[DEBUG] TRAIN → /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/train/patches-0000.tar\n",
      "[DEBUG] VAL → /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/val/patches-0000.tar\n",
      "[DEBUG] TEST → /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test/patches-0000.tar\n",
      "[DEBUG] EXP_BASE → /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250626011748\n",
      "[DEBUG] Scritto   /Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250626011748/training_20250626011748.yaml\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 – Configuration & Directory Setup (formatted and absolute paths)\n",
    "import yaml, datetime, os\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 0) EXP_CODE: riprendi da YAML → env → genera nuovo                 #\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "yaml_exp = cfg.get(\"exp_code\", \"\")           # <─ nuovo parametro\n",
    "env_exp  = os.environ.get(\"EXP_CODE\", \"\")\n",
    "\n",
    "if yaml_exp:                                 # 1) priorità al file YAML\n",
    "    EXP_CODE = yaml_exp\n",
    "elif env_exp:                                # 2) poi variabile d’ambiente\n",
    "    EXP_CODE = env_exp\n",
    "else:                                        # 3) altrimenti nuovo timestamp\n",
    "    EXP_CODE = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# salva nel processo per eventuali figli\n",
    "os.environ[\"EXP_CODE\"] = EXP_CODE\n",
    "\n",
    "print(f\"[DEBUG] EXP_CODE → {EXP_CODE}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1) Carica configurazione generale                                  #\n",
    "# ------------------------------------------------------------------ #\n",
    "cfg_path  = PROJECT_ROOT / \"config\" / \"training.yaml\"\n",
    "cfg       = yaml.safe_load(cfg_path.read_text())\n",
    "DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n",
    "cfg[\"experiment_code\"] = EXP_CODE     # lo inseriamo nel dict per eventuali usi downstream\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2) Percorsi assoluti (train / val / test)                          #\n",
    "# ------------------------------------------------------------------ #\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    rel = cfg[\"data\"][split].format(dataset_id=DATASET_ID)\n",
    "    abs_ = (PROJECT_ROOT / rel).resolve()\n",
    "    cfg[\"data\"][split] = str(abs_)\n",
    "    print(f\"[DEBUG] {split.upper()} → {abs_}\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3) Struttura directory esperimento                                 #\n",
    "# ------------------------------------------------------------------ #\n",
    "EXP_ROOT = PROJECT_ROOT / \"data\" / \"processed\" / str(DATASET_ID)\n",
    "EXP_BASE = EXP_ROOT / \"experiments\" / EXP_CODE                   # unica per tutta la run\n",
    "EXP_BASE.mkdir(parents=True, exist_ok=True)\n",
    "(EXP_ROOT / \"experiments.md\").touch(exist_ok=True)               # indice globale\n",
    "\n",
    "print(f\"[DEBUG] EXP_BASE → {EXP_BASE}\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 4) Salva la **copia YAML** solo se non esiste già                  #\n",
    "# ------------------------------------------------------------------ #\n",
    "exp_yaml = EXP_BASE / f\"training_{EXP_CODE}.yaml\"\n",
    "if not exp_yaml.exists():                          # evita duplicati\n",
    "    exp_yaml.write_text(yaml.dump(cfg, sort_keys=False))\n",
    "    print(f\"[DEBUG] Scritto   {exp_yaml}\")\n",
    "else:\n",
    "    print(f\"[DEBUG] Config già presente → {exp_yaml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Registered trainers: ['simclr', 'moco_v2', 'rotation', 'jigsaw', 'supervised', 'transfer']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 – Import all trainer modules\n",
    "import importlib, sys\n",
    "from utils.training_utils import TRAINER_REGISTRY\n",
    "\n",
    "trainer_mods = [\n",
    "    \"trainers.simclr\",\n",
    "    \"trainers.moco_v2\",\n",
    "    \"trainers.rotation\",\n",
    "    \"trainers.jigsaw\",\n",
    "    \"trainers.supervised\",   # ✅ CORRETTO\n",
    "    \"trainers.transfer\",\n",
    "]\n",
    "\n",
    "for m in trainer_mods:\n",
    "    importlib.reload(sys.modules[m]) if m in sys.modules else importlib.import_module(m)\n",
    "\n",
    "print(f\"[DEBUG] Registered trainers: {list(TRAINER_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% -------------------------------------------------------------------- #\n",
    "# Cell 6 – Helper utilities (Tee, paths, selezione, …)                    #\n",
    "# ----------------------------------------------------------------------- #\n",
    "import contextlib, sys, time, inspect\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from utils.training_utils import get_latest_checkpoint, load_checkpoint\n",
    "from trainers.train_classifier import train_classifier\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "# I/O helpers                                                             #\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "class _Tee:\n",
    "    \"\"\"Duplica stdout / stderr su console *e* file.\"\"\"\n",
    "    def __init__(self, *targets): self.targets = targets\n",
    "    def write(self, data):  [t.write(data) and t.flush() for t in self.targets]\n",
    "    def flush(self):        [t.flush()      for t in self.targets]\n",
    "\n",
    "def _global_experiments_append(line: str):\n",
    "    \"\"\"Aggiunge una riga all’indice globale `experiments.md`.\"\"\"\n",
    "    exp_md = EXP_ROOT / \"experiments.md\"\n",
    "    with exp_md.open(\"a\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "# Path builders & artefact checks                                         #\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "def _paths(model_name: str) -> dict[str, Path]:\n",
    "    \"\"\"Restituisce tutti i path (dir/log/ckpt/features/clf) per un modello.\"\"\"\n",
    "    mdir = EXP_BASE / model_name\n",
    "    mdir.mkdir(parents=True, exist_ok=True)\n",
    "    return {\n",
    "        \"dir\"      : mdir,\n",
    "        \"log\"      : mdir / f\"log_{EXP_CODE}.md\",\n",
    "        \"features\" : mdir / f\"{model_name}_features.pt\",\n",
    "        \"clf\"      : mdir / f\"{model_name}_classifier.joblib\",\n",
    "        \"ckpt_pref\": f\"{model_name}_epoch\",        # usato da save_checkpoint\n",
    "    }\n",
    "\n",
    "def _completed(paths: dict, is_ssl: bool) -> bool:\n",
    "    \"\"\"True se TUTTI gli artefatti richiesti sono già presenti.\"\"\"\n",
    "    ckpt_ok = get_latest_checkpoint(paths[\"dir\"], prefix=paths[\"ckpt_pref\"]) is not None\n",
    "    if not ckpt_ok:\n",
    "        return False\n",
    "    if is_ssl:\n",
    "        return paths[\"features\"].exists() and paths[\"clf\"].exists()\n",
    "    return True\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "# Selezione modelli & trainer helpers                                     #\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "def _select_models(cfg: dict) -> dict[str, dict]:\n",
    "    sel = cfg.get(\"run_model\", \"all\").lower()\n",
    "    return {\n",
    "        n: c for n, c in cfg[\"models\"].items()\n",
    "        if sel in (\"all\", n) or\n",
    "           (sel == \"sl\"  and c.get(\"type\") == \"sl\") or\n",
    "           (sel == \"ssl\" and c.get(\"type\") == \"ssl\")\n",
    "    }\n",
    "\n",
    "def _init_trainer(name: str, m_cfg: dict, data_cfg: dict, ckpt_dir: Path):\n",
    "    Trainer = TRAINER_REGISTRY[name]\n",
    "    tr = Trainer(m_cfg, data_cfg)\n",
    "    tr.ckpt_dir = ckpt_dir         # salva i .pt qui\n",
    "    return tr\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "# Training / resume / artefacts                                           #\n",
    "# ──────────────────────────────────────────────────────────────────────── #\n",
    "def _run_full_training(trainer, epochs: int):\n",
    "    \"\"\"Loop di training (identico al codice esistente, solo incapsulato).\"\"\"\n",
    "    has_val = hasattr(trainer, \"validate_epoch\")\n",
    "    total_batches = getattr(trainer, \"batches_train\", None)\n",
    "    if total_batches is None:\n",
    "        try: total_batches = len(trainer.train_loader)\n",
    "        except TypeError: total_batches = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time(); run_loss = run_corr = seen = 0\n",
    "        print(f\"--- Epoch {epoch}/{epochs} ---\")\n",
    "        for i, batch in enumerate(trainer.train_loader, 1):\n",
    "            sig = inspect.signature(trainer.train_step)\n",
    "            res = trainer.train_step(batch) if len(sig.parameters) == 1 \\\n",
    "                  else trainer.train_step(*batch)\n",
    "            if len(res) == 4: _, loss, corr, bs = res\n",
    "            else:             loss, bs = res; corr = 0\n",
    "            run_loss += loss * bs; run_corr += corr; seen += bs\n",
    "\n",
    "            # progress bar\n",
    "            if total_batches:\n",
    "                pct = (i/total_batches)*100\n",
    "                eta = ((time.time()-t0)/i)*(total_batches-i)\n",
    "                msg = (f\"  Batch {i}/{total_batches} ({pct:.1f}%) | \"\n",
    "                       f\"Loss: {run_loss/seen:.4f}\")\n",
    "                if has_val: msg += f\" | Acc: {run_corr/seen:.3f}\"\n",
    "                msg += f\" | Elapsed: {time.time()-t0:.1f}s | ETA: {eta:.1f}s\"\n",
    "            else:\n",
    "                msg = f\"  Batch {i} | Loss: {run_loss/seen:.4f}\"\n",
    "                if has_val: msg += f\" | Acc: {run_corr/seen:.3f}\"\n",
    "                msg += f\" | Elapsed: {time.time()-t0:.1f}s\"\n",
    "            print(msg)\n",
    "\n",
    "        if has_val:\n",
    "            v_loss, v_acc = trainer.validate_epoch()\n",
    "            trainer.post_epoch(epoch, v_acc)\n",
    "            print(f\"Val -> Loss: {v_loss:.4f} | Acc: {v_acc:.3f}\")\n",
    "        else:\n",
    "            trainer.post_epoch(epoch, run_loss/seen)\n",
    "\n",
    "        print(f\"Epoch {epoch} completed in {time.time()-t0:.1f}s\\n\")\n",
    "\n",
    "def _resume_or_train(trainer, paths: dict, epochs: int):\n",
    "    ckpt = get_latest_checkpoint(paths[\"dir\"], prefix=paths[\"ckpt_pref\"])\n",
    "    if ckpt:\n",
    "        print(f\"⏩ Resume da {ckpt.name}\")\n",
    "        load_checkpoint(\n",
    "            ckpt,\n",
    "            model=torch.nn.Sequential(\n",
    "                trainer.encoder,\n",
    "                getattr(trainer, \"projector\", torch.nn.Identity())\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        _run_full_training(trainer, epochs)\n",
    "\n",
    "def _ensure_ssl_artifacts(trainer, paths: dict):\n",
    "    if not paths[\"features\"].exists():\n",
    "        trainer.extract_features_to(str(paths[\"features\"]))\n",
    "    if not paths[\"clf\"].exists():\n",
    "        train_classifier(str(paths[\"features\"]), str(paths[\"clf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rMV3BYIyE7FM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Training delegato a SLURM: skip esecuzione locale.\n"
     ]
    }
   ],
   "source": [
    "# %% -------------------------------------------------------------------- #\n",
    "# Cell 7 – Modular Launch & Auto-Recover                                  #\n",
    "# ----------------------------------------------------------------------- #\n",
    "def launch_training(cfg: dict) -> None:\n",
    "    \"\"\"Lancia (o recupera) tutti i modelli selezionati.\"\"\"\n",
    "    for name, m_cfg in _select_models(cfg).items():\n",
    "        paths   = _paths(name)\n",
    "        is_ssl  = m_cfg.get(\"type\") == \"ssl\"\n",
    "        epochs  = int(m_cfg[\"training\"][\"epochs\"])\n",
    "\n",
    "        # ---------- logging (append) ----------------------------------- #\n",
    "        with open(paths[\"log\"], \"a\") as lf, \\\n",
    "             contextlib.redirect_stdout(_Tee(sys.stdout, lf)), \\\n",
    "             contextlib.redirect_stderr(_Tee(sys.stderr, lf)):\n",
    "\n",
    "            if _completed(paths, is_ssl):\n",
    "                print(f\"✅ Artefatti completi per '{name}' – skip\\n\")\n",
    "                continue\n",
    "\n",
    "            trainer = _init_trainer(name, m_cfg, cfg[\"data\"], paths[\"dir\"])\n",
    "            print(f\"Device: {trainer.device} 🚀  Starting training for '{name}'\")\n",
    "            print(f\"→ Model config: {m_cfg}\\n\")\n",
    "\n",
    "            _resume_or_train(trainer, paths, epochs)\n",
    "\n",
    "            if is_ssl:\n",
    "                _ensure_ssl_artifacts(trainer, paths)\n",
    "\n",
    "            # Aggiorna indice globale\n",
    "            last_ckpt = get_latest_checkpoint(paths[\"dir\"], prefix=paths[\"ckpt_pref\"])\n",
    "            rel = last_ckpt.relative_to(EXP_ROOT) if last_ckpt else \"-\"\n",
    "            _global_experiments_append(f\"| {EXP_CODE} | {name} | {epochs} | {rel} |\")\n",
    "\n",
    "# 🚀 Avvio immediato\n",
    "# 🚀 Avvio immediato SOLO se siamo in Colab\n",
    "if IN_COLAB:\n",
    "    launch_training(cfg)\n",
    "else:\n",
    "    print(\"⏩ Training delegato a SLURM: skip esecuzione locale.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  Downloading artifacts of EXP_CODE=20250626011748 …\n",
      "⚠️  Download failed (job forse non ancora terminato):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rsync: change_dir \"/home/mla_group_01/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250626011748\" failed: No such file or directory (2)\n",
      "rsync(14138): warning: receiver has empty file list: exiting\n",
      "rsync(14138): warning: child 14139 exited with status 23\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 – Post-SLURM auto-download (solo VSCode + LEGION)\n",
    "if IN_VSCODE:\n",
    "    import subprocess, shutil, os\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "    # Carica .env per usare di nuovo USER/HOST/PASSWORD\n",
    "    load_dotenv(find_dotenv(), override=True)\n",
    "    CLUSTER_USER  = os.getenv(\"CLUSTER_USER\")\n",
    "    CLUSTER_HOST  = os.getenv(\"CLUSTER_HOST\")\n",
    "    REMOTE_BASE   = os.getenv(\"REMOTE_BASE_PATH\")\n",
    "    CLUSTER_PW    = os.getenv(\"CLUSTER_PASSWORD\", \"\")\n",
    "    USE_SSHPASS   = bool(CLUSTER_PW and shutil.which(\"sshpass\"))\n",
    "\n",
    "    # ----- funzione rsync con/​senza sshpass ----------------------------------#\n",
    "    def _rsync(src: str, dest: str):\n",
    "        args = [\"rsync\", \"-avz\"]\n",
    "        if USE_SSHPASS:\n",
    "            sshpart = f\"sshpass -p {CLUSTER_PW} ssh -o StrictHostKeyChecking=no\"\n",
    "            args += [\"-e\", sshpart]\n",
    "        args += [src, dest]\n",
    "        subprocess.run(args, check=True)\n",
    "\n",
    "    dataset_id = cfg[\"data\"][\"dataset_id\"]\n",
    "    remote_exp = f\"{REMOTE_BASE}/data/processed/{dataset_id}/experiments/{EXP_CODE}/\"\n",
    "    local_exp  = EXP_ROOT / \"experiments\" / EXP_CODE\n",
    "    local_exp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"⬇️  Downloading artifacts of EXP_CODE={EXP_CODE} …\")\n",
    "    try:\n",
    "        _rsync(f\"{CLUSTER_USER}@{CLUSTER_HOST}:{remote_exp}\", str(local_exp))\n",
    "        print(\"✅  Artifacts downloaded in:\", local_exp)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"⚠️  Download failed (job forse non ancora terminato):\")\n",
    "        print(e.stderr or \"\")\n",
    "else:\n",
    "    print(\"↩️  Download skipped: non siamo in VSCode/LEGION.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Terminale VSCode (Remote SSH o locale)**\n",
    "   Dopo aver sottoposto il job, prendi il `JOBID` dallo stdout di `sbatch`.\n",
    "   Poi apri un terminale in VSCode e digiti:\n",
    "\n",
    "   ```bash\n",
    "   ssh mla_group_01@legionlogin.polito.it\n",
    "   cd /home/mla_group_01/wsi-ssrl-rcc_project\n",
    "   tail -f rcc_ssrl_launch_<JOBID>.out\n",
    "   ```\n",
    "\n",
    "   In questo modo vedrai **in tempo reale** tutti i `print` man mano che il training avanza.\n",
    "\n",
    "2. **VSCode “Remote SSH” Extension**\n",
    "   Se installi l’estensione Remote SSH di VSCode, puoi:\n",
    "\n",
    "   * Connetterti direttamente al nodo di login (`legionlogin.polito.it`)\n",
    "   * Aprire il file `rcc_ssrl_launch_<JOBID>.out` nell’editor\n",
    "   * Abilitare “Auto Save” e “Follow Tail” (clic destro → *Tail Follow*), per visualizzare i nuovi messaggi senza uscire dall’IDE."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMUn+/uyACrBWAZGivnsuX",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wsi-ssrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
