{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cOZCcsp0RCa",
        "outputId": "5111af5f-a28c-4b1b-f34e-055b582035fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "!pip install --quiet torch torchvision webdataset tqdm pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CPkMc3ONzi0J",
        "outputId": "1af41cff-a5f7-448e-faec-de2ba7041c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔥 PROJECT_ROOT: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import yaml\n",
        "import sys\n",
        "import time\n",
        "import importlib\n",
        "import logging\n",
        "import torch\n",
        "from typing import Any, Dict\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "\n",
        "config_path = Path('/content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/config/training.yaml')\n",
        "\n",
        "with config_path.open('r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "colab_root = Path(cfg['env_paths']['colab'])\n",
        "local_root = Path(cfg['env_paths']['local'])\n",
        "PROJECT_ROOT = colab_root if colab_root.exists() else local_root\n",
        "if not PROJECT_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Project root not find: {PROJECT_ROOT}\")\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
        "\n",
        "\n",
        "from importlib.util import spec_from_file_location, module_from_spec\n",
        "\n",
        "utils_dir = PROJECT_ROOT / 'src' / 'utils'\n",
        "src_file = utils_dir / 'training_utils.py'\n",
        "\n",
        "spec = spec_from_file_location('utils.training_utils', str(src_file))\n",
        "training_utils = module_from_spec(spec)\n",
        "spec.loader.exec_module(training_utils)\n",
        "\n",
        "sys.modules['utils.training_utils'] = training_utils\n",
        "\n",
        "from utils.training_utils import TRAINER_REGISTRY\n",
        "from trainers.extract_features import extract_features\n",
        "from trainers.train_classifier import train_classifier\n",
        "from utils.training_utils import get_latest_checkpoint, load_checkpoint\n",
        "print(f\"🔥 PROJECT_ROOT: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lIancWXKKuOY",
        "outputId": "58b4d5a4-3581-4a56-dfc2-e5d959a463d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Dataset paths:\n",
            "  • train: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed/webdataset_2500/train/patches-0000.tar\n",
            "  • val: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed/webdataset_2500/val/patches-0000.tar\n",
            "  • test: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed/webdataset_2500/test/patches-0000.tar\n"
          ]
        }
      ],
      "source": [
        "for split in ['train','val','test']:\n",
        "    rel = cfg['data'].get(split)\n",
        "    if rel:\n",
        "        cfg['data'][split] = str(PROJECT_ROOT / rel)\n",
        "\n",
        "print(\"📂 Dataset paths:\")\n",
        "for split in ['train','val','test']:\n",
        "    print(f\"  • {split}: {cfg['data'][split]}\")\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / 'src'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8UIj4XOZP6U9"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)-8s | %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "logger = logging.getLogger(\"LAUNCHER\")\n",
        "logger.info(\"✅ Logger initialized at INFO level\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jwx0_Pr9E59j"
      },
      "outputs": [],
      "source": [
        "trainer_modules = [\n",
        "    \"trainers.simclr\",\n",
        "    \"trainers.moco_v2\",\n",
        "    \"trainers.rotation\",\n",
        "    \"trainers.jigsaw\",\n",
        "    \"trainers.supervised\",\n",
        "    \"trainers.transfer\",\n",
        "]\n",
        "for module_name in trainer_modules:\n",
        "    if module_name in sys.modules:\n",
        "        importlib.reload(sys.modules[module_name])\n",
        "    else:\n",
        "        importlib.import_module(module_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rMV3BYIyE7FM"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import time\n",
        "\n",
        "def launch_training(cfg: dict) -> None:\n",
        "    models_cfg = cfg.get('models', {})\n",
        "    run_model = cfg.get('run_model', 'all').lower()\n",
        "    if run_model == 'all':\n",
        "        tasks = list(models_cfg.items())\n",
        "    else:\n",
        "        if run_model not in models_cfg:\n",
        "            raise KeyError(f\"Model '{run_model}' not found in cfg['models']\")\n",
        "        tasks = [(run_model, models_cfg[run_model])]\n",
        "\n",
        "    for name, m_cfg in tasks:\n",
        "        if name not in TRAINER_REGISTRY:\n",
        "            raise KeyError(f\"No trainer registered for '{name}'\")\n",
        "\n",
        "        trainer = TRAINER_REGISTRY[name](m_cfg, cfg['data'])\n",
        "        device = getattr(trainer, 'device', 'n/a')\n",
        "        epochs = int(m_cfg['training'].get('epochs', 0))\n",
        "        batch_size = int(m_cfg['training'].get('batch_size', 0))\n",
        "\n",
        "        print(f\"Device: {device} 🚀  Starting training for model '{name}'\")\n",
        "        print(f\"→ Model config: {m_cfg}\")\n",
        "        print(f\"Epochs: {epochs} | Batch size: {batch_size}\\n\")\n",
        "\n",
        "        has_validation = hasattr(trainer, 'validate_epoch')\n",
        "        experiment_id =  \"prova\"\n",
        "        experiment_dir = PROJECT_ROOT / f\"data/processed2/dataset_9f30917e/experiments/{experiment_id}/{name}\"\n",
        "        ckpt_path_list = sorted(experiment_dir.glob(f\"{trainer.__class__.__name__}_best_epoch*.pt\"))\n",
        "        ckpt_path = ckpt_path_list[-1] if ckpt_path_list else None\n",
        "\n",
        "\n",
        "        skip_training = False\n",
        "        if ckpt_path and ckpt_path.exists():\n",
        "            print(f\"⏭️  Checkpoint found for '{name}' → skipping training and loading encoder/projector/model.\")\n",
        "            if hasattr(trainer, \"encoder\") and hasattr(trainer, \"projector\"):\n",
        "                model = torch.nn.Sequential(trainer.encoder, trainer.projector)\n",
        "                load_checkpoint(ckpt_path, model=model)\n",
        "                trainer.encoder = model[0].to(trainer.device)\n",
        "                trainer.projector = model[1].to(trainer.device)\n",
        "            elif hasattr(trainer, \"model\"):\n",
        "                load_checkpoint(ckpt_path, model=trainer.model)\n",
        "                trainer.model = trainer.model.to(trainer.device)\n",
        "            elif hasattr(trainer, \"encoder\") and hasattr(trainer, \"head\"):\n",
        "                model = torch.nn.Sequential(trainer.encoder, trainer.head)\n",
        "                load_checkpoint(ckpt_path, model=model)\n",
        "                trainer.encoder = model[0].to(trainer.device)\n",
        "                trainer.head = model[1].to(trainer.device)\n",
        "            else:\n",
        "                raise AttributeError(f\"❌ Trainer '{name}' has no encoder/projector or model to load into.\")\n",
        "            skip_training = True\n",
        "\n",
        "\n",
        "        if not skip_training:\n",
        "            for epoch in range(1, epochs + 1):\n",
        "                epoch_start = time.time()\n",
        "                total_batches = getattr(trainer, 'batches_train', None)\n",
        "                print(f\"TOTAL BATCHES {total_batches}\")\n",
        "                running_loss, running_correct, total_samples = 0.0, 0, 0\n",
        "\n",
        "                print(f\"--- Epoch {epoch}/{epochs} ---\")\n",
        "                for i, batch in enumerate(trainer.train_loader, start=1):\n",
        "                    sig = inspect.signature(trainer.train_step)\n",
        "                    result = trainer.train_step(batch) if len(sig.parameters) == 1 else trainer.train_step(*batch)\n",
        "                    if len(result) == 4:\n",
        "                        _, loss, correct, bs = result\n",
        "                    else:\n",
        "                        loss, bs = result\n",
        "                        correct = 0\n",
        "\n",
        "                    running_loss += loss * bs\n",
        "                    running_correct += correct\n",
        "                    total_samples += bs\n",
        "                    avg_loss = running_loss / total_samples\n",
        "                    avg_acc = (running_correct / total_samples) if has_validation else 0.0\n",
        "                    elapsed = time.time() - epoch_start\n",
        "                    pct = (i / total_batches) * 100 if total_batches else 0.0\n",
        "                    eta = (elapsed / i) * (total_batches - i) if total_batches else 0.0\n",
        "\n",
        "                    msg = f\"  Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {avg_loss:.4f}\"\n",
        "                    if has_validation:\n",
        "                        msg += f\" | Acc: {avg_acc:.3f}\"\n",
        "                    msg += f\" | Elapsed: {elapsed:.1f}s | ETA: {eta:.1f}s\"\n",
        "                    print(msg)\n",
        "\n",
        "                if has_validation:\n",
        "                    val_loss, val_acc = trainer.validate_epoch()\n",
        "                    print(f\"Val -> Loss: {val_loss:.4f} | Acc: {val_acc:.3f}\")\n",
        "                    trainer.post_epoch(epoch, val_acc)\n",
        "                else:\n",
        "                    epoch_loss = running_loss / total_samples\n",
        "                    trainer.post_epoch(epoch, epoch_loss)\n",
        "\n",
        "                print(f\"Epoch {epoch} completed in {time.time() - epoch_start:.1f}s\\n\")\n",
        "\n",
        "        best = trainer.summary()\n",
        "        if isinstance(best, tuple) and len(best) == 2:\n",
        "            be, bm = best\n",
        "            print(f\"✅ Training for '{name}' completed. Best @ epoch {be} -> {bm:.3f}\")\n",
        "\n",
        "        # Only for self-supervised\n",
        "        if not has_validation:\n",
        "            print(f\"🔍 Extracting features from model '{name}'\")\n",
        "            feature_path = experiment_dir / f\"{name}_features.pt\"\n",
        "            classifier_path = experiment_dir / f\"{name}_classifier.joblib\"\n",
        "            feature_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            if hasattr(trainer, \"extract_features_to\"):\n",
        "                trainer.extract_features_to(str(feature_path))\n",
        "                print(f\"✅ Features saved to {feature_path}\")\n",
        "            else:\n",
        "                print(f\"⚠️ Trainer '{name}' does not implement extract_features_to(), skipping.\")\n",
        "\n",
        "            print(f\"🧠 Training classifier on features '{name}'\")\n",
        "            train_classifier(\n",
        "                features_path=str(feature_path),\n",
        "                output_model=str(classifier_path)\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xFDhQTgUOwYJ",
        "outputId": "71f6277d-3041-43d5-c8eb-203b7ab2cd76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu 🚀  Starting training for model 'jigsaw'\n",
            "→ Model config: {'backbone': 'resnet18', 'grid_size': 3, 'training': {'epochs': 50, 'batch_size': 64, 'optimizer': 'adam', 'learning_rate': '1e-4', 'weight_decay': '1e-5'}}\n",
            "Epochs: 50 | Batch size: 64\n",
            "\n",
            "⏭️  Checkpoint found for 'jigsaw' → skipping training and loading encoder/projector/model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Training for 'jigsaw' completed. Best @ epoch 0 -> inf\n",
            "🔍 Extracting features from model 'jigsaw'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 24it [02:33,  6.41s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Jigsaw features saved to /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed2/dataset_9f30917e/experiments/prova/jigsaw/jigsaw_features.pt\n",
            "✅ Features saved to /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed2/dataset_9f30917e/experiments/prova/jigsaw/jigsaw_features.pt\n",
            "🧠 Training classifier on features 'jigsaw'\n",
            "✅ Loaded 1475 keys and (1475, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('not_tumor'): 299, np.str_('ONCO'): 297, np.str_('CHROMO'): 293, np.str_('ccRCC'): 293, np.str_('pRCC'): 293})\n",
            "✅ Filtered dataset: 1475 samples\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.58      0.12      0.20        58\n",
            "        ONCO       0.26      0.58      0.36        59\n",
            "       ccRCC       0.28      0.12      0.17        59\n",
            "   not_tumor       0.28      0.13      0.18        60\n",
            "        pRCC       0.33      0.56      0.42        59\n",
            "\n",
            "    accuracy                           0.30       295\n",
            "   macro avg       0.35      0.30      0.26       295\n",
            "weighted avg       0.35      0.30      0.26       295\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 7 24  4  3 20]\n",
            " [ 3 34  3  6 13]\n",
            " [ 1 26  7  6 19]\n",
            " [ 0 34  4  8 14]\n",
            " [ 1 12  7  6 33]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed2/dataset_9f30917e/experiments/prova/jigsaw/jigsaw_classifier.joblib\n"
          ]
        }
      ],
      "source": [
        "launch_training(cfg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}