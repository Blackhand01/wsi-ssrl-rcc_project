{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GFLbAUPJoOq",
        "outputId": "a9ad895b-5eca-4efd-bb91-b0587c103f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi1bbemBZE99"
      },
      "source": [
        "## 1. 🧠 Self-Supervised Models (SimCLR, MoCo, Rotation, JEPA)\n",
        "\n",
        "### 1. Pre-training\n",
        "\n",
        "* Allena l’**encoder una sola volta**, su tutti i pazienti di `train_fold0`.\n",
        "* Salva i checkpoint (encoder + projector).\n",
        "\n",
        "### 2. Estrazione delle feature\n",
        "\n",
        "* Carica il **checkpoint fisso** del pretraining.\n",
        "* Estrai le feature su tutti i fold (`train_fold{i}`, `val_fold{i}`, `test_holdout`).\n",
        "* Salva i file come `{model}_features_fold{i}.pt`.\n",
        "\n",
        "### 3. Linear-Probe per ogni fold `i ∈ [0, ..., N-1]`\n",
        "\n",
        "* **Train**: allena la testa lineare (LogReg, MLP) sulle feature di `train_fold{i}`.\n",
        "* **Validation**: usa `val_fold{i}` per:\n",
        "\n",
        "  * early stopping,\n",
        "  * salvare il miglior probe,\n",
        "  * applicare **Temperature Scaling**.\n",
        "* **Test**: valuta il probe calibrato su `test_holdout`, usando anche **MC-Dropout**.\n",
        "\n",
        "### 4. Aggregazione finale\n",
        "\n",
        "* Raccogli i risultati su `test_holdout` per ogni fold.\n",
        "* Calcola **media ± deviazione standard** per ogni metrica (Accuracy, F1, AUC, ECE, incertezza).\n",
        "* Questo è il risultato finale del tuo modello SSL + probe.\n",
        "\n",
        "> 🔒 **Non riaddestrare l’encoder nei fold 1–N.**\n",
        "> Lo scopo è testarne la **capacità di generalizzazione task-agnostica**, non adattarlo a ogni fold.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 🧪 Supervised & Transfer Learning\n",
        "\n",
        "Qui **non hai un encoder fisso**. Ogni fold ha il proprio training da zero.\n",
        "\n",
        "### Per ciascun fold `i ∈ [0, ..., N-1]`\n",
        "\n",
        "* **Train**: allena l’intero modello (es. ResNet-50) su `train_fold{i}`.\n",
        "* **Validation**: usa `val_fold{i}` per early-stopping e calibrazione.\n",
        "* **Test**: valuta sempre su `test_holdout` (o sul val-fold se non esiste un holdout).\n",
        "* Salva un checkpoint per ogni fold (`supervised_fold{i}.pt`, `transfer_fold{i}.pt`, ecc.).\n",
        "\n",
        "### Aggregazione finale\n",
        "\n",
        "* Come per gli SSL, calcola la **media ± deviazione** delle metriche su `test_holdout` per i vari fold.\n",
        "* Non scegli il modello col miglior punteggio, ma riporti la **media aggregata**.\n",
        "\n",
        "> ℹ️ **Facoltativo**: puoi evidenziare il fold più vicino alla media, **solo a scopo illustrativo**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 🔁 Confronto tra pipeline SSL e SL\n",
        "\n",
        "| Step                   | SSL models (SimCLR, MoCo, ...)  | SL/Transfer models              |\n",
        "| ---------------------- | ------------------------------- | ------------------------------- |\n",
        "| **Encoder training**   | Solo su fold0                   | Uno per ogni fold               |\n",
        "| **Feature extraction** | Uno per ogni fold               | –                               |\n",
        "| **Probe/classifier**   | Uno per ogni fold               | Uno per ogni fold               |\n",
        "| **Inference**          | Su `test_holdout` per ogni fold | Su `test_holdout` per ogni fold |\n",
        "| **Checkpoint**         | Encoder 1×                      | 1× per fold                     |\n",
        "| **Output finale**      | Media ± std su tutti i fold     | Media ± std su tutti i fold     |\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Perché questa architettura?\n",
        "\n",
        "* Nei **modelli SSL**, vogliamo dimostrare che un encoder **generalizza** a nuovi pazienti come **feature extractor**, senza mai essere fine-tuned.\n",
        "* Nei **modelli supervisionati**, alleniamo da capo su ogni fold per valutare una baseline comparabile (ma meno task-agnostica).\n",
        "\n",
        "In entrambi i casi:\n",
        "\n",
        "✅ **Non scegli il miglior fold**,\n",
        "✅ **Riporti solo le medie cross-fold**,\n",
        "✅ **Dimostri affidabilità, generalizzazione e riproducibilità**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Output finale\n",
        "\n",
        "Le metriche aggregate sono presentate in una tabella riassuntiva:\n",
        "\n",
        "| Model             | Accuracy (μ±σ) | Macro-F1 (μ±σ) | ECE (μ±σ)   | Uncertainty (μ±σ) |\n",
        "| ----------------- | -------------- | -------------- | ----------- | ----------------- |\n",
        "| SimCLR + probe    | 0.83 ± 0.04    | 0.79 ± 0.06    | 0.03 ± 0.01 | 0.12 ± 0.03       |\n",
        "| MoCo-v2 + probe   | 0.78 ± 0.05    | …              | …           | …                 |\n",
        "| Rotation + probe  | 0.74 ± 0.08    | …              | …           | …                 |\n",
        "| Supervised        | 0.80 ± 0.03    | …              | …           | …                 |\n",
        "| Transfer learning | 0.82 ± 0.04    | …              | …           | …                 |\n",
        "\n",
        "> Ogni riga rappresenta una valutazione **completa e aggregata** del modello, utile per confronti clinici e accademici.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPkMc3ONzi0J",
        "outputId": "62ff39ac-351b-4f24-d04d-e7ba2e871b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 [DEBUG] Avvio configurazione ambiente…\n",
            "📍 [DEBUG] Google Colab rilevato.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "📁 [DEBUG] PROJECT_ROOT → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\n",
            "✅ [DEBUG] PyTorch già presente (2.6.0+cu124)\n",
            "🔧 [DEBUG] Installazione pacchetti ausiliari mancanti: ['pillow', 'pyyaml']\n",
            "🖥️ [DEBUG] Torch device disponibile → cpu\n",
            "📦 [DEBUG] DATA_TARBALL → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed\n"
          ]
        }
      ],
      "source": [
        "# ## Cell 1 – Environment Setup & Dependencies\n",
        "# Compatibile con Google Colab (GPU/CPU) e ambiente locale (VS Code).\n",
        "\n",
        "# %%\n",
        "import os, sys, subprocess, importlib.util\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"📦 [DEBUG] Avvio configurazione ambiente…\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 1) Rileva ambiente (Colab vs locale)\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "IN_COLAB = Path(\"/content\").exists()\n",
        "if IN_COLAB:\n",
        "    print(\"📍 [DEBUG] Google Colab rilevato.\")\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "else:\n",
        "    print(\"💻 [DEBUG] Ambiente locale (VS Code / CLI) rilevato.\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 2) Definisci PROJECT_ROOT (ENV > default mapping)\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "DEFAULT_ENV_PATHS = {\n",
        "    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n",
        "    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n",
        "}\n",
        "PROJECT_ROOT = Path(os.getenv(\n",
        "    \"PROJECT_ROOT\",\n",
        "    DEFAULT_ENV_PATHS[\"colab\" if IN_COLAB else \"local\"])\n",
        ").resolve()\n",
        "\n",
        "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
        "print(f\"📁 [DEBUG] PROJECT_ROOT → {PROJECT_ROOT}\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 3) Utility per installare pacchetti mancanti\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "def _missing(pkgs):\n",
        "    return [p for p in pkgs if importlib.util.find_spec(p) is None]\n",
        "\n",
        "def _install(pkgs, idx_url=None):\n",
        "    if not pkgs:\n",
        "        return\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"]\n",
        "    if idx_url:\n",
        "        cmd += [\"--index-url\", idx_url]\n",
        "    subprocess.check_call(cmd + pkgs)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 4) Verifica / installa PyTorch (se non presente)\n",
        "#    • In Colab non sovrascrive la versione pre-installata\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "TORCH_PKGS = [\"torch\", \"torchvision\", \"torchaudio\"]\n",
        "\n",
        "if _missing([\"torch\"]):\n",
        "    print(\"🔧 [DEBUG] PyTorch non trovato → installazione in corso…\")\n",
        "    if IN_COLAB:\n",
        "        GPU = Path(\"/usr/local/cuda\").exists()\n",
        "        INDEX = \"https://download.pytorch.org/whl/cu121\" if GPU else \"https://download.pytorch.org/whl/cpu\"\n",
        "        _install(TORCH_PKGS, INDEX)\n",
        "    else:  # locale: lascia scegliere all'utente il build corretto\n",
        "        _install(TORCH_PKGS)\n",
        "else:\n",
        "    import torch\n",
        "    print(f\"✅ [DEBUG] PyTorch già presente ({torch.__version__})\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 5) Installazione pacchetti ausiliari (sempre sicura)\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "AUX_PKGS = [\"webdataset\", \"tqdm\", \"pillow\", \"pyyaml\", \"joblib\"]\n",
        "missing_aux = _missing(AUX_PKGS)\n",
        "if missing_aux:\n",
        "    print(f\"🔧 [DEBUG] Installazione pacchetti ausiliari mancanti: {missing_aux}\")\n",
        "    _install(missing_aux)\n",
        "else:\n",
        "    print(\"✅ [DEBUG] Pacchetti ausiliari già presenti.\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 6) Info dispositivo\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"🖥️ [DEBUG] Torch device disponibile → {DEVICE}\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "# 7) Costante path per Data Tarball (utilizzata negli step successivi)\n",
        "# ────────────────────────────────────────────────────────────────────────\n",
        "DATA_TARBALL = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "print(f\"📦 [DEBUG] DATA_TARBALL → {DATA_TARBALL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwx0_Pr9E59j",
        "outputId": "9d04e09e-a2a2-49ed-ce4a-b4e32ca776c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 [DEBUG] Config caricata da: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/config/training.yaml\n",
            "🔑 [DEBUG] EXP_CODE → 20250712175557 (fonte: TIMESTAMP)\n",
            "🧬 [DEBUG] DATASET_ID         = dataset_9f30917e\n",
            "🔁 [DEBUG] Folds              = [0, 1, 2]\n",
            "🔒 [DEBUG] train_encoder_once = True\n",
            "📂 [DEBUG] EXP_BASE → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557\n",
            "💾 [DEBUG] Salvato snapshot → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/training_20250712175557.yaml\n",
            "🎲 [DEBUG] Seed globale impostato a: 42\n"
          ]
        }
      ],
      "source": [
        "# %% -------------------------------------------------------------------- #\n",
        "# Cell 2 – Configurazione & Setup Esperimento                            #\n",
        "# ----------------------------------------------------------------------- #\n",
        "import yaml\n",
        "import datetime\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# ─── 1) Carica il file di configurazione ───────────────────────────────\n",
        "cfg_path = PROJECT_ROOT / \"config\" / \"training.yaml\"\n",
        "assert cfg_path.exists(), f\"❌ File mancante: {cfg_path}\"\n",
        "cfg = yaml.safe_load(cfg_path.read_text())\n",
        "print(f\"📄 [DEBUG] Config caricata da: {cfg_path}\")\n",
        "\n",
        "# ─── 2) Genera EXP_CODE ────────────────────────────────────────────────\n",
        "yaml_exp = cfg.get(\"exp_code\", \"\")\n",
        "env_exp  = os.getenv(\"EXP_CODE\", \"\")\n",
        "if yaml_exp:\n",
        "    EXP_CODE, src = yaml_exp, \"YAML\"\n",
        "elif env_exp:\n",
        "    EXP_CODE, src = env_exp, \"ENV\"\n",
        "else:\n",
        "    EXP_CODE, src = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"), \"TIMESTAMP\"\n",
        "os.environ[\"EXP_CODE\"] = EXP_CODE\n",
        "cfg[\"exp_code\"] = EXP_CODE\n",
        "print(f\"🔑 [DEBUG] EXP_CODE → {EXP_CODE} (fonte: {src})\")\n",
        "\n",
        "# ─── 3) Parametri dataset & folds ──────────────────────────────────────\n",
        "DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n",
        "FOLDS = cfg.get(\"folds\", [0])\n",
        "TRAIN_ENCODER_ONCE = cfg.get(\"train_encoder_once\", False)\n",
        "\n",
        "print(f\"🧬 [DEBUG] DATASET_ID         = {DATASET_ID}\")\n",
        "print(f\"🔁 [DEBUG] Folds              = {FOLDS}\")\n",
        "print(f\"🔒 [DEBUG] train_encoder_once = {TRAIN_ENCODER_ONCE}\")\n",
        "\n",
        "# ─── 4) Crea la directory dell’esperimento ─────────────────────────────\n",
        "exp_dir_rel = cfg[\"output\"][\"exp_dir\"].format(\n",
        "    dataset_id=DATASET_ID,\n",
        "    exp_code=EXP_CODE\n",
        ")\n",
        "# Use PROJECT_ROOT without early resolve to ensure Drive path\n",
        "EXP_BASE = PROJECT_ROOT / exp_dir_rel\n",
        "# Safety check: ensure EXP_BASE is under PROJECT_ROOT\n",
        "if not str(EXP_BASE.resolve()).startswith(str(PROJECT_ROOT.resolve())):\n",
        "    raise RuntimeError(f\"🚨 EXP_BASE NON sotto PROJECT_ROOT! → {EXP_BASE}\")\n",
        "# Create experiment directory\n",
        "EXP_BASE.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"📂 [DEBUG] EXP_BASE → {EXP_BASE.resolve()}\")\n",
        "\n",
        "# ─── 5) Salva snapshot del file YAML ───────────────────────────────────\n",
        "snap = EXP_BASE / f\"training_{EXP_CODE}.yaml\"\n",
        "if not snap.exists():\n",
        "    snap.write_text(yaml.dump(cfg, sort_keys=False))\n",
        "    print(f\"💾 [DEBUG] Salvato snapshot → {snap.resolve()}\")\n",
        "else:\n",
        "    print(f\"ℹ️  [DEBUG] Snapshot già presente → {snap.resolve()}\")\n",
        "\n",
        "# ─── 6) Imposta seed per la riproducibilità ────────────────────────────\n",
        "SEED = cfg.get(\"seed\", 42)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(f\"🎲 [DEBUG] Seed globale impostato a: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aVDklAZ1Yurj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e15ef0-1b08-4871-8090-14b868ba60a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 [DEBUG] Modelli configurati in YAML: ['simclr', 'jepa', 'moco_v2', 'rotation', 'supervised', 'transfer']\n",
            "✅ [DEBUG] Importato trainers.simclr\n",
            "✅ [DEBUG] Importato trainers.jepa\n",
            "✅ [DEBUG] Importato trainers.moco_v2\n",
            "✅ [DEBUG] Importato trainers.rotation\n",
            "✅ [DEBUG] Importato trainers.supervised\n",
            "✅ [DEBUG] Importato trainers.transfer\n",
            "📚 [DEBUG] Trainer registry contiene e corrispondenti tipi:\n",
            "  • simclr       → SSL\n",
            "  • jepa         → SSL\n",
            "  • moco_v2      → SSL\n",
            "  • rotation     → SSL\n",
            "  • supervised   → SL\n",
            "  • transfer     → SL\n",
            "🔎 [DEBUG] Sotto-moduli caricati da utils.training_utils:\n",
            "  • registry     → functions: ['register_trainer']\n",
            "  • device_io    → functions: ['choose_device', 'get_latest_checkpoint', 'load_checkpoint', 'load_json', 'save_checkpoint', 'save_joblib', 'save_json']\n",
            "  • data_utils   → functions: ['build_loader', 'count_samples', 'default_transforms', 'discover_classes', 'extract_labels_from_keys', 'load_classifier', 'load_features', 'parse_label_from_filename', 'save_classifier', 'save_features']\n",
            "  • model_utils  → functions: ['create_backbone', 'mc_dropout_predictions']\n",
            "  • metrics      → functions: ['accuracy_score', 'aggregate_fold_metrics', 'apply_temperature_scaling', 'classification_report', 'compute_classification_metrics', 'confusion_matrix', 'expected_calibration_error', 'f1_score', 'mc_dropout_statistics', 'minimize', 'roc_auc_score', 'softmax']\n"
          ]
        }
      ],
      "source": [
        "# ## Cell 3 – Import & Registrazione dei Trainer\n",
        "# Import dinamico dei modelli definiti in YAML e verifica del registry + tipo (SSL/SL).\n",
        "import sys\n",
        "import importlib\n",
        "from utils.training_utils.registry import TRAINER_REGISTRY\n",
        "\n",
        "# 1) Leggi i nomi dei modelli e i loro tipi da YAML\n",
        "model_cfgs = cfg[\"models\"]\n",
        "model_names = list(model_cfgs.keys())\n",
        "print(f\"🔄 [DEBUG] Modelli configurati in YAML: {model_names}\")\n",
        "\n",
        "# 2) Import dinamico di ciascun modulo trainers.{model}\n",
        "for name in model_names:\n",
        "    module_name = f\"trainers.{name}\"\n",
        "    try:\n",
        "        if module_name in sys.modules:\n",
        "            importlib.reload(sys.modules[module_name])\n",
        "            print(f\"✅ [DEBUG] Ricaricato {module_name}\")\n",
        "        else:\n",
        "            importlib.import_module(module_name)\n",
        "            print(f\"✅ [DEBUG] Importato {module_name}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"❌ [DEBUG] Errore importazione {module_name}: {e}\")\n",
        "\n",
        "# 3) Verifica che tutti i modelli siano registrati e mostra il loro tipo\n",
        "missing = [n for n in model_names if n not in TRAINER_REGISTRY]\n",
        "if missing:\n",
        "    print(f\"❌ [DEBUG] Trainer mancanti nel registry: {missing}\")\n",
        "else:\n",
        "    print(\"📚 [DEBUG] Trainer registry contiene e corrispondenti tipi:\")\n",
        "    for name in model_names:\n",
        "        # Usa solo il type dal file YAML per ogni modello\n",
        "        ttype = model_cfgs[name].get(\"type\", \"unknown\").upper()\n",
        "        print(f\"  • {name:<12} → {ttype}\")\n",
        "\n",
        "# 3) Verifica funzioni in sottomoduli\n",
        "import importlib, pkgutil, inspect\n",
        "\n",
        "print(\"🔎 [DEBUG] Sotto-moduli caricati da utils.training_utils:\")\n",
        "for mod in (\"registry\", \"device_io\", \"data_utils\", \"model_utils\", \"metrics\"):\n",
        "    m = importlib.import_module(f\"utils.training_utils.{mod}\")\n",
        "    print(f\"  • {mod:<12} → functions:\", [n for n, o in inspect.getmembers(m) if inspect.isfunction(o)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zie12NaWYurk"
      },
      "outputs": [],
      "source": [
        "# %% -------------------------------------------------------------------- #\n",
        "# Cell 4 – Helper utilities (Tee, paths, selezione, …)                    #\n",
        "# ----------------------------------------------------------------------- #\n",
        "import contextlib\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import inspect\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "from utils.training_utils.registry import TRAINER_REGISTRY\n",
        "from utils.training_utils.device_io import (\n",
        "    get_latest_checkpoint,\n",
        "    load_checkpoint,\n",
        "    save_json,\n",
        "    save_joblib,\n",
        ")\n",
        "from utils.training_utils.data_utils import count_samples\n",
        "from trainers.train_classifier import train_classifier\n",
        "from utils.training_utils.metrics import apply_temperature_scaling\n",
        "\n",
        "\n",
        "class _Tee:\n",
        "    \"\"\"\n",
        "    Classe per duplicare lo stdout/stderr su più target (es. console + file).\n",
        "    \"\"\"\n",
        "    def __init__(self, *tgts):\n",
        "        self.tgts = tgts\n",
        "\n",
        "    def write(self, data: str):\n",
        "        for t in self.tgts:\n",
        "            t.write(data)\n",
        "            t.flush()\n",
        "\n",
        "    def flush(self):\n",
        "        for t in self.tgts:\n",
        "            t.flush()\n",
        "\n",
        "\n",
        "def _global_experiments_append(line: str):\n",
        "    \"\"\"\n",
        "    Appende una riga al file esperimenti globale (esperiments.md).\n",
        "    \"\"\"\n",
        "    global_file = EXP_BASE.parent.parent / \"experiments.md\"\n",
        "    with open(global_file, \"a\") as f:\n",
        "        f.write(line.rstrip() + \"\\n\")\n",
        "\n",
        "\n",
        "# %% ----------------------------------------------------------------------- #\n",
        "# Cell 4 – Helper utilities (Tee, paths, selezione, …)                    #\n",
        "# ------------------------------------------------------------------------ #\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "from utils.training_utils.registry import TRAINER_REGISTRY\n",
        "from utils.training_utils.device_io import (\n",
        "    get_latest_checkpoint,\n",
        "    load_checkpoint,\n",
        "    save_json,\n",
        "    save_joblib,\n",
        ")\n",
        "from utils.training_utils.data_utils import count_samples\n",
        "from trainers.train_classifier import train_classifier\n",
        "from utils.training_utils.metrics import apply_temperature_scaling\n",
        "\n",
        "\n",
        "def _paths(cfg: dict, model: str, fold: int) -> dict[str, Path]:\n",
        "    \"\"\"\n",
        "    Costruisce tutti i path di output (training, inference, explain, aggregate, ecc.)\n",
        "    per uno specifico modello e fold, sempre ancorati a PROJECT_ROOT.\n",
        "    \"\"\"\n",
        "    # Parametri di formattazione iniziali\n",
        "    ph = {\n",
        "        'dataset_id': cfg['data']['dataset_id'],\n",
        "        'exp_code': cfg['exp_code'],\n",
        "        'model_name': model,\n",
        "        'fold_idx': fold,\n",
        "    }\n",
        "    # 1) exp_dir relativo\n",
        "    rel_exp_dir = cfg['output']['exp_dir'].format(**ph)\n",
        "    ph['exp_dir'] = rel_exp_dir\n",
        "    # 2) exp_model_dir relativo (usa ph['exp_dir'])\n",
        "    rel_exp_model_dir = cfg['output']['exp_model_dir'].format(**ph)\n",
        "\n",
        "    # Costruisci path assoluti sotto PROJECT_ROOT\n",
        "    exp_dir       = (PROJECT_ROOT / rel_exp_dir)\n",
        "    exp_model_dir = (PROJECT_ROOT / rel_exp_model_dir)\n",
        "\n",
        "    # Directory per training, inference, explain, aggregate, experiment level\n",
        "    tr = exp_model_dir / f\"fold{fold}\" / \"training\"\n",
        "    inf = exp_model_dir / f\"fold{fold}\" / \"inference\"\n",
        "    ex = exp_model_dir / f\"fold{fold}\" / \"explain\"\n",
        "    ag = exp_model_dir / \"_aggregate\"\n",
        "    el = exp_dir / \"_experiment_level\"\n",
        "\n",
        "    # Creazione cartelle\n",
        "    for d in (tr, inf, ex, ag, el):\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Ritorna i path assoluti per tutti gli artefatti\n",
        "    return {\n",
        "        'ckpt_dir':       tr.resolve(),\n",
        "        'ckpt_tpl':       (tr / f\"{model}_bestepoch{{epoch:03d}}_fold{fold}.pt\").resolve(),\n",
        "        'features':       (tr / f\"{model}_features_fold{fold}.pt\").resolve(),\n",
        "        'features_train': (tr / f\"{model}_features_train_fold{fold}.pt\").resolve(),\n",
        "        'features_val':   (tr / f\"{model}_features_val_fold{fold}.pt\").resolve(),\n",
        "        'clf':            (tr / f\"{model}_classifier_fold{fold}.joblib\").resolve(),\n",
        "        'scaler':         (tr / f\"{model}_ts_scaler_fold{fold}.joblib\").resolve(),\n",
        "        'log':            (tr / f\"{model}_train_log_fold{fold}.md\").resolve(),\n",
        "        'loss_json':      (tr / f\"{model}_train_valid_loss_fold{fold}.json\").resolve(),\n",
        "\n",
        "        'patch_preds':    (inf / f\"{model}_patch_preds_fold{fold}.pt\").resolve(),\n",
        "        'patient_preds':  (inf / f\"{model}_patient_preds_fold{fold}.csv\").resolve(),\n",
        "        'mc_logits':      (inf / f\"{model}_mc_logits_fold{fold}.npy\").resolve(),\n",
        "        'metrics':        (inf / f\"{model}_metrics_fold{fold}.json\").resolve(),\n",
        "\n",
        "        'gradcam_dir':    ex.resolve(),\n",
        "        'metadata_csv':   (ex / f\"{model}_metadata_gradcam_fold{fold}.csv\").resolve(),\n",
        "\n",
        "        'aggregate_metrics': (ag / f\"{model}_metrics.json\").resolve(),\n",
        "        'aggregate_summary': (ag / f\"{model}_summary_agg.jpg\").resolve(),\n",
        "\n",
        "        'comparison_json':    (el / \"models_comparison.json\").resolve(),\n",
        "        'comparison_img':     (el / \"models_comparison.jpg\").resolve(),\n",
        "\n",
        "        'readme':            (exp_dir / \"README_EXPERIMENT.md\").resolve(),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _completed(paths: dict[str, Path], is_ssl: bool) -> bool:\n",
        "    \"\"\"\n",
        "    Verifica se il training + artefatti SSL sono già stati completati.\n",
        "    \"\"\"\n",
        "    if not get_latest_checkpoint(paths[\"ckpt_dir\"]):\n",
        "        return False\n",
        "    if is_ssl:\n",
        "        return all(paths[k].exists() for k in (\n",
        "            \"features_train\", \"features_val\", \"clf\", \"scaler\", \"loss_json\"\n",
        "        ))\n",
        "    return True\n",
        "\n",
        "\n",
        "def _select_models(cfg: dict) -> dict[str, dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Seleziona i modelli da eseguire in base a `run_models` o tutti i modelli.\n",
        "    \"\"\"\n",
        "    wanted = cfg.get(\"run_models\") or list(cfg[\"models\"].keys())\n",
        "    return {name: cfg[\"models\"][name] for name in wanted}\n",
        "\n",
        "\n",
        "def _init_trainer(name: str, m_cfg: dict, data_cfg: dict, ckpt_dir: Path):\n",
        "    \"\"\"\n",
        "    Inizializza il trainer registrato per nome.\n",
        "    \"\"\"\n",
        "    tr = TRAINER_REGISTRY[name](m_cfg, data_cfg)\n",
        "    tr.ckpt_dir = ckpt_dir\n",
        "    tr.m_cfg = m_cfg\n",
        "    tr.data_cfg = data_cfg\n",
        "    tr.is_ssl = m_cfg.get(\"type\", \"\").lower() == \"ssl\"\n",
        "    return tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ptdtjmnBBJe8"
      },
      "outputs": [],
      "source": [
        "# %% ----------------------------------------------------------------------- #\n",
        "# Cell 5 – Training loop                                                     #\n",
        "# ----------------------------------------------------------------------- #\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "from utils.training_utils.device_io import get_latest_checkpoint, load_checkpoint, save_json\n",
        "from utils.training_utils.data_utils import count_samples\n",
        "from utils.training_utils.metrics import TemperatureScaler\n",
        "from utils.training_utils.data_utils import extract_labels_from_keys\n",
        "from trainers.train_classifier import train_classifier\n",
        "\n",
        "\n",
        "def _run_full_training(trainer, paths: dict[str, Path], epochs: int):\n",
        "    \"\"\"\n",
        "    Full training loop with per-batch logging, ETA estimation,\n",
        "    and unified support for SSL and SL trainers.\n",
        "    \"\"\"\n",
        "    is_ssl = getattr(trainer, \"is_ssl\", False)\n",
        "    batch_size = int(trainer.m_cfg[\"training\"][\"batch_size\"])\n",
        "\n",
        "    # Compute total batches if not set\n",
        "    if getattr(trainer, \"batches_train\", None) is None:\n",
        "        n_samples = count_samples(Path(trainer.data_cfg[\"train\"]))\n",
        "        trainer.batches_train = math.ceil(n_samples / batch_size)\n",
        "    total_batches = trainer.batches_train\n",
        "\n",
        "    history: list[dict] = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        start_time = time.time()\n",
        "        loss_sum = 0.0\n",
        "        corr_sum = 0\n",
        "        seen = 0\n",
        "        print(f\"[fold{trainer.cfg_fold}] ── Epoch {epoch}/{epochs} ──\")\n",
        "\n",
        "        for i, batch in enumerate(trainer.train_loader, start=1):\n",
        "            if i == 3:\n",
        "              break\n",
        "            # Support both signatures: train_step(batch) or train_step(x, y)\n",
        "            sig = inspect.signature(trainer.train_step)\n",
        "            result = (\n",
        "                trainer.train_step(*batch)\n",
        "                if len(sig.parameters) > 1\n",
        "                else trainer.train_step(batch)\n",
        "            )\n",
        "            if len(result) == 4:\n",
        "                _, loss, correct, bs = result\n",
        "            else:\n",
        "                loss, bs = result\n",
        "                correct = 0\n",
        "\n",
        "            loss_sum += loss * bs\n",
        "            corr_sum += correct\n",
        "            seen += bs\n",
        "\n",
        "            pct = (i / total_batches) * 100\n",
        "            eta = (time.time() - start_time) / i * (total_batches - i)\n",
        "            msg = (\n",
        "                f\"[fold{trainer.cfg_fold}] Batch {i}/{total_batches} \"\n",
        "                f\"({pct:5.1f}%) | Loss {loss_sum/seen:.4f}\"\n",
        "            )\n",
        "            if not is_ssl:\n",
        "                msg += f\" | Acc {corr_sum/seen:.3f}\"\n",
        "            msg += f\" | ETA {eta:6.1f}s\"\n",
        "            print(msg, flush=True)\n",
        "\n",
        "        train_loss = loss_sum / seen\n",
        "        if not is_ssl:\n",
        "            val_loss, val_acc = trainer.validate_epoch()\n",
        "            trainer.post_epoch(epoch, val_acc)\n",
        "            history.append({\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_acc\": val_acc,\n",
        "            })\n",
        "            print(f\"[fold{trainer.cfg_fold}] Val → Loss {val_loss:.4f} | Acc {val_acc:.3f}\")\n",
        "        else:\n",
        "            trainer.post_epoch(epoch, train_loss)\n",
        "            history.append({\"epoch\": epoch, \"train_loss\": train_loss})\n",
        "            print(f\"[fold{trainer.cfg_fold}] Train → Loss {train_loss:.4f}\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"[fold{trainer.cfg_fold}] ⏱  {elapsed:.1f}s\\n\")\n",
        "\n",
        "    # Save training history\n",
        "    save_json(history, paths[\"loss_json\"])\n",
        "\n",
        "\n",
        "def _resume_or_train(trainer, paths: dict[str, Path], epochs: int):\n",
        "    \"\"\"\n",
        "    Resume from last checkpoint if available, otherwise run full training.\n",
        "    \"\"\"\n",
        "    ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n",
        "    if ckpt:\n",
        "        print(f\"[fold{trainer.cfg_fold}] ⏩  Resuming from {ckpt.name}\")\n",
        "        model, optimizer = trainer.get_resume_model_and_optimizer()\n",
        "        load_checkpoint(ckpt, model, optimizer)\n",
        "    _run_full_training(trainer, paths, epochs)\n",
        "\n",
        "\n",
        "def _ensure_ssl_artifacts(trainer, paths: dict[str, Path]):\n",
        "    \"\"\"\n",
        "    For SSL models:\n",
        "      1) Extract train and val features\n",
        "      2) Train a linear probe on train features\n",
        "      3) Calibrate the probe via temperature scaling on val logits\n",
        "    \"\"\"\n",
        "    # 1) Extract train features\n",
        "    if not paths[\"features_train\"].exists():\n",
        "        trainer.extract_features_to(paths[\"features_train\"], split=\"train\")\n",
        "\n",
        "    # 2) Extract val features\n",
        "    if not paths[\"features_val\"].exists():\n",
        "        trainer.extract_features_to(paths[\"features_val\"], split=\"val\")\n",
        "\n",
        "    # 3) Train the linear classifier\n",
        "    if not paths[\"clf\"].exists():\n",
        "        train_classifier(str(paths[\"features_train\"]), str(paths[\"clf\"]))\n",
        "\n",
        "    # 4) Temperature scaling on validation logits\n",
        "    if not paths[\"scaler\"].exists():\n",
        "        # Load validation features\n",
        "        val_data = torch.load(paths[\"features_val\"], map_location=\"cpu\")\n",
        "        X_val = val_data[\"features\"].numpy()\n",
        "        keys_val = val_data[\"keys\"]\n",
        "\n",
        "        # Load classifier and label encoder\n",
        "        bundle = joblib.load(paths[\"clf\"])\n",
        "        clf = bundle[\"model\"]\n",
        "        le = bundle[\"label_encoder\"]\n",
        "\n",
        "        # Obtain logits or convert probs to logits\n",
        "        if hasattr(clf, \"decision_function\"):\n",
        "            logits = clf.decision_function(X_val)\n",
        "        else:\n",
        "            probs = clf.predict_proba(X_val)\n",
        "            logits = np.log(probs + 1e-12)\n",
        "\n",
        "        # Extract labels from keys\n",
        "        labels = extract_labels_from_keys(keys_val, le)\n",
        "\n",
        "        # Fit the TemperatureScaler\n",
        "        scaler = TemperatureScaler().fit(logits, labels)\n",
        "        joblib.dump(scaler, str(paths[\"scaler\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rMV3BYIyE7FM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9239fd5b-c372-418f-8b3d-23edc20eb0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀  Modello 'rotation'  (SSL) – epochs=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:381: UserWarning: set WebDataset(shardshuffle=...) to a positive integer or 0 or False\n",
            "  warnings.warn(\"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold0] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold0/training\n",
            "[fold0] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n",
            "[fold0] 🚀  avvio trainer  → 'rotation'\n",
            "[fold0] ── Epoch 1/2 ──\n",
            "[fold0] Batch 1/62 (  1.6%) | Loss 1.4971 | ETA 1152.4s\n",
            "[fold0] Batch 2/62 (  3.2%) | Loss 1.8998 | ETA  951.6s\n",
            "[fold0] Train → Loss 1.8998\n",
            "[fold0] ⏱  32.3s\n",
            "\n",
            "[fold0] ── Epoch 2/2 ──\n",
            "[fold0] Batch 1/62 (  1.6%) | Loss 6.1737 | ETA  810.1s\n",
            "[fold0] Batch 2/62 (  3.2%) | Loss 4.0599 | ETA  778.0s\n",
            "[fold0] Train → Loss 4.0599\n",
            "[fold0] ⏱  26.4s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n",
            "Extracting features: 185it [11:48,  3.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Rotation features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold0/training/rotation_features_train_fold0.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n",
            "Extracting features: 62it [04:03,  3.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Rotation features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold0/training/rotation_features_val_fold0.pt\n",
            "✅ Loaded 1473 keys and (1473, 4) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 300, np.str_('ONCO'): 298, np.str_('ccRCC'): 297, np.str_('pRCC'): 294, np.str_('not_tumor'): 284})\n",
            "✅ Filtered dataset: 1473 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.39      0.28      0.33        60\n",
            "        ONCO       0.38      0.25      0.30        60\n",
            "       ccRCC       0.34      0.39      0.36        59\n",
            "   not_tumor       0.40      0.53      0.45        57\n",
            "        pRCC       0.65      0.76      0.70        59\n",
            "\n",
            "    accuracy                           0.44       295\n",
            "   macro avg       0.43      0.44      0.43       295\n",
            "weighted avg       0.43      0.44      0.43       295\n",
            "\n",
            "Confusion Matrix:\n",
            " [[17 13 18  8  4]\n",
            " [11 15 14 10 10]\n",
            " [11  3 23 20  2]\n",
            " [ 5  2 12 30  8]\n",
            " [ 0  6  1  7 45]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold0/training/rotation_classifier_fold0.joblib\n",
            "[fold0] ✅  completato\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:381: UserWarning: set WebDataset(shardshuffle=...) to a positive integer or 0 or False\n",
            "  warnings.warn(\"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold1] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold1/training\n",
            "[fold1] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold1/train/patches-0000.tar\n",
            "[fold1] 🚀  avvio trainer  → 'rotation'\n",
            "[fold1] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold0/training/RotationTrainer_bestepoch001.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n",
            "Extracting features: 189it [12:38,  4.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Rotation features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold1/training/rotation_features_train_fold1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n",
            "Extracting features: 58it [03:28,  3.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Rotation features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold1/training/rotation_features_val_fold1.pt\n",
            "✅ Loaded 1506 keys and (1506, 4) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('pRCC'): 309, np.str_('ONCO'): 308, np.str_('CHROMO'): 303, np.str_('ccRCC'): 293, np.str_('not_tumor'): 293})\n",
            "✅ Filtered dataset: 1506 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.55      0.82      0.66        61\n",
            "        ONCO       0.39      0.37      0.38        62\n",
            "       ccRCC       0.44      0.14      0.21        59\n",
            "   not_tumor       0.44      0.48      0.46        58\n",
            "        pRCC       0.56      0.65      0.60        62\n",
            "\n",
            "    accuracy                           0.49       302\n",
            "   macro avg       0.48      0.49      0.46       302\n",
            "weighted avg       0.48      0.49      0.46       302\n",
            "\n",
            "Confusion Matrix:\n",
            " [[50 11  0  0  0]\n",
            " [18 23  0  8 13]\n",
            " [15 16  8 11  9]\n",
            " [ 7  4 10 28  9]\n",
            " [ 1  5  0 16 40]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold1/training/rotation_classifier_fold1.joblib\n",
            "[fold1] ✅  completato\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:381: UserWarning: set WebDataset(shardshuffle=...) to a positive integer or 0 or False\n",
            "  warnings.warn(\"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold2] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold2/training"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:381: UserWarning: set WebDataset(shardshuffle=...) to a positive integer or 0 or False\n",
            "  warnings.warn(\"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[fold2] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold2/train/patches-0000.tar\n",
            "[fold2] 🚀  avvio trainer  → 'rotation'\n",
            "[fold2] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold0/training/RotationTrainer_bestepoch001.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n",
            "Extracting features: 182it [11:39,  3.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Rotation features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold2/training/rotation_features_train_fold2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n",
            "Extracting features: 65it [03:48,  3.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Rotation features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold2/training/rotation_features_val_fold2.pt\n",
            "✅ Loaded 1452 keys and (1452, 4) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 298, np.str_('ccRCC'): 297, np.str_('ONCO'): 291, np.str_('pRCC'): 283, np.str_('not_tumor'): 283})\n",
            "✅ Filtered dataset: 1452 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.52      0.77      0.62        60\n",
            "        ONCO       0.32      0.14      0.19        58\n",
            "       ccRCC       0.52      0.46      0.49        59\n",
            "   not_tumor       0.48      0.44      0.46        57\n",
            "        pRCC       0.70      0.89      0.78        57\n",
            "\n",
            "    accuracy                           0.54       291\n",
            "   macro avg       0.51      0.54      0.51       291\n",
            "weighted avg       0.51      0.54      0.51       291\n",
            "\n",
            "Confusion Matrix:\n",
            " [[46  5  2  5  2]\n",
            " [27  8  4  9 10]\n",
            " [14 10 27  8  0]\n",
            " [ 1  2 19 25 10]\n",
            " [ 1  0  0  5 51]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/rotation/fold2/training/rotation_classifier_fold2.joblib\n",
            "[fold2] ✅  completato\n",
            "\n",
            "\n",
            "🚀  Modello 'simclr'  (SSL) – epochs=2\n",
            "[fold0] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold0/training\n",
            "[fold0] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n",
            "[fold0] 🚀  avvio trainer  → 'simclr'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold0] ── Epoch 1/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 2.7063 | ETA 4529.4s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 2.8485 | ETA 4534.7s\n",
            "[fold0] Train → Loss 2.8485\n",
            "[fold0] ⏱  50.8s\n",
            "\n",
            "[fold0] ── Epoch 2/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 3.7342 | ETA 4754.3s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 3.2813 | ETA 4676.1s\n",
            "[fold0] Train → Loss 3.2813\n",
            "[fold0] ⏱  52.0s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 185it [10:36,  3.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SimCLR features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold0/training/simclr_features_train_fold0.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 62it [03:34,  3.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SimCLR features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold0/training/simclr_features_val_fold0.pt\n",
            "✅ Loaded 1473 keys and (1473, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 300, np.str_('ONCO'): 298, np.str_('ccRCC'): 297, np.str_('pRCC'): 294, np.str_('not_tumor'): 284})\n",
            "✅ Filtered dataset: 1473 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.76      0.75      0.76        60\n",
            "        ONCO       0.86      0.53      0.66        60\n",
            "       ccRCC       0.59      0.66      0.62        59\n",
            "   not_tumor       0.39      0.46      0.42        57\n",
            "        pRCC       0.76      0.86      0.81        59\n",
            "\n",
            "    accuracy                           0.65       295\n",
            "   macro avg       0.67      0.65      0.65       295\n",
            "weighted avg       0.68      0.65      0.66       295\n",
            "\n",
            "Confusion Matrix:\n",
            " [[45  5  4  4  2]\n",
            " [ 7 32  0 15  6]\n",
            " [ 4  0 39 15  1]\n",
            " [ 3  0 21 26  7]\n",
            " [ 0  0  2  6 51]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold0/training/simclr_classifier_fold0.joblib\n",
            "[fold0] ✅  completato\n",
            "\n",
            "[fold1] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold1/training\n",
            "[fold1] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold1/train/patches-0000.tar\n",
            "[fold1] 🚀  avvio trainer  → 'simclr'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold1] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold0/training/SimCLRTrainer_bestepoch001.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 189it [11:05,  3.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SimCLR features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold1/training/simclr_features_train_fold1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 58it [03:23,  3.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SimCLR features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold1/training/simclr_features_val_fold1.pt\n",
            "✅ Loaded 1506 keys and (1506, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('pRCC'): 309, np.str_('ONCO'): 308, np.str_('CHROMO'): 303, np.str_('ccRCC'): 293, np.str_('not_tumor'): 293})\n",
            "✅ Filtered dataset: 1506 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.60      0.82      0.69        61\n",
            "        ONCO       0.53      0.31      0.39        62\n",
            "       ccRCC       0.36      0.42      0.39        59\n",
            "   not_tumor       0.50      0.31      0.38        58\n",
            "        pRCC       0.55      0.68      0.61        62\n",
            "\n",
            "    accuracy                           0.51       302\n",
            "   macro avg       0.51      0.51      0.49       302\n",
            "weighted avg       0.51      0.51      0.49       302\n",
            "\n",
            "Confusion Matrix:\n",
            " [[50  4  6  0  1]\n",
            " [12 19 10  3 18]\n",
            " [16  0 25  5 13]\n",
            " [ 6  7 25 18  2]\n",
            " [ 0  6  4 10 42]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold1/training/simclr_classifier_fold1.joblib\n",
            "[fold1] ✅  completato\n",
            "\n",
            "[fold2] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold2/training\n",
            "[fold2] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold2/train/patches-0000.tar\n",
            "[fold2] 🚀  avvio trainer  → 'simclr'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold2] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold0/training/SimCLRTrainer_bestepoch001.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 182it [10:41,  3.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SimCLR features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold2/training/simclr_features_train_fold2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 65it [03:47,  3.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SimCLR features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold2/training/simclr_features_val_fold2.pt\n",
            "✅ Loaded 1452 keys and (1452, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 298, np.str_('ccRCC'): 297, np.str_('ONCO'): 291, np.str_('pRCC'): 283, np.str_('not_tumor'): 283})\n",
            "✅ Filtered dataset: 1452 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.74      0.75      0.74        60\n",
            "        ONCO       0.72      0.71      0.71        58\n",
            "       ccRCC       0.61      0.58      0.59        59\n",
            "   not_tumor       0.56      0.53      0.54        57\n",
            "        pRCC       0.78      0.86      0.82        57\n",
            "\n",
            "    accuracy                           0.68       291\n",
            "   macro avg       0.68      0.68      0.68       291\n",
            "weighted avg       0.68      0.68      0.68       291\n",
            "\n",
            "Confusion Matrix:\n",
            " [[45  8  1  5  1]\n",
            " [ 3 41  2  5  7]\n",
            " [12  1 34 11  1]\n",
            " [ 1  3 18 30  5]\n",
            " [ 0  4  1  3 49]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/simclr/fold2/training/simclr_classifier_fold2.joblib\n",
            "[fold2] ✅  completato\n",
            "\n",
            "\n",
            "🚀  Modello 'moco_v2'  (SSL) – epochs=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold0] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold0/training\n",
            "[fold0] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n",
            "[fold0] 🚀  avvio trainer  → 'moco_v2'\n",
            "[fold0] ── Epoch 1/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 0.0167 | ETA 3804.0s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 1.1125 | ETA 3590.6s\n",
            "[fold0] Train → Loss 1.1125\n",
            "[fold0] ⏱  43.4s\n",
            "\n",
            "[fold0] ── Epoch 2/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 2.8359 | ETA 3521.7s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 3.0364 | ETA 3745.8s\n",
            "[fold0] Train → Loss 3.0364\n",
            "[fold0] ⏱  42.6s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 185it [10:49,  3.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MoCo v2 features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold0/training/moco_v2_features_train_fold0.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 62it [03:38,  3.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MoCo v2 features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold0/training/moco_v2_features_val_fold0.pt\n",
            "✅ Loaded 1473 keys and (1473, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 300, np.str_('ONCO'): 298, np.str_('ccRCC'): 297, np.str_('pRCC'): 294, np.str_('not_tumor'): 284})\n",
            "✅ Filtered dataset: 1473 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.78      0.82      0.80        60\n",
            "        ONCO       0.84      0.70      0.76        60\n",
            "       ccRCC       0.65      0.83      0.73        59\n",
            "   not_tumor       0.67      0.49      0.57        57\n",
            "        pRCC       0.83      0.92      0.87        59\n",
            "\n",
            "    accuracy                           0.75       295\n",
            "   macro avg       0.75      0.75      0.75       295\n",
            "weighted avg       0.75      0.75      0.75       295\n",
            "\n",
            "Confusion Matrix:\n",
            " [[49  7  2  1  1]\n",
            " [13 42  1  2  2]\n",
            " [ 0  0 49  9  1]\n",
            " [ 0  0 22 28  7]\n",
            " [ 1  1  1  2 54]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold0/training/moco_v2_classifier_fold0.joblib\n",
            "[fold0] ✅  completato\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold1] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold1/training\n",
            "[fold1] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold1/train/patches-0000.tar\n",
            "[fold1] 🚀  avvio trainer  → 'moco_v2'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold1] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold0/training/MoCoV2Trainer_bestepoch001.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 189it [11:15,  3.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MoCo v2 features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold1/training/moco_v2_features_train_fold1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 58it [03:29,  3.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MoCo v2 features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold1/training/moco_v2_features_val_fold1.pt\n",
            "✅ Loaded 1506 keys and (1506, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('pRCC'): 309, np.str_('ONCO'): 308, np.str_('CHROMO'): 303, np.str_('ccRCC'): 293, np.str_('not_tumor'): 293})\n",
            "✅ Filtered dataset: 1506 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.84      0.93      0.88        61\n",
            "        ONCO       0.91      0.79      0.84        62\n",
            "       ccRCC       0.61      0.53      0.56        59\n",
            "   not_tumor       0.59      0.66      0.62        58\n",
            "        pRCC       0.74      0.77      0.76        62\n",
            "\n",
            "    accuracy                           0.74       302\n",
            "   macro avg       0.74      0.74      0.73       302\n",
            "weighted avg       0.74      0.74      0.74       302\n",
            "\n",
            "Confusion Matrix:\n",
            " [[57  2  1  0  1]\n",
            " [ 5 49  5  1  2]\n",
            " [ 5  1 31 13  9]\n",
            " [ 1  0 14 38  5]\n",
            " [ 0  2  0 12 48]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold1/training/moco_v2_classifier_fold1.joblib\n",
            "[fold1] ✅  completato\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold2] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold2/training\n",
            "[fold2] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold2/train/patches-0000.tar\n",
            "[fold2] 🚀  avvio trainer  → 'moco_v2'\n",
            "[fold2] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold0/training/MoCoV2Trainer_bestepoch001.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 182it [10:56,  3.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MoCo v2 features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold2/training/moco_v2_features_train_fold2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 65it [03:52,  3.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ MoCo v2 features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold2/training/moco_v2_features_val_fold2.pt\n",
            "✅ Loaded 1452 keys and (1452, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 298, np.str_('ccRCC'): 297, np.str_('ONCO'): 291, np.str_('pRCC'): 283, np.str_('not_tumor'): 283})\n",
            "✅ Filtered dataset: 1452 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.86      0.85      0.86        60\n",
            "        ONCO       0.87      0.78      0.82        58\n",
            "       ccRCC       0.68      0.80      0.73        59\n",
            "   not_tumor       0.72      0.54      0.62        57\n",
            "        pRCC       0.79      0.95      0.86        57\n",
            "\n",
            "    accuracy                           0.78       291\n",
            "   macro avg       0.79      0.78      0.78       291\n",
            "weighted avg       0.79      0.78      0.78       291\n",
            "\n",
            "Confusion Matrix:\n",
            " [[51  6  0  1  2]\n",
            " [ 6 45  2  0  5]\n",
            " [ 2  1 47  8  1]\n",
            " [ 0  0 20 31  6]\n",
            " [ 0  0  0  3 54]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/moco_v2/fold2/training/moco_v2_classifier_fold2.joblib\n",
            "[fold2] ✅  completato\n",
            "\n",
            "\n",
            "🚀  Modello 'jepa'  (SSL) – epochs=2\n",
            "[fold0] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold0/training\n",
            "[fold0] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n",
            "[fold0] 🚀  avvio trainer  → 'jepa'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold0] ── Epoch 1/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 4.1079 | ETA 4805.0s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 3.5735 | ETA 4661.1s\n",
            "[fold0] Train → Loss 3.5735\n",
            "[fold0] ⏱  52.0s\n",
            "\n",
            "[fold0] ── Epoch 2/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 2.7193 | ETA 4471.5s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 2.7679 | ETA 4421.2s\n",
            "[fold0] Train → Loss 2.7679\n",
            "[fold0] ⏱  48.9s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 185it [10:46,  3.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JEPA features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold0/training/jepa_features_train_fold0.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 62it [03:36,  3.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JEPA features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold0/training/jepa_features_val_fold0.pt\n",
            "✅ Loaded 1473 keys and (1473, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 300, np.str_('ONCO'): 298, np.str_('ccRCC'): 297, np.str_('pRCC'): 294, np.str_('not_tumor'): 284})\n",
            "✅ Filtered dataset: 1473 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.79      0.82      0.80        60\n",
            "        ONCO       0.85      0.67      0.75        60\n",
            "       ccRCC       0.64      0.76      0.70        59\n",
            "   not_tumor       0.60      0.51      0.55        57\n",
            "        pRCC       0.76      0.88      0.82        59\n",
            "\n",
            "    accuracy                           0.73       295\n",
            "   macro avg       0.73      0.73      0.72       295\n",
            "weighted avg       0.73      0.73      0.73       295\n",
            "\n",
            "Confusion Matrix:\n",
            " [[49  7  1  1  2]\n",
            " [ 9 40  0  3  8]\n",
            " [ 3  0 45 10  1]\n",
            " [ 0  0 23 29  5]\n",
            " [ 1  0  1  5 52]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold0/training/jepa_classifier_fold0.joblib\n",
            "[fold0] ✅  completato\n",
            "\n",
            "[fold1] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold1/training\n",
            "[fold1] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold1/train/patches-0000.tar\n",
            "[fold1] 🚀  avvio trainer  → 'jepa'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold1] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold0/training/JEPATrainer_bestepoch002.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 189it [11:13,  3.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JEPA features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold1/training/jepa_features_train_fold1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 58it [03:25,  3.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JEPA features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold1/training/jepa_features_val_fold1.pt\n",
            "✅ Loaded 1506 keys and (1506, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('pRCC'): 309, np.str_('ONCO'): 308, np.str_('CHROMO'): 303, np.str_('ccRCC'): 293, np.str_('not_tumor'): 293})\n",
            "✅ Filtered dataset: 1506 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.75      0.92      0.82        61\n",
            "        ONCO       0.95      0.56      0.71        62\n",
            "       ccRCC       0.39      0.46      0.42        59\n",
            "   not_tumor       0.40      0.40      0.40        58\n",
            "        pRCC       0.69      0.69      0.69        62\n",
            "\n",
            "    accuracy                           0.61       302\n",
            "   macro avg       0.63      0.61      0.61       302\n",
            "weighted avg       0.64      0.61      0.61       302\n",
            "\n",
            "Confusion Matrix:\n",
            " [[56  1  3  0  1]\n",
            " [ 6 35  9  4  8]\n",
            " [11  0 27 16  5]\n",
            " [ 2  0 28 23  5]\n",
            " [ 0  1  3 15 43]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold1/training/jepa_classifier_fold1.joblib\n",
            "[fold1] ✅  completato\n",
            "\n",
            "[fold2] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold2/training\n",
            "[fold2] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold2/train/patches-0000.tar\n",
            "[fold2] 🚀  avvio trainer  → 'jepa'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold2] ✅ encoder da fold0 → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold0/training/JEPATrainer_bestepoch002.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 182it [10:52,  3.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JEPA features (train) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold2/training/jepa_features_train_fold2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 65it [03:53,  3.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JEPA features (val) saved → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold2/training/jepa_features_val_fold2.pt\n",
            "✅ Loaded 1452 keys and (1452, 512) features\n",
            "📊 Class distribution:\n",
            "Counter({np.str_('CHROMO'): 298, np.str_('ccRCC'): 297, np.str_('ONCO'): 291, np.str_('pRCC'): 283, np.str_('not_tumor'): 283})\n",
            "✅ Filtered dataset: 1452 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.91      0.82      0.86        60\n",
            "        ONCO       0.85      0.69      0.76        58\n",
            "       ccRCC       0.66      0.78      0.71        59\n",
            "   not_tumor       0.61      0.54      0.57        57\n",
            "        pRCC       0.75      0.91      0.83        57\n",
            "\n",
            "    accuracy                           0.75       291\n",
            "   macro avg       0.76      0.75      0.75       291\n",
            "weighted avg       0.76      0.75      0.75       291\n",
            "\n",
            "Confusion Matrix:\n",
            " [[49  7  0  1  3]\n",
            " [ 2 40  3  5  8]\n",
            " [ 3  0 46 10  0]\n",
            " [ 0  0 20 31  6]\n",
            " [ 0  0  1  4 52]]\n",
            "💾 Classifier saved to /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/jepa/fold2/training/jepa_classifier_fold2.joblib\n",
            "[fold2] ✅  completato\n",
            "\n",
            "\n",
            "🚀  Modello 'supervised'  (SL) – epochs=2\n",
            "[fold0] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/supervised/fold0/training\n",
            "[fold0] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n",
            "[fold0] 🚀  avvio trainer  → 'supervised'\n",
            "[fold0] ── Epoch 1/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 1.6853 | Acc 0.000 | ETA 6661.6s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 0.8426 | Acc 0.500 | ETA 6313.6s\n",
            "[fold0] Val → Loss 19059.2732 | Acc 0.202\n",
            "[fold0] ⏱  620.1s\n",
            "\n",
            "[fold0] ── Epoch 2/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 0.0000 | Acc 1.000 | ETA 6203.6s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 0.0000 | Acc 1.000 | ETA 6037.7s\n",
            "[fold0] Val → Loss 45306.4093 | Acc 0.202\n",
            "[fold0] ⏱  618.2s\n",
            "\n",
            "[fold0] ✅  completato\n",
            "\n",
            "[fold1] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/supervised/fold1/training\n",
            "[fold1] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold1/train/patches-0000.tar\n",
            "[fold1] 🚀  avvio trainer  → 'supervised'\n",
            "[fold1] ── Epoch 1/2 ──\n",
            "[fold1] Batch 1/189 (  0.5%) | Loss 1.9659 | Acc 0.000 | ETA 6197.4s\n",
            "[fold1] Batch 2/189 (  1.1%) | Loss 0.9829 | Acc 0.500 | ETA 6244.3s\n",
            "[fold1] Val → Loss 13198.7759 | Acc 0.222\n",
            "[fold1] ⏱  643.4s\n",
            "\n",
            "[fold1] ── Epoch 2/2 ──\n",
            "[fold1] Batch 1/189 (  0.5%) | Loss 0.0000 | Acc 1.000 | ETA 6212.7s\n",
            "[fold1] Batch 2/189 (  1.1%) | Loss 0.0000 | Acc 1.000 | ETA 6069.7s\n",
            "[fold1] Val → Loss 33301.9259 | Acc 0.222\n",
            "[fold1] ⏱  581.5s\n",
            "\n",
            "[fold1] ✅  completato\n",
            "\n",
            "[fold2] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/supervised/fold2/training\n",
            "[fold2] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold2/train/patches-0000.tar\n",
            "[fold2] 🚀  avvio trainer  → 'supervised'\n",
            "[fold2] ── Epoch 1/2 ──\n",
            "[fold2] Batch 1/182 (  0.5%) | Loss 1.5516 | Acc 0.000 | ETA 5688.4s\n",
            "[fold2] Batch 2/182 (  1.1%) | Loss 0.7758 | Acc 0.500 | ETA 5659.5s\n",
            "[fold2] Val → Loss 17979.7677 | Acc 0.191\n",
            "[fold2] ⏱  703.6s\n",
            "\n",
            "[fold2] ── Epoch 2/2 ──\n",
            "[fold2] Batch 1/182 (  0.5%) | Loss 0.0000 | Acc 1.000 | ETA 5984.8s\n",
            "[fold2] Batch 2/182 (  1.1%) | Loss 0.0000 | Acc 1.000 | ETA 5874.0s\n",
            "[fold2] Val → Loss 59791.5754 | Acc 0.191\n",
            "[fold2] ⏱  710.3s\n",
            "\n",
            "[fold2] ✅  completato\n",
            "\n",
            "\n",
            "🚀  Modello 'transfer'  (SL) – epochs=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 143MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold0] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/transfer/fold0/training\n",
            "[fold0] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold0/train/patches-0000.tar\n",
            "[fold0] 🚀  avvio trainer  → 'transfer'\n",
            "[fold0] ── Epoch 1/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 1.6698 | Acc 0.000 | ETA 1968.2s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 1.6536 | Acc 0.000 | ETA 1890.1s\n",
            "[fold0] Val → Loss 1.6085 | Acc 0.181\n",
            "[fold0] ⏱  538.6s\n",
            "\n",
            "[fold0] ── Epoch 2/2 ──\n",
            "[fold0] Batch 1/185 (  0.5%) | Loss 1.6160 | Acc 0.000 | ETA 1981.4s\n",
            "[fold0] Batch 2/185 (  1.1%) | Loss 1.6032 | Acc 0.062 | ETA 1913.0s\n",
            "[fold0] Val → Loss 1.6009 | Acc 0.258\n",
            "[fold0] ⏱  530.3s\n",
            "\n",
            "[fold0] ✅  completato\n",
            "\n",
            "[fold1] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/transfer/fold1/training\n",
            "[fold1] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold1/train/patches-0000.tar\n",
            "[fold1] 🚀  avvio trainer  → 'transfer'\n",
            "[fold1] ── Epoch 1/2 ──\n",
            "[fold1] Batch 1/189 (  0.5%) | Loss 1.6173 | Acc 0.125 | ETA 1985.3s\n",
            "[fold1] Batch 2/189 (  1.1%) | Loss 1.5913 | Acc 0.250 | ETA 1852.6s\n",
            "[fold1] Val → Loss 1.6105 | Acc 0.192\n",
            "[fold1] ⏱  497.2s\n",
            "\n",
            "[fold1] ── Epoch 2/2 ──\n",
            "[fold1] Batch 1/189 (  0.5%) | Loss 1.5709 | Acc 0.625 | ETA 1807.9s\n",
            "[fold1] Batch 2/189 (  1.1%) | Loss 1.5338 | Acc 0.812 | ETA 1860.0s\n",
            "[fold1] Val → Loss 1.6068 | Acc 0.186\n",
            "[fold1] ⏱  500.8s\n",
            "\n",
            "[fold1] ✅  completato\n",
            "\n",
            "[fold2] 📂  ckpt dir      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250712175557/transfer/fold2/training\n",
            "[fold2] 🏷   train shards  → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/fold2/train/patches-0000.tar\n",
            "[fold2] 🚀  avvio trainer  → 'transfer'\n",
            "[fold2] ── Epoch 1/2 ──\n",
            "[fold2] Batch 1/182 (  0.5%) | Loss 1.6136 | Acc 0.125 | ETA 1787.5s\n",
            "[fold2] Batch 2/182 (  1.1%) | Loss 1.6112 | Acc 0.062 | ETA 1826.6s\n",
            "[fold2] Val → Loss 1.5947 | Acc 0.215\n",
            "[fold2] ⏱  552.5s\n",
            "\n",
            "[fold2] ── Epoch 2/2 ──\n",
            "[fold2] Batch 1/182 (  0.5%) | Loss 1.5663 | Acc 0.500 | ETA 1664.7s\n",
            "[fold2] Batch 2/182 (  1.1%) | Loss 1.5528 | Acc 0.625 | ETA 1753.3s\n",
            "[fold2] Val → Loss 1.5984 | Acc 0.195\n",
            "[fold2] ⏱  553.8s\n",
            "\n",
            "[fold2] ✅  completato\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %% -------------------------------------------------------------------- #\n",
        "# Cell 6 – Modular Launch & Auto-Recover (cross-fold)                     #\n",
        "# ------------------------------------------------------------------------#\n",
        "from pathlib import Path\n",
        "import contextlib, sys, torch, os\n",
        "from utils.training_utils.device_io import get_latest_checkpoint, load_checkpoint\n",
        "\n",
        "def launch_training(cfg: dict) -> None:\n",
        "    \"\"\"Esegue training + generazione artefatti per tutti i modelli e fold.\"\"\"\n",
        "    for name, m_cfg in _select_models(cfg).items():\n",
        "        is_ssl = (m_cfg.get(\"type\") == \"ssl\")\n",
        "        epochs = int(m_cfg[\"training\"][\"epochs\"])\n",
        "        print(f\"\\n🚀  Modello '{name}'  ({'SSL' if is_ssl else 'SL'}) – epochs={epochs}\")\n",
        "\n",
        "        for fold in cfg[\"folds\"]:\n",
        "            # 1) Configurazione dati\n",
        "            data_cfg = {\n",
        "                \"train\": str((PROJECT_ROOT / cfg[\"data\"][\"train\"]\n",
        "                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"], fold_idx=fold))\n",
        "                             .resolve()),\n",
        "                \"val\":   str((PROJECT_ROOT / cfg[\"data\"][\"val\"]\n",
        "                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"], fold_idx=fold))\n",
        "                             .resolve()),\n",
        "                \"test\":  str((PROJECT_ROOT / cfg[\"data\"][\"test\"]\n",
        "                              .format(dataset_id=cfg[\"data\"][\"dataset_id\"]))\n",
        "                             .resolve())\n",
        "            }\n",
        "            if not Path(data_cfg[\"train\"]).exists():\n",
        "                print(f\"[fold{fold}] ⚠️  shard train mancante → skip\")\n",
        "                continue\n",
        "\n",
        "            # 2) Path output + trainer\n",
        "            paths   = _paths(cfg, name, fold)\n",
        "            trainer = _init_trainer(name, m_cfg, data_cfg, paths[\"ckpt_dir\"])\n",
        "            trainer.cfg_fold     = fold\n",
        "            trainer.train_loader = trainer.build_loader(\"train\")\n",
        "            if hasattr(trainer, \"validate_epoch\"):\n",
        "                trainer.val_loader = trainer.build_loader(\"val\")\n",
        "\n",
        "            # 3) Logging su stdout + file\n",
        "            with open(paths[\"log\"], \"a\") as logf, \\\n",
        "                 contextlib.redirect_stdout(_Tee(sys.stdout, logf)), \\\n",
        "                 contextlib.redirect_stderr(_Tee(sys.stderr, logf)):\n",
        "\n",
        "                print(f\"[fold{fold}] 📂  ckpt dir      → {paths['ckpt_dir'].resolve()}\")\n",
        "                print(f\"[fold{fold}] 🏷   train shards  → {Path(data_cfg['train']).resolve()}\")\n",
        "                print(f\"[fold{fold}] 🚀  avvio trainer  → '{name}'\")\n",
        "\n",
        "                # 3.1) Se abbiamo già tutti gli artefatti, salta interamente\n",
        "                if _completed(paths, is_ssl):\n",
        "                    print(f\"[fold{fold}] ⚡  Artefatti già presenti → skip training + SSL pipeline\")\n",
        "                else:\n",
        "                    # 4) SSL: encoder once su fold0 oppure training completo\n",
        "                    if is_ssl and cfg.get(\"train_encoder_once\", False) and fold > 0:\n",
        "                        fold0_ckpt = get_latest_checkpoint(_paths(cfg, name, 0)[\"ckpt_dir\"])\n",
        "                        if fold0_ckpt:\n",
        "                            mdl, _ = trainer.get_resume_model_and_optimizer()\n",
        "                            load_checkpoint(fold0_ckpt, mdl, None)\n",
        "                            print(f\"[fold{fold}] ✅ encoder da fold0 → {fold0_ckpt.resolve()}\")\n",
        "                        else:\n",
        "                            print(f\"[fold{fold}] ⚠️  encoder fold0 mancante → train completo\")\n",
        "                            _resume_or_train(trainer, paths, epochs)\n",
        "                    else:\n",
        "                        _resume_or_train(trainer, paths, epochs)\n",
        "\n",
        "                    # 5) SSL – Feature extraction, probe, T-scaling\n",
        "                    if is_ssl:\n",
        "                        _ensure_ssl_artifacts(trainer, paths)\n",
        "\n",
        "                # 6) Append a esperimenti globali (anche in caso di skip)\n",
        "                latest_ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n",
        "                if latest_ckpt:\n",
        "                    base_dir = EXP_BASE.parent.parent.resolve()\n",
        "                    try:\n",
        "                        rel_path = os.path.relpath(str(latest_ckpt.resolve()), str(base_dir))\n",
        "                    except Exception:\n",
        "                        rel_path = str(latest_ckpt.resolve())\n",
        "                else:\n",
        "                    rel_path = \"-\"\n",
        "                _global_experiments_append(\n",
        "                    f\"| {cfg['exp_code']} | {name} | fold{fold} | {epochs} | {rel_path} |\"\n",
        "                )\n",
        "\n",
        "                print(f\"[fold{fold}] ✅  completato\\n\")\n",
        "\n",
        "# ─── Avvio automatico in Colab ───────────────────────────────────────────\n",
        "if IN_COLAB:\n",
        "    launch_training(cfg)\n",
        "else:\n",
        "    print(\"⏩  Ambiente locale: esegui manualmente  launch_training(cfg) per partire.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6196183d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24942ba1-84bb-4c24-aa94-76fae079c1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Verifica artefatti TRAINING\n",
            "\n",
            "❌ Mancano 11 artefatti TRAINING:\n",
            "\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/jepa/fold0/training/jepa_bestepoch*_fold0.pt\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/moco_v2/fold0/training/moco_v2_bestepoch*_fold0.pt\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/moco_v2/fold0/training/moco_v2_train_log_fold0.md\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/moco_v2/fold0/training/moco_v2_train_valid_loss_fold0.json\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/rotation/fold0/training/rotation_bestepoch*_fold0.pt\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/simclr/fold0/training/simclr_bestepoch*_fold0.pt\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/supervised/fold0/training/supervised_bestepoch*_fold0.pt\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/supervised/fold1/training/supervised_bestepoch*_fold1.pt\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/supervised/fold1/training/supervised_train_valid_loss_fold1.json\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/transfer/fold0/training/transfer_bestepoch*_fold0.pt\n",
            " • /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129/transfer/fold1/training/transfer_bestepoch*_fold1.pt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Configura esperimento\n",
        "EXP_CODE = \"20250709142129\"\n",
        "ROOT = Path(\"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\")\n",
        "EXP_DIR = ROOT / \"data/processed/dataset_9f30917e/experiments\" / EXP_CODE\n",
        "\n",
        "# Modelli\n",
        "SSL_MODELS = {\"simclr\", \"rotation\", \"moco_v2\", \"jepa\"}\n",
        "SL_MODELS  = {\"supervised\", \"transfer\"}\n",
        "ALL_MODELS = SSL_MODELS | SL_MODELS\n",
        "N_FOLDS = 2\n",
        "\n",
        "# File attesi per tipo di modello\n",
        "FILES_PER_MODEL = {\n",
        "    \"SSL\": [\n",
        "        \"{model}_bestepoch*_fold{i}.pt\",\n",
        "        \"{model}_train_log_fold{i}.md\",\n",
        "        \"{model}_train_valid_loss_fold{i}.json\",\n",
        "    ],\n",
        "    \"SL\": [\n",
        "        \"{model}_bestepoch*_fold{i}.pt\",\n",
        "        \"{model}_train_log_fold{i}.md\",\n",
        "        \"{model}_train_valid_loss_fold{i}.json\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "missing = []\n",
        "\n",
        "for model in sorted(ALL_MODELS):\n",
        "    model_type = \"SSL\" if model in SSL_MODELS else \"SL\"\n",
        "    folds = [0] if model_type == \"SSL\" else list(range(N_FOLDS))\n",
        "\n",
        "    for i in folds:\n",
        "        train_dir = EXP_DIR / model / f\"fold{i}\" / \"training\"\n",
        "        for pattern in FILES_PER_MODEL[model_type]:\n",
        "            pattern_path = pattern.format(model=model, i=i)\n",
        "            matched = list(train_dir.glob(pattern_path))  # usa direttamente il pattern\n",
        "            if not matched:\n",
        "                missing.append(str(train_dir / pattern_path))\n",
        "\n",
        "# Stampa risultato\n",
        "print(\"📂 Verifica artefatti TRAINING\\n\")\n",
        "if not missing:\n",
        "    print(\"✅ Tutti i file richiesti sono presenti.\")\n",
        "else:\n",
        "    print(f\"❌ Mancano {len(missing)} artefatti TRAINING:\\n\")\n",
        "    for m in missing:\n",
        "        print(\" •\", m)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}