{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cOZCcsp0RCa",
        "outputId": "a6446687-530f-4b6e-f464-75aa35624b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "!pip install --quiet torch torchvision webdataset tqdm pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPkMc3ONzi0J",
        "outputId": "3670d8f8-1b6c-4869-cc30-6e12ffbc6e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî• PROJECT_ROOT: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import yaml\n",
        "import sys\n",
        "import time\n",
        "import importlib\n",
        "import logging\n",
        "import torch\n",
        "from typing import Any, Dict\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "\n",
        "config_path = Path('/content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/config/training.yaml')\n",
        "\n",
        "with config_path.open('r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "colab_root = Path(cfg['env_paths']['colab'])\n",
        "local_root = Path(cfg['env_paths']['local'])\n",
        "PROJECT_ROOT = colab_root if colab_root.exists() else local_root\n",
        "if not PROJECT_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Project root not find: {PROJECT_ROOT}\")\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
        "\n",
        "\n",
        "from importlib.util import spec_from_file_location, module_from_spec\n",
        "\n",
        "utils_dir = PROJECT_ROOT / 'src' / 'utils'\n",
        "src_file = utils_dir / 'training_utils.py'\n",
        "\n",
        "spec = spec_from_file_location('utils.training_utils', str(src_file))\n",
        "training_utils = module_from_spec(spec)\n",
        "spec.loader.exec_module(training_utils)\n",
        "\n",
        "sys.modules['utils.training_utils'] = training_utils\n",
        "\n",
        "from utils.training_utils import TRAINER_REGISTRY\n",
        "from trainers.extract_features import extract_features\n",
        "from trainers.train_classifier import train_classifier\n",
        "from utils.training_utils import get_latest_checkpoint, load_checkpoint\n",
        "print(f\"üî• PROJECT_ROOT: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIancWXKKuOY",
        "outputId": "63e8f3f2-f7de-40a0-8a44-db5ccb8dada1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Dataset paths:\n",
            "  ‚Ä¢ train: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed/webdataset_2500/train/patches-0000.tar\n",
            "  ‚Ä¢ val: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed/webdataset_2500/val/patches-0000.tar\n",
            "  ‚Ä¢ test: /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed/webdataset_2500/test/patches-0000.tar\n"
          ]
        }
      ],
      "source": [
        "for split in ['train','val','test']:\n",
        "    rel = cfg['data'].get(split)\n",
        "    if rel:\n",
        "        cfg['data'][split] = str(PROJECT_ROOT / rel)\n",
        "\n",
        "print(\"üìÇ Dataset paths:\")\n",
        "for split in ['train','val','test']:\n",
        "    print(f\"  ‚Ä¢ {split}: {cfg['data'][split]}\")\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / 'src'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8UIj4XOZP6U9"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)-8s | %(name)s: %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "logger = logging.getLogger(\"LAUNCHER\")\n",
        "logger.info(\"‚úÖ Logger initialized at INFO level\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jwx0_Pr9E59j"
      },
      "outputs": [],
      "source": [
        "trainer_modules = [\n",
        "    \"trainers.simclr\",\n",
        "    \"trainers.moco_v2\",\n",
        "    \"trainers.rotation\",\n",
        "    \"trainers.jigsaw\",\n",
        "    \"trainers.supervised\",\n",
        "    \"trainers.transfer\",\n",
        "]\n",
        "for module_name in trainer_modules:\n",
        "    if module_name in sys.modules:\n",
        "        importlib.reload(sys.modules[module_name])\n",
        "    else:\n",
        "        importlib.import_module(module_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rMV3BYIyE7FM"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import time\n",
        "\n",
        "def launch_training(cfg: dict) -> None:\n",
        "    models_cfg = cfg.get('models', {})\n",
        "    run_model = cfg.get('run_model', 'all').lower()\n",
        "    if run_model == 'all':\n",
        "        tasks = list(models_cfg.items())\n",
        "    else:\n",
        "        if run_model not in models_cfg:\n",
        "            raise KeyError(f\"Model '{run_model}' not found in cfg['models']\")\n",
        "        tasks = [(run_model, models_cfg[run_model])]\n",
        "\n",
        "    for name, m_cfg in tasks:\n",
        "        if name not in TRAINER_REGISTRY:\n",
        "            raise KeyError(f\"No trainer registered for '{name}'\")\n",
        "\n",
        "        trainer = TRAINER_REGISTRY[name](m_cfg, cfg['data'])\n",
        "        device = getattr(trainer, 'device', 'n/a')\n",
        "        epochs = int(m_cfg['training'].get('epochs', 0))\n",
        "        batch_size = int(m_cfg['training'].get('batch_size', 0))\n",
        "\n",
        "        print(f\"Device: {device} üöÄ  Starting training for model '{name}'\")\n",
        "        print(f\"‚Üí Model config: {m_cfg}\")\n",
        "        print(f\"Epochs: {epochs} | Batch size: {batch_size}\\n\")\n",
        "\n",
        "        has_validation = hasattr(trainer, 'validate_epoch')\n",
        "        experiment_id =  \"20250624222813\"\n",
        "        experiment_dir = PROJECT_ROOT / f\"data/processed2/dataset_9f30917e/experiments/{experiment_id}/{name}\"\n",
        "        ckpt_path_list = sorted(experiment_dir.glob(f\"{trainer.__class__.__name__}_best_epoch*.pt\"))\n",
        "        ckpt_path = ckpt_path_list[-1] if ckpt_path_list else None\n",
        "\n",
        "\n",
        "        skip_training = False\n",
        "        if ckpt_path and ckpt_path.exists():\n",
        "            print(f\"‚è≠Ô∏è  Checkpoint found for '{name}' ‚Üí skipping training and loading encoder/projector/model.\")\n",
        "            if hasattr(trainer, \"encoder\") and hasattr(trainer, \"projector\"):\n",
        "                model = torch.nn.Sequential(trainer.encoder, trainer.projector)\n",
        "                load_checkpoint(ckpt_path, model=model)\n",
        "                trainer.encoder = model[0].to(trainer.device)\n",
        "                trainer.projector = model[1].to(trainer.device)\n",
        "            elif hasattr(trainer, \"model\"):\n",
        "                load_checkpoint(ckpt_path, model=trainer.model)\n",
        "                trainer.model = trainer.model.to(trainer.device)\n",
        "            else:\n",
        "                raise AttributeError(f\"‚ùå Trainer '{name}' has no encoder/projector or model to load into.\")\n",
        "            skip_training = True\n",
        "\n",
        "\n",
        "        if not skip_training:\n",
        "            for epoch in range(1, epochs + 1):\n",
        "                epoch_start = time.time()\n",
        "                total_batches = getattr(trainer, 'batches_train', None)\n",
        "                print(f\"TOTAL BATCHES {total_batches}\")\n",
        "                running_loss, running_correct, total_samples = 0.0, 0, 0\n",
        "\n",
        "                print(f\"--- Epoch {epoch}/{epochs} ---\")\n",
        "                for i, batch in enumerate(trainer.train_loader, start=1):\n",
        "                    sig = inspect.signature(trainer.train_step)\n",
        "                    result = trainer.train_step(batch) if len(sig.parameters) == 1 else trainer.train_step(*batch)\n",
        "                    if len(result) == 4:\n",
        "                        _, loss, correct, bs = result\n",
        "                    else:\n",
        "                        loss, bs = result\n",
        "                        correct = 0\n",
        "\n",
        "                    running_loss += loss * bs\n",
        "                    running_correct += correct\n",
        "                    total_samples += bs\n",
        "                    avg_loss = running_loss / total_samples\n",
        "                    avg_acc = (running_correct / total_samples) if has_validation else 0.0\n",
        "                    elapsed = time.time() - epoch_start\n",
        "                    pct = (i / total_batches) * 100 if total_batches else 0.0\n",
        "                    eta = (elapsed / i) * (total_batches - i) if total_batches else 0.0\n",
        "\n",
        "                    msg = f\"  Batch {i}/{total_batches} ({pct:.1f}%) | Loss: {avg_loss:.4f}\"\n",
        "                    if has_validation:\n",
        "                        msg += f\" | Acc: {avg_acc:.3f}\"\n",
        "                    msg += f\" | Elapsed: {elapsed:.1f}s | ETA: {eta:.1f}s\"\n",
        "                    print(msg)\n",
        "\n",
        "                if has_validation:\n",
        "                    val_loss, val_acc = trainer.validate_epoch()\n",
        "                    print(f\"Val -> Loss: {val_loss:.4f} | Acc: {val_acc:.3f}\")\n",
        "                    trainer.post_epoch(epoch, val_acc)\n",
        "                else:\n",
        "                    epoch_loss = running_loss / total_samples\n",
        "                    trainer.post_epoch(epoch, epoch_loss)\n",
        "\n",
        "                print(f\"Epoch {epoch} completed in {time.time() - epoch_start:.1f}s\\n\")\n",
        "\n",
        "        best = trainer.summary()\n",
        "        if isinstance(best, tuple) and len(best) == 2:\n",
        "            be, bm = best\n",
        "            print(f\"‚úÖ Training for '{name}' completed. Best @ epoch {be} -> {bm:.3f}\")\n",
        "\n",
        "        # Only for self-supervised\n",
        "        if not has_validation:\n",
        "            print(f\"üîç Extracting features from model '{name}'\")\n",
        "            feature_path = experiment_dir / f\"{name}_features.pt\"\n",
        "            classifier_path = experiment_dir / f\"{name}_classifier.joblib\"\n",
        "            feature_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            if hasattr(trainer, \"extract_features_to\"):\n",
        "                trainer.extract_features_to(str(feature_path))\n",
        "                print(f\"‚úÖ Features saved to {feature_path}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Trainer '{name}' does not implement extract_features_to(), skipping.\")\n",
        "\n",
        "            print(f\"üß† Training classifier on features '{name}'\")\n",
        "            train_classifier(\n",
        "                features_path=str(feature_path),\n",
        "                output_model=str(classifier_path)\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFDhQTgUOwYJ",
        "outputId": "bcfea706-83a0-4951-fc53-e81cf8253b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda üöÄ  Starting training for model 'jigsaw'\n",
            "‚Üí Model config: {'backbone': 'resnet18', 'grid_size': 3, 'training': {'epochs': 2, 'batch_size': 128, 'optimizer': 'adam', 'learning_rate': '1e-4', 'weight_decay': '1e-5'}}\n",
            "Epochs: 2 | Batch size: 128\n",
            "\n",
            "‚è≠Ô∏è  Checkpoint found for 'jigsaw' ‚Üí skipping training and loading encoder/projector/model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
            "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training for 'jigsaw' completed. Best @ epoch 0 -> inf\n",
            "üîç Extracting features from model 'jigsaw'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 12it [00:04,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Jigsaw features saved to /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed2/dataset_9f30917e/experiments/20250624222813/jigsaw/jigsaw_features.pt\n",
            "‚úÖ Features saved to /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed2/dataset_9f30917e/experiments/20250624222813/jigsaw/jigsaw_features.pt\n",
            "üß† Training classifier on features 'jigsaw'\n",
            "‚úÖ Loaded 1475 keys and (1475, 512) features\n",
            "üìä Class distribution:\n",
            "Counter({np.str_('not_tumor'): 299, np.str_('ONCO'): 297, np.str_('CHROMO'): 293, np.str_('ccRCC'): 293, np.str_('pRCC'): 293})\n",
            "‚úÖ Filtered dataset: 1475 samples\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      CHROMO       0.46      0.10      0.17        58\n",
            "        ONCO       0.33      0.53      0.41        59\n",
            "       ccRCC       0.24      0.08      0.12        59\n",
            "   not_tumor       0.32      0.72      0.44        60\n",
            "        pRCC       0.52      0.27      0.36        59\n",
            "\n",
            "    accuracy                           0.34       295\n",
            "   macro avg       0.37      0.34      0.30       295\n",
            "weighted avg       0.37      0.34      0.30       295\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 6 39  2 10  1]\n",
            " [ 5 31  2 19  2]\n",
            " [ 1 12  5 33  8]\n",
            " [ 1  9  3 43  4]\n",
            " [ 0  3  9 31 16]]\n",
            "üíæ Classifier saved to /content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/data/processed2/dataset_9f30917e/experiments/20250624222813/jigsaw/jigsaw_classifier.joblib\n"
          ]
        }
      ],
      "source": [
        "launch_training(cfg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}