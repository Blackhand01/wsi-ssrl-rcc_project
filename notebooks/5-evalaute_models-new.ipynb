{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6700a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/notebooks/config/training.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m     13\u001b[0m CONFIG_PATH \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/config/training.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m IN_COLAB\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mcwd() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m CFG \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(\u001b[43mCONFIG_PATH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Define PROJECT_ROOT based on current environment\u001b[39;00m\n\u001b[1;32m     21\u001b[0m colab_root \u001b[38;5;241m=\u001b[39m Path(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_paths\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolab\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/pathlib.py:1134\u001b[0m, in \u001b[0;36mPath.read_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/wsi-ssrl/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project/notebooks/config/training.yaml'"
     ]
    }
   ],
   "source": [
    "# Cell 1 – Environment Setup & Dependency Management (Colab & VSCode compatible)\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Google Colab environment\n",
    "IN_COLAB = Path(\"/content\").exists()\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive                          # type: ignore\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "# Load YAML config from correct path\n",
    "import yaml\n",
    "CONFIG_PATH = (\n",
    "    Path(\"/content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/config/training.yaml\")\n",
    "    if IN_COLAB\n",
    "    else Path.cwd() / \"config\" / \"training.yaml\"\n",
    ")\n",
    "CFG = yaml.safe_load(CONFIG_PATH.read_text())\n",
    "\n",
    "# Define PROJECT_ROOT based on current environment\n",
    "colab_root = Path(CFG[\"env_paths\"][\"colab\"])\n",
    "local_root = Path(CFG[\"env_paths\"][\"local\"])\n",
    "PROJECT_ROOT = colab_root if colab_root.exists() else local_root\n",
    "\n",
    "# Add `src/` directory to Python path\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "# Install missing Python dependencies (only when needed)\n",
    "def install_if_missing(packages):\n",
    "    import importlib.util\n",
    "    missing = [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "    if missing:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n",
    "\n",
    "install_if_missing([\n",
    "    \"torch\", \"torchvision\", \"webdataset\", \"tqdm\",\n",
    "    \"pillow\", \"scikit-learn\", \"joblib\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 – Path normalization and debug print (Colab & VSCode)\n",
    "from pathlib import Path\n",
    "\n",
    "# Normalize relative dataset paths using PROJECT_ROOT\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    rel_path = CFG[\"data\"].get(split)\n",
    "    if rel_path:\n",
    "        CFG[\"data\"][split] = str(PROJECT_ROOT / rel_path)\n",
    "\n",
    "# Extract dataset ID and model output directory\n",
    "DATASET_ID = CFG[\"data\"][\"dataset_id\"]\n",
    "MODELS_DIR = (PROJECT_ROOT / CFG[\"output_dir\"].format(dataset_id=DATASET_ID)).resolve()\n",
    "\n",
    "# Debug info (print only)\n",
    "print(\"📁 Project root   :\", PROJECT_ROOT)\n",
    "print(\"📦 Dataset ID     :\", DATASET_ID)\n",
    "print(\"💾 Models dir     :\", MODELS_DIR)\n",
    "print(\"🧪 Normalized paths:\")\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    print(f\"   • {split}: {CFG['data'].get(split)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 – Dynamic import of training_utils and trainer modules\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Load utils/training_utils.py dynamically\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\" / \"training_utils.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"utils.training_utils\", str(utils_path))\n",
    "training_utils = importlib.util.module_from_spec(spec)              # type: ignore[arg-type]\n",
    "assert spec and spec.loader\n",
    "spec.loader.exec_module(training_utils)                             # type: ignore[assignment]\n",
    "sys.modules[\"utils.training_utils\"] = training_utils\n",
    "\n",
    "# Import core functions from training_utils\n",
    "from utils.training_utils import TRAINER_REGISTRY, load_checkpoint, get_latest_checkpoint\n",
    "\n",
    "# Import all trainer modules (SimCLR, MoCo, etc.)\n",
    "trainer_modules = [\n",
    "    \"trainers.simclr\",\n",
    "    \"trainers.moco_v2\",\n",
    "    \"trainers.rotation\",\n",
    "    \"trainers.jigsaw\",\n",
    "    \"trainers.supervised\",\n",
    "    \"trainers.transfer\",\n",
    "]\n",
    "for module_name in trainer_modules:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "    else:\n",
    "        importlib.import_module(module_name)\n",
    "\n",
    "print(\"✅ training_utils and trainer modules loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 – Evaluation helpers and majority voting utilities\n",
    "import numpy as np\n",
    "import torch, joblib, webdataset as wds\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- Utility: Extract patient ID and label from key ------------------------ #\n",
    "def extract_patient_id(key: str) -> str:\n",
    "    for part in key.split(\"_\"):\n",
    "        if part.startswith((\"HP\", \"H\")):\n",
    "            return part\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def extract_label_from_key(key: str) -> str:\n",
    "    return \"not_tumor\" if key.startswith(\"not_tumor\") else key.split(\"_\")[0]\n",
    "\n",
    "# --- Utility: Load test data from WebDataset ------------------------------- #\n",
    "def make_loader(wds_path: str, batch_size: int = 64):\n",
    "    dataset = (\n",
    "        wds.WebDataset(wds_path, shardshuffle=False, handler=wds.warn_and_continue, empty_check=False)\n",
    "        .decode(\"pil\")\n",
    "        .map(lambda sample: {\n",
    "            \"img\": T.ToTensor()(\n",
    "                next((v for k, v in sample.items() if isinstance(v, Image.Image)), None).convert(\"RGB\")),\n",
    "            \"key\": sample[\"__key__\"] + \".\" + next((k for k in sample.keys() if k.endswith(\".jpg\")), \"\")\n",
    "        })\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                                       num_workers=0, pin_memory=True)\n",
    "\n",
    "# --- Utility: Patient-level majority voting ------------------------------- #\n",
    "def majority_vote(keys: list[str], preds: list[int], label_encoder, exclude_label: str = \"not_tumor\"):\n",
    "    votes_by_patient = defaultdict(list)\n",
    "    labels_by_patient = defaultdict(list)\n",
    "\n",
    "    for k, p in zip(keys, preds):\n",
    "        pid = extract_patient_id(k)\n",
    "        label = extract_label_from_key(k)\n",
    "        if label_encoder.classes_[p] != exclude_label:\n",
    "            votes_by_patient[pid].append(p)\n",
    "        labels_by_patient[pid].append(label)\n",
    "\n",
    "    y_true, y_pred, valid_pids = [], [], []\n",
    "    for pid, vote_list in votes_by_patient.items():\n",
    "        gt_labels = [lab for lab in labels_by_patient[pid] if lab != exclude_label]\n",
    "        if len(set(gt_labels)) != 1 or not vote_list:\n",
    "            continue\n",
    "        gt_index = label_encoder.transform([gt_labels[0]])[0]\n",
    "        majority = Counter(vote_list).most_common(1)[0][0]\n",
    "        y_true.append(gt_index)\n",
    "        y_pred.append(majority)\n",
    "        valid_pids.append(pid)\n",
    "\n",
    "    return y_true, y_pred, valid_pids\n",
    "\n",
    "# --- Evaluation for Self-Supervised models ------------------------------- #\n",
    "def evaluate_selfsupervised(trainer, classifier_path: str, test_path: str):\n",
    "    print(f\"\\n🧪 Evaluating Self-Supervised model: {classifier_path}\")\n",
    "    model_bundle = joblib.load(classifier_path)\n",
    "    clf = model_bundle[\"model\"]\n",
    "    le  = model_bundle[\"label_encoder\"]\n",
    "\n",
    "    from trainers.extract_features import extract_features\n",
    "    loader = make_loader(test_path)\n",
    "    feats = extract_features(trainer.encoder, loader, trainer.device)\n",
    "\n",
    "    X     = feats[\"features\"].cpu().numpy()\n",
    "    keys  = feats[\"keys\"]\n",
    "    preds = clf.predict(X)\n",
    "\n",
    "    y_true, y_pred, patient_ids = majority_vote(keys, preds, le)\n",
    "\n",
    "    if not y_true:\n",
    "        print(\"⚠️ No evaluable patients (Self-Supervised)\")\n",
    "        return\n",
    "\n",
    "    _print_report(y_true, y_pred, le, patient_ids)\n",
    "\n",
    "# --- Evaluation for Supervised and Transfer models ------------------------ #\n",
    "def evaluate_supervised(trainer, test_path: str):\n",
    "    print(f\"\\n🧪 Evaluating Supervised/Transfer model...\")\n",
    "    loader = make_loader(test_path)\n",
    "    model = trainer.model.to(trainer.device).eval()\n",
    "    le = trainer.label_encoder\n",
    "\n",
    "    preds, keys = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            logits = model(batch[\"img\"].to(trainer.device))\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            keys.extend(batch[\"key\"])\n",
    "\n",
    "    y_true, y_pred, patient_ids = majority_vote(keys, preds, le)\n",
    "\n",
    "    if not y_true:\n",
    "        print(\"⚠️ No evaluable patients (Supervised/Transfer)\")\n",
    "        return\n",
    "\n",
    "    _print_report(y_true, y_pred, le, patient_ids)\n",
    "\n",
    "# --- Report printing ------------------------------------------------------ #\n",
    "def _print_report(y_true, y_pred, label_encoder, patient_ids):\n",
    "    classes = [c for c in label_encoder.classes_ if c != \"not_tumor\"]\n",
    "    print(\"\\n📊 Classification report (majority voting per patient):\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes))\n",
    "    print(\"📉 Confusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\"✅ Total evaluated patients: {len(y_true)}\")\n",
    "\n",
    "    print(\"\\n🧾 Per-patient results:\")\n",
    "    for pid, t, p in zip(patient_ids, y_true, y_pred):\n",
    "        true_label = label_encoder.inverse_transform([t])[0]\n",
    "        pred_label = label_encoder.inverse_transform([p])[0]\n",
    "        print(f\"• Patient {pid}: predicted = {pred_label} | true = {true_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 – Evaluation loop for all models\n",
    "run_model = CFG.get(\"run_model\", \"all\").lower()\n",
    "models_cfg = CFG[\"models\"]\n",
    "tasks = models_cfg.items() if run_model == \"all\" else [(run_model, models_cfg[run_model])]\n",
    "\n",
    "test_path = CFG[\"data\"][\"test\"]\n",
    "\n",
    "for name, model_cfg in tasks:\n",
    "    print(f\"\\n🔍 Evaluating model: {name}\")\n",
    "\n",
    "    if name not in TRAINER_REGISTRY:\n",
    "        raise KeyError(f\"❌ Trainer '{name}' is not registered.\")\n",
    "\n",
    "    trainer = TRAINER_REGISTRY[name](model_cfg, CFG[\"data\"])\n",
    "    ckpt_dir = MODELS_DIR / name / \"checkpoints\"\n",
    "    ckpt = get_latest_checkpoint(ckpt_dir, prefix=trainer.__class__.__name__)\n",
    "    if ckpt is None:\n",
    "        print(f\"⚠️ No checkpoint found for '{name}', skipping evaluation.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"📥 Loading checkpoint: {ckpt.name}\")\n",
    "    \n",
    "    if name in (\"supervised\", \"transfer\"):\n",
    "        # Load full supervised/transfer model\n",
    "        load_checkpoint(ckpt, model=trainer.model)\n",
    "        trainer.model = trainer.model.to(trainer.device)\n",
    "        evaluate_supervised(trainer, test_path)\n",
    "    else:\n",
    "        # Load encoder + projector + external classifier\n",
    "        clf_path = MODELS_DIR / name / \"classifier\" / f\"{name}_classifier.joblib\"\n",
    "        if not clf_path.exists():\n",
    "            print(f\"⚠️ No classifier found for '{name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        model = torch.nn.Sequential(trainer.encoder, trainer.projector)\n",
    "        load_checkpoint(ckpt, model=model)\n",
    "        trainer.encoder = model[0].to(trainer.device)\n",
    "        trainer.projector = model[1].to(trainer.device)\n",
    "        evaluate_selfsupervised(trainer, clf_path, test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70169676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 – Check saved artifacts for each evaluated model\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n📂 Artifacts summary:\")\n",
    "\n",
    "for name, _ in tasks:\n",
    "    model_dir = MODELS_DIR / name\n",
    "    if not model_dir.exists():\n",
    "        continue\n",
    "    print(f\"\\n🔎 Model: {name}\")\n",
    "    for sub in [\"checkpoints\", \"features\", \"classifier\"]:\n",
    "        subdir = model_dir / sub\n",
    "        if subdir.exists():\n",
    "            files = list(subdir.glob(\"*\"))\n",
    "            if files:\n",
    "                print(f\"  📁 {sub}/\")\n",
    "                for f in files:\n",
    "                    print(f\"     - {f.name}\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ {sub}/ is empty.\")\n",
    "        else:\n",
    "            print(f\"  ❌ Missing subdirectory: {sub}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsi-ssrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
