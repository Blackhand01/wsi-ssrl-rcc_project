{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"18aVreXE_S816USEyxxUnhWV1s1hss58Y","authorship_tag":"ABX9TyNCzeCNoGRC62dLCirmtE/J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fbjhtw6U8RIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753293892181,"user_tz":-120,"elapsed":119435,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"e7da341a-276f-45e2-b809-da83a1e1ccf2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XnLTyrxFrF3N","executionInfo":{"status":"ok","timestamp":1753294006104,"user_tz":-120,"elapsed":113932,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fe88d6c-72a4-4aa8-d652-caae8c8180ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Cell 1 – Install & Imports\n","!pip install --quiet torch torchvision webdataset tqdm pillow scikit-learn joblib matplotlib seaborn pyyaml\n","\n","import os, sys, json, yaml, joblib\n","from pathlib import Path\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# set matplotlib style\n","plt.rcParams.update({\"figure.max_open_warning\": 0})\n"]},{"cell_type":"code","source":["# Cell 2 – Load Configuration & Paths (robust version)\n","import os\n","import yaml\n","from pathlib import Path\n","\n","# Define default root paths\n","DEFAULT_ENV_PATHS = {\n","    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n","    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n","}\n","\n","# Determine environment\n","IN_COLAB = Path(\"/content\").exists()\n","PROJECT_ROOT = Path(\n","    os.getenv(\"PROJECT_ROOT\", DEFAULT_ENV_PATHS[\"colab\" if IN_COLAB else \"local\"])\n",").resolve()\n","\n","# Path to YAML config (always in config/)\n","cfg_path = PROJECT_ROOT / \"config\" / \"training.yaml\"\n","\n","# Check config file\n","if not cfg_path.exists():\n","    raise FileNotFoundError(f\"❌ training.yaml not found at: {cfg_path}\")\n","\n","# Load YAML config\n","with cfg_path.open() as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Extract config values\n","EXP_CODE   = \"20250723141202\" #cfg.get(\"exp_code\") or os.getenv(\"EXP_CODE\") or \"missing_code\"\n","DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n","\n","# Build central experiment path\n","EXP_DIR = PROJECT_ROOT / cfg[\"output\"][\"exp_dir\"].format(\n","    dataset_id=DATASET_ID, exp_code=EXP_CODE\n",")\n","\n","# Ovverride general training yaml file\n","cfg_path = EXP_DIR / f\"training_{EXP_CODE}.yaml\"\n","# Check config file\n","if not cfg_path.exists():\n","    raise FileNotFoundError(f\"❌ training.yaml not found at: {cfg_path}\")\n","\n","# Load YAML config\n","with cfg_path.open() as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Display\n","print(f\"📁 PROJECT_ROOT → {PROJECT_ROOT}\")\n","print(f\"📄 YAML loaded  → {cfg_path.name}\")\n","print(f\"🔑 EXP_CODE     → {EXP_CODE}\")\n","print(f\"📂 EXP_DIR      → {EXP_DIR}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ovj5W--3rN4J","executionInfo":{"status":"ok","timestamp":1753294011490,"user_tz":-120,"elapsed":5377,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"7530446d-02c8-441c-87c9-67566641b68e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["📁 PROJECT_ROOT → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\n","📄 YAML loaded  → training_20250723141202.yaml\n","🔑 EXP_CODE     → 20250723141202\n","📂 EXP_DIR      → /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723141202\n"]}]},{"cell_type":"code","source":["# Cell 3 – Extend PYTHONPATH & Import Trainers + Utils\n","\n","import sys\n","import importlib\n","from pathlib import Path\n","\n","# Extend PYTHONPATH to include src/\n","SRC_DIR = PROJECT_ROOT / \"src\"\n","sys.path[:0] = [str(PROJECT_ROOT), str(SRC_DIR)]\n","\n","# Import training utils from utils.training_utils\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.device_io import (\n","    choose_device,\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json,\n","    save_joblib,\n",")\n","from utils.training_utils.data_utils import (\n","    build_loader,\n","    load_classifier,\n","    parse_label_from_filename,\n",")\n","from utils.training_utils.model_utils import mc_dropout_predictions\n","from utils.training_utils.metrics import (\n","    compute_classification_metrics,\n","    aggregate_fold_metrics,\n","    expected_calibration_error,\n",")\n","\n","# Import trainer modules dynamically to ensure registration\n","trainer_names = [\"simclr\", \"moco_v2\", \"rotation\", \"jepa\", \"supervised\", \"transfer\"]\n","for name in trainer_names:\n","    try:\n","        importlib.import_module(f\"trainers.{name}\")\n","        print(f\"✅ Imported trainer module: {name}\")\n","    except ImportError as e:\n","        print(f\"❌ Failed to import trainer {name}: {e}\")\n","\n","# Verify registration\n","for name in trainer_names:\n","    assert name in TRAINER_REGISTRY, f\"❌ Missing trainer in registry: {name}\"\n","print(\"📚 All trainers successfully registered.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"047G_s2BrPQc","executionInfo":{"status":"ok","timestamp":1753294035350,"user_tz":-120,"elapsed":23852,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"a1376d7d-b0ec-4b3e-9c29-c2e04c666b85"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Imported trainer module: simclr\n","✅ Imported trainer module: moco_v2\n","✅ Imported trainer module: rotation\n","✅ Imported trainer module: jepa\n","✅ Imported trainer module: supervised\n","✅ Imported trainer module: transfer\n","📚 All trainers successfully registered.\n"]}]},{"cell_type":"code","source":["# Cell 4 – Evaluation Settings (debug-friendly)\n","\n","device = choose_device()\n","eval_cfg = cfg.get(\"evaluation\", {})\n","\n","# 🔧 Debug: riduci MC_PASSES ed ECE_BINS per velocizzare\n","MC_PASSES = min(int(eval_cfg.get(\"mc_dropout_passes\", 20)), 3)\n","ECE_BINS  = min(int(eval_cfg.get(\"ece_bins\", 15)), 5)\n","\n","GCAM_TOPK  = int(eval_cfg.get(\"gradcam\", {}).get(\"top_k\", 5))\n","GCAM_LAYER = eval_cfg.get(\"gradcam\", {}).get(\"layer\", None)\n","\n","print(f\"🖥️  Device:         {device}\")\n","print(f\"🔄  MC-dropout:     {MC_PASSES} passes\")\n","print(f\"📊  ECE bins:       {ECE_BINS}\")\n","print(f\"🔍  GradCAM++ top-k:{GCAM_TOPK}\")\n","print(f\"📐  GradCAM++ layer:{GCAM_LAYER}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mEfJ91DerQr1","executionInfo":{"status":"ok","timestamp":1753294035369,"user_tz":-120,"elapsed":15,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"494a5dce-9e35-4d25-a076-aca2fd9db163"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["🖥️  Device:         cpu\n","🔄  MC-dropout:     3 passes\n","📊  ECE bins:       5\n","🔍  GradCAM++ top-k:5\n","📐  GradCAM++ layer:layer4\n"]}]},{"cell_type":"code","source":["# Cell 5 – Helper Functions & Path Centralization\n","\n","from pathlib import Path\n","\n","def _paths(model_name: str, fold: int, patient_id: str = None) -> dict[str, Path]:\n","    \"\"\"\n","    Build and return all relevant paths for a given model/fold,\n","    directly from the patterns in cfg['output'], WITHOUT formatting {epoch}.\n","    \"\"\"\n","    # Base placeholders\n","    ph = {\n","        \"dataset_id\": DATASET_ID,\n","        \"exp_code\":   EXP_CODE,\n","        \"model_name\": model_name,\n","        \"fold_idx\":   fold,\n","        \"patient_id\": patient_id or \"{patient_id}\",\n","    }\n","\n","    # Experiment directories\n","    ph[\"exp_dir\"]       = cfg[\"output\"][\"exp_dir\"].format(**ph)\n","    ph[\"exp_model_dir\"] = cfg[\"output\"][\"exp_model_dir\"].format(**ph)\n","\n","    out: dict[str, Path] = {}\n","\n","    # ─── Training ────────────────────────────────────────────────────────\n","    out[\"ckpt_dir\"] = PROJECT_ROOT / ph[\"exp_model_dir\"] / f\"fold{fold}\" / \"training\"\n","\n","    t = cfg[\"output\"][\"training\"]\n","    out[\"features_train\"] = PROJECT_ROOT / t[\"features\"].format(**ph)\n","    out[\"clf\"]            = PROJECT_ROOT / t[\"clf\"].format(**ph)\n","    out[\"scaler\"]         = PROJECT_ROOT / t[\"scaler\"].format(**ph)\n","    out[\"loss_json\"]      = PROJECT_ROOT / t[\"loss_json\"].format(**ph)\n","    out[\"log\"]            = PROJECT_ROOT / t[\"log\"].format(**ph)\n","\n","    # ─── Inference ───────────────────────────────────────────────────────\n","    i = cfg[\"output\"][\"inference\"]\n","    out[\"patch_preds\"]   = PROJECT_ROOT / i[\"patch_preds\"].format(**ph)\n","    out[\"patient_preds\"] = PROJECT_ROOT / i[\"patient_preds\"].format(**ph)\n","    out[\"mc_logits\"]     = PROJECT_ROOT / i[\"mc_logits\"].format(**ph)\n","    out[\"metrics\"]       = PROJECT_ROOT / i[\"metrics\"].format(**ph)\n","\n","    # ─── Explainability ──────────────────────────────────────────────────\n","    e = cfg[\"output\"][\"explain\"]\n","    out[\"gradcam_dir\"]  = PROJECT_ROOT / e[\"gradcam_dir\"].format(**ph)\n","    out[\"metadata_csv\"] = PROJECT_ROOT / e[\"metadata_csv\"].format(**ph)\n","\n","    # ─── Aggregation ─────────────────────────────────────────────────────\n","    a = cfg[\"output\"][\"aggregate\"]\n","    out[\"agg_metrics\"] = PROJECT_ROOT / a[\"metrics\"].format(**ph)\n","    out[\"agg_summary\"] = PROJECT_ROOT / a[\"summary_img\"].format(**ph)\n","\n","    # ─── Experiment-Level ────────────────────────────────────────────────\n","    x = cfg[\"output\"][\"experiment_level\"]\n","    out[\"exp_json\"] = PROJECT_ROOT / x[\"comparison_json\"].format(**ph)\n","    out[\"exp_img\"]  = PROJECT_ROOT / x[\"comparison_img\"].format(**ph)\n","\n","    # ─── Ensure directories exist ────────────────────────────────────────\n","    for key, p in out.items():\n","        if \"dir\" in key:\n","            p.mkdir(parents=True, exist_ok=True)\n","        else:\n","            p.parent.mkdir(parents=True, exist_ok=True)\n","\n","    return out\n","\n","def _completed(paths: dict[str, Path], is_ssl: bool) -> bool:\n","    \"\"\"\n","    Returns True if all the necessary inference artifacts for this fold\n","    are already on disk, so we can skip evaluation.\n","    \"\"\"\n","    # Always require patch‐level preds + metrics JSON\n","    required = [\"patch_preds\", \"metrics\"]\n","    # For SSL models also require MC logits and patient‐level CSV\n","    if is_ssl:\n","        required += [\"mc_logits\", \"patient_preds\"]\n","    return all(paths[k].exists() for k in required)\n","\n","\n","def extract_patient_id(key: str) -> str:\n","    \"\"\"\n","    Extract patient ID from a key formatted like 'CLASS_HPxxxx_x_y'.\n","    \"\"\"\n","    parts = key.split(\"_\")\n","    return next((p for p in parts if p.startswith((\"HP\", \"H\"))), \"UNKNOWN\")\n","\n","\n","def ece(probs, labels):\n","    \"\"\"\n","    Expected Calibration Error (ECE) helper for quick access.\n","    \"\"\"\n","    return expected_calibration_error(probs, labels, n_bins=ECE_BINS)"],"metadata":{"id":"VkUXkgUyrR0e","executionInfo":{"status":"ok","timestamp":1753294035403,"user_tz":-120,"elapsed":25,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Cell 6a – Core Evaluation Functions\n","\n","import torch\n","import torch.nn as nn\n","import joblib\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from tqdm import tqdm\n","from collections import defaultdict, Counter\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    roc_auc_score,\n","    confusion_matrix,\n","    classification_report\n",")\n","\n","from utils.training_utils.metrics import (\n","    TemperatureScaler,\n","    expected_calibration_error,\n","    mc_dropout_statistics\n",")\n","from utils.training_utils.data_utils import load_classifier, parse_label_from_filename\n","from utils.training_utils.device_io import (\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json\n",")\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.model_utils import mc_dropout_predictions\n","\n","def _fix_fc_from_ckpt(model: torch.nn.Module, ckpt_path: Path) -> None:\n","    \"\"\"\n","    Replace model.fc so that out_features matches the checkpoint metadata.\n","\n","    Parameters\n","    ----------\n","    model : torch.nn.Module\n","        The backbone with a (possibly wrong) final fully-connected layer.\n","    ckpt_path : Path\n","        Path to the .pt checkpoint; must contain 'class_to_idx' metadata.\n","    \"\"\"\n","    ckpt_data = torch.load(ckpt_path, map_location=\"cpu\")\n","    class_to_idx = ckpt_data.get(\"class_to_idx\", {})\n","    n_classes = len(class_to_idx)\n","    if n_classes <= 0:\n","        raise ValueError(\"Checkpoint lacks valid 'class_to_idx' metadata\")\n","    if not hasattr(model, \"fc\"):\n","        raise AttributeError(\"Model has no .fc attribute to patch\")\n","    if model.fc.out_features != n_classes:\n","        in_f = model.fc.in_features\n","        model.fc = nn.Linear(in_f, n_classes)\n","        print(f\"🔧 Patched fc layer: in_features={in_f} → out_features={n_classes}\")\n","\n","def load_model_and_components(model_name: str, fold: int):\n","    \"\"\"\n","    Carica:\n","      - il modello PyTorch dal checkpoint\n","      - per SSL: il probe (clf + label encoder) e, se presente, il TemperatureScaler\n","\n","    Restituisce:\n","      model        : nn.Module      – backbone/encoder pronto per inferenza\n","      is_ssl       : bool           – True se modello SSL\n","      clf          : Any            – classificatore (probe) o None per SL\n","      le           : Any            – LabelEncoder per SSL, altrimenti None\n","      temp_scaler  : TemperatureScaler | None\n","    \"\"\"\n","    paths   = _paths(model_name, fold)\n","    cfg_m   = cfg[\"models\"][model_name]\n","    is_ssl  = cfg_m[\"type\"] == \"ssl\"\n","    trainer = TRAINER_REGISTRY[model_name](cfg_m, cfg[\"data\"])\n","\n","    # 1️⃣ Scegli checkpoint\n","    if is_ssl and cfg.get(\"train_encoder_once\", False) and fold > 0:\n","        ckpt = get_latest_checkpoint(_paths(model_name, 0)[\"ckpt_dir\"])\n","        if ckpt is None:\n","            raise FileNotFoundError(f\"❌ Nessun checkpoint trovato in fold0 per {model_name}\")\n","        print(f\"   ➔ SSL+train_encoder_once → encoder da fold0: {ckpt.name}\")\n","    else:\n","        ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","        if ckpt is None:\n","            raise FileNotFoundError(f\"❌ Nessun checkpoint trovato in fold{fold} per {model_name}\")\n","        print(f\"   ➔ Caricamento checkpoint fold{fold} → {ckpt.name}\")\n","\n","    # 2️⃣ Carica pesi nel modello\n","    if is_ssl:\n","        full_model, _ = trainer.get_resume_model_and_optimizer()\n","        load_checkpoint(ckpt, model=full_model)\n","        feat_mod = getattr(trainer, \"encoder\", None) or getattr(trainer, \"model\", None)\n","        if feat_mod is None:\n","            raise AttributeError(f\"No feature submodule found on {trainer}\")\n","        model = feat_mod.to(device).eval()\n","    else:\n","        # Patch dinamica del layer fc per SL/transfer\n","        _fix_fc_from_ckpt(trainer.model, ckpt)\n","        load_checkpoint(ckpt, model=trainer.model)\n","        model = trainer.model.to(device).eval()\n","\n","    # 3️⃣ Carica probe / classifier / calibratore\n","    clf = le = temp_scaler = None\n","    if is_ssl:\n","        # classificatore joblib salvato in training SSL\n","        clf, le = load_classifier(paths[\"clf\"])\n","        # eventualmente temperature scaling\n","        scaler_path = paths[\"scaler\"]\n","        if scaler_path.exists():\n","            obj = joblib.load(scaler_path)\n","            if isinstance(obj, TemperatureScaler):\n","                temp_scaler = obj\n","                print(\"   ➔ Loaded TemperatureScaler (calibrate probs)\")\n","        else:\n","            print(f\"   ➔ No TemperatureScaler for {model_name} fold {fold}\")\n","\n","    # 4️⃣ Debug printout\n","    print(f\"   ➔ SSL pipeline?  {is_ssl}\")\n","    print(f\"   ➔ Classifier?    {clf is not None}\")\n","    print(f\"   ➔ Temp-scaler?   {temp_scaler is not None}\")\n","\n","    return model, is_ssl, clf, le, temp_scaler\n"],"metadata":{"id":"t21HY__kszl_","executionInfo":{"status":"ok","timestamp":1753294035692,"user_tz":-120,"elapsed":216,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#  Cell 6b – Core Evaluation Functions\n","def run_patch_inference(model, loader, is_ssl, clf, temp_scaler):\n","    \"\"\"\n","    Inferenzia patch-level. Restituisce:\n","      keys (dummy), y_true, y_pred, probs (calibrated if temp_scaler)\n","    \"\"\"\n","    keys, y_true, y_pred = [], [], []\n","    probs_list = []\n","\n","    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Patches\")):\n","        imgs, labels = batch\n","        imgs = imgs.to(device)\n","\n","        if is_ssl:\n","            with torch.no_grad():\n","                feats = model(imgs).cpu().numpy()\n","            raw_p = clf.predict_proba(feats)\n","        else:\n","            with torch.no_grad():\n","                logits = model(imgs)\n","                raw_p = torch.softmax(logits, dim=1).cpu().numpy()\n","\n","        # calibration\n","        if temp_scaler is not None:\n","            logits_for_cal = np.log(raw_p + 1e-12)\n","            p = temp_scaler.transform_proba(logits_for_cal)\n","        else:\n","            p = raw_p\n","\n","        preds = p.argmax(axis=1)\n","        t     = labels.cpu().numpy()\n","\n","        y_true.extend(t.tolist())\n","        y_pred.extend(preds.tolist())\n","        probs_list.append(p)\n","\n","        print(f\"   • Batch {batch_idx} done.\")\n","\n","    probs = np.vstack(probs_list)\n","    return keys, np.array(y_true), np.array(y_pred), probs\n","\n","\n","def save_patch_outputs(model_name, fold, keys, y_true, y_pred, probs):\n","    torch.save({\n","        \"keys\": keys,\n","        \"true\": y_true,\n","        \"pred\": y_pred,\n","        \"probs\": probs\n","    }, _paths(model_name, fold)[\"patch_preds\"])\n","\n","\n","def save_mc_logits(model_name, fold, model, loader):\n","    mc = mc_dropout_predictions(model, loader, device=device, T=MC_PASSES)\n","    np.save(_paths(model_name, fold)[\"mc_logits\"], mc)\n","    return mc\n","\n","\n","def compute_and_save_metrics(model_name, fold, y_true, y_pred, probs, mc=None):\n","    \"\"\"\n","    Calcola tutte le metriche, MC-stats e ECE post-calibrazione.\n","    \"\"\"\n","    mc_stats = mc_dropout_statistics(mc) if mc is not None else {}\n","\n","    acc  = accuracy_score(y_true, y_pred)\n","    f1   = f1_score(y_true, y_pred, average=\"macro\")\n","    try:\n","        auc = roc_auc_score(y_true, y_pred, average=\"macro\", multi_class=\"ovo\")\n","    except ValueError:\n","        auc = None\n","    cm = confusion_matrix(y_true, y_pred).tolist()\n","    cr = classification_report(y_true, y_pred, output_dict=True)\n","\n","    mets = {\n","        \"accuracy\": acc,\n","        \"macro_f1\": f1,\n","        \"roc_auc\": auc,\n","        \"confusion_matrix\": cm,\n","        \"class_report\": cr,\n","        **mc_stats\n","    }\n","\n","    # ECE post-calibrazione\n","    mets[\"ece_post\"] = expected_calibration_error(probs, y_true, n_bins=ECE_BINS)\n","\n","    save_json(mets, _paths(model_name, fold)[\"metrics\"])\n","    return mets\n"],"metadata":{"id":"9PAs8FCDrTNz","executionInfo":{"status":"ok","timestamp":1753294035696,"user_tz":-120,"elapsed":190,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Cell 7 – Per-Fold Evaluation (aggiornato)\n","\n","from utils.training_utils.data_utils import default_transforms, build_loader, KNOWN_LABELS, parse_label_from_filename\n","from utils.training_utils.device_io import get_latest_checkpoint\n","from collections import defaultdict, Counter\n","import numpy as np\n","import pandas as pd\n","\n","def aggregate_patient_results(model_name, fold, keys, y_pred, probs):\n","    \"\"\"\n","    Aggrega predizioni per paziente (majority voting) e salva CSV.\n","    \"\"\"\n","    by_pt = defaultdict(list)\n","    for k, y, conf in zip(keys, y_pred, probs.max(axis=1)):\n","        pid = extract_patient_id(k)\n","        by_pt[pid].append((y, conf))\n","\n","    rows = []\n","    for pid, recs in by_pt.items():\n","        votes, confs = zip(*recs)\n","        rows.append({\n","            \"patient_id\": pid,\n","            \"true_label\": parse_label_from_filename(pid),\n","            \"pred_label\": Counter(votes).most_common(1)[0][0],\n","            \"n_patches\": len(recs),\n","            \"mean_conf_raw\": float(np.mean(confs))\n","        })\n","\n","    pd.DataFrame(rows).to_csv(\n","        _paths(model_name, fold)[\"patient_preds\"], index=False\n","    )\n","\n","def evaluate_fold(model_name: str, fold: int):\n","    print(f\"\\n🔍 Evaluating {model_name} fold {fold}…\")\n","\n","    # 0️⃣ Paths e checkpoint\n","    paths = _paths(model_name, fold)\n","    ckpt  = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","    if ckpt is None:\n","        print(f\"⚠️ Nessun checkpoint trovato per {model_name} fold {fold}, salto valutazione\")\n","        return\n","\n","    # 1️⃣ Skip se artefatti già presenti\n","    is_ssl = (cfg[\"models\"][model_name][\"type\"] == \"ssl\")\n","    if _completed(paths, is_ssl):\n","        print(f\"⚡ Skipping {model_name} fold {fold}: artifacts already present\")\n","        return\n","\n","    # 2️⃣ Carica modello, probe, scaler\n","    print(\"📥 Loading model, classifier, scaler…\")\n","    model, is_ssl, clf, le, temp_scaler = load_model_and_components(model_name, fold)\n","\n","    # 3️⃣ Build test loader\n","    patch_size = cfg[\"models\"][model_name].get(\"patch_size\", 224)\n","    batch_size = cfg[\"models\"][model_name][\"training\"][\"batch_size\"]\n","    test_rel   = cfg[\"data\"][\"test\"].format(\n","        fold_idx=fold,\n","        dataset_id=cfg[\"data\"][\"dataset_id\"]\n","    )\n","    test_wds   = (PROJECT_ROOT / test_rel).resolve()\n","    print(f\"🧪 Using test shard: {test_wds}\")\n","    assert test_wds.exists(), f\"❌ Test shard not found: {test_wds}\"\n","\n","    # ❗ mappatura globale delle classi\n","    all_classes  = sorted(KNOWN_LABELS)  # {\"ccRCC\",\"pRCC\",\"CHROMO\",\"ONCO\",\"not_tumor\"} :contentReference[oaicite:0]{index=0}\n","    class_to_idx = {cls: i for i, cls in enumerate(all_classes)}\n","\n","    loader = build_loader(\n","        str(test_wds),\n","        class_to_idx=class_to_idx,\n","        patch_size=patch_size,\n","        batch_size=batch_size,\n","        device=device,\n","        augment=False,\n","    )\n","    print(f\"📦 DataLoader ready with batch_size = {batch_size}\")\n","\n","    # 4️⃣ Patch-level inference\n","    print(\"▶️ Running inference (patch-level)…\")\n","    keys, y_true, y_pred, probs = run_patch_inference(\n","        model, loader, is_ssl, clf, temp_scaler\n","    )\n","    print(f\"   ➔ Patches processed: {len(y_true)}\")\n","    save_patch_outputs(model_name, fold, keys, y_true, y_pred, probs)\n","    print(\"💾 Saved patch-level outputs.\")\n","\n","    # 5️⃣ MC-Dropout (solo SSL)\n","    if is_ssl:\n","        print(\"🔄 Running MC-Dropout…\")\n","        mc = save_mc_logits(model_name, fold, model, loader)\n","        print(\"💾 Saved MC-Dropout logits.\")\n","    else:\n","        mc = None\n","\n","    # 6️⃣ Compute & save metrics\n","    print(\"📊 Computing metrics…\")\n","    compute_and_save_metrics(model_name, fold, y_true, y_pred, probs, mc)\n","    print(\"💾 Saved metrics JSON.\")\n","\n","    # 7️⃣ Patient-level aggregation (solo SSL)\n","    if is_ssl:\n","        print(\"👨‍⚕️ Aggregating patient-level results…\")\n","        aggregate_patient_results(model_name, fold, keys, y_pred, probs)\n","        print(\"💾 Saved patient-level CSV.\")\n","\n","    print(f\"✅ Done {model_name} fold {fold}\\n\")\n","\n","\n","# 🚀 Run evaluation for all models and folds\n","for model_name in cfg[\"run_models\"]:\n","    for fold_idx in cfg[\"folds\"]:\n","        evaluate_fold(model_name, fold_idx)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"t_oHDrpWrU3x","executionInfo":{"status":"error","timestamp":1753296300074,"user_tz":-120,"elapsed":2264496,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"f0d13e10-325a-4c06-8a5d-310ceaadde34"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","🔍 Evaluating rotation fold 0…\n","📥 Loading model, classifier, scaler…\n","   ➔ Caricamento checkpoint fold0 → RotationTrainer_bestepoch001.pt\n","   ➔ Loaded TemperatureScaler (calibrate probs)\n","   ➔ SSL pipeline?  True\n","   ➔ Classifier?    True\n","   ➔ Temp-scaler?   True\n","🧪 Using test shard: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test_holdout.tar\n","📦 DataLoader ready with batch_size = 64\n","▶️ Running inference (patch-level)…\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 1it [00:34, 34.10s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 0 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 2it [01:08, 34.13s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 1 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 3it [01:41, 33.63s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 2 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 4it [02:13, 32.98s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 3 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 5it [02:45, 32.81s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 4 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 6it [03:18, 32.78s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 5 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 7it [03:50, 32.57s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 6 done.\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 8it [04:10, 31.36s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 7 done.\n","   ➔ Patches processed: 487\n","💾 Saved patch-level outputs.\n","🔄 Running MC-Dropout…\n","🔁 MC-Dropout pass 1/3\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["    • Batch 1 / MC-pass 1\n","    • Batch 2 / MC-pass 1\n","    • Batch 3 / MC-pass 1\n","    • Batch 4 / MC-pass 1\n","    • Batch 5 / MC-pass 1\n","    • Batch 6 / MC-pass 1\n","    • Batch 7 / MC-pass 1\n","    • Batch 8 / MC-pass 1\n","🔁 MC-Dropout pass 2/3\n","    • Batch 1 / MC-pass 2\n","    • Batch 2 / MC-pass 2\n","    • Batch 3 / MC-pass 2\n","    • Batch 4 / MC-pass 2\n","    • Batch 5 / MC-pass 2\n","    • Batch 6 / MC-pass 2\n","    • Batch 7 / MC-pass 2\n","    • Batch 8 / MC-pass 2\n","🔁 MC-Dropout pass 3/3\n","    • Batch 1 / MC-pass 3\n","    • Batch 2 / MC-pass 3\n","    • Batch 3 / MC-pass 3\n","    • Batch 4 / MC-pass 3\n","    • Batch 5 / MC-pass 3\n","    • Batch 6 / MC-pass 3\n","    • Batch 7 / MC-pass 3\n","    • Batch 8 / MC-pass 3\n","✅ MC-Dropout completato: 3 pass, 487 patch per pass.\n","💾 Saved MC-Dropout logits.\n","📊 Computing metrics…\n","💾 Saved metrics JSON.\n","👨‍⚕️ Aggregating patient-level results…\n","💾 Saved patient-level CSV.\n","✅ Done rotation fold 0\n","\n","\n","🔍 Evaluating rotation fold 1…\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["⚠️ Nessun checkpoint trovato per rotation fold 1, salto valutazione\n","\n","🔍 Evaluating simclr fold 0…\n","📥 Loading model, classifier, scaler…\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"stream","name":"stdout","text":["   ➔ Caricamento checkpoint fold0 → SimCLRTrainer_bestepoch003.pt\n","   ➔ Loaded TemperatureScaler (calibrate probs)\n","   ➔ SSL pipeline?  True\n","   ➔ Classifier?    True\n","   ➔ Temp-scaler?   True\n","🧪 Using test shard: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test_holdout.tar\n","📦 DataLoader ready with batch_size = 64\n","▶️ Running inference (patch-level)…\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 1it [00:34, 34.92s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 0 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 2it [01:08, 33.95s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 1 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 3it [01:46, 36.10s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 2 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 4it [02:20, 35.12s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 3 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 5it [02:54, 34.71s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 4 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 6it [03:28, 34.54s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 5 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 7it [04:01, 33.97s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 6 done.\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 8it [04:22, 32.79s/it]"]},{"output_type":"stream","name":"stdout","text":["   • Batch 7 done.\n","   ➔ Patches processed: 487\n","💾 Saved patch-level outputs.\n","🔄 Running MC-Dropout…\n","🔁 MC-Dropout pass 1/3\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["    • Batch 1 / MC-pass 1\n","    • Batch 2 / MC-pass 1\n","    • Batch 3 / MC-pass 1\n","    • Batch 4 / MC-pass 1\n","    • Batch 5 / MC-pass 1\n","    • Batch 6 / MC-pass 1\n","    • Batch 7 / MC-pass 1\n","    • Batch 8 / MC-pass 1\n","🔁 MC-Dropout pass 2/3\n","    • Batch 1 / MC-pass 2\n","    • Batch 2 / MC-pass 2\n","    • Batch 3 / MC-pass 2\n","    • Batch 4 / MC-pass 2\n","    • Batch 5 / MC-pass 2\n","    • Batch 6 / MC-pass 2\n","    • Batch 7 / MC-pass 2\n","    • Batch 8 / MC-pass 2\n","🔁 MC-Dropout pass 3/3\n","    • Batch 1 / MC-pass 3\n","    • Batch 2 / MC-pass 3\n","    • Batch 3 / MC-pass 3\n","    • Batch 4 / MC-pass 3\n","    • Batch 5 / MC-pass 3\n","    • Batch 6 / MC-pass 3\n","    • Batch 7 / MC-pass 3\n","    • Batch 8 / MC-pass 3\n","✅ MC-Dropout completato: 3 pass, 487 patch per pass.\n","💾 Saved MC-Dropout logits.\n","📊 Computing metrics…\n","💾 Saved metrics JSON.\n","👨‍⚕️ Aggregating patient-level results…\n","💾 Saved patient-level CSV.\n","✅ Done simclr fold 0\n","\n","\n","🔍 Evaluating simclr fold 1…\n","⚠️ Nessun checkpoint trovato per simclr fold 1, salto valutazione\n","\n","🔍 Evaluating moco_v2 fold 0…\n","📥 Loading model, classifier, scaler…\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"stream","name":"stdout","text":["   ➔ Caricamento checkpoint fold0 → MoCoV2Trainer_bestepoch001.pt\n"]},{"output_type":"error","ename":"AttributeError","evalue":"No feature submodule found on <trainers.moco_v2.MoCoV2Trainer object at 0x7c8f8cbe9810>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-9-3033625619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"folds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mevaluate_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-9-3033625619.py\u001b[0m in \u001b[0;36mevaluate_fold\u001b[0;34m(model_name, fold)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# 2️⃣ Carica modello, probe, scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"📥 Loading model, classifier, scaler…\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# 3️⃣ Build test loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-7-3066692115.py\u001b[0m in \u001b[0;36mload_model_and_components\u001b[0;34m(model_name, fold)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mfeat_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeat_mod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No feature submodule found on {trainer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: No feature submodule found on <trainers.moco_v2.MoCoV2Trainer object at 0x7c8f8cbe9810>"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/\n"],"metadata":{"id":"BLBMOnmBzfsa","executionInfo":{"status":"aborted","timestamp":1753296301741,"user_tz":-120,"elapsed":2529236,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 8 – Fold-Level & Experiment-Level Aggregation (robust)\n","\n","import json\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","\n","from utils.training_utils.device_io import save_json\n","from utils.training_utils.metrics import aggregate_fold_metrics\n","\n","def aggregate_model(model_name: str):\n","    \"\"\"\n","    Raccoglie e aggrega le metriche per ciascun fold disponibile.\n","    Salta automaticamente i fold senza file di metrics.\n","    \"\"\"\n","    # 1️⃣ Raccogli le metriche per fold esistenti\n","    all_m = []\n","    available_folds = []\n","    for f in cfg[\"folds\"]:\n","        metrics_path = Path(_paths(model_name, f)[\"metrics\"])\n","        if metrics_path.exists():\n","            all_m.append(json.load(open(metrics_path)))\n","            available_folds.append(f)\n","        else:\n","            print(f\"⚠️ Nessuna metrica trovata per {model_name} fold {f}, skip\")\n","\n","    if not all_m:\n","        print(f\"❌ Nessuna metrica disponibile per modello '{model_name}', skip aggregation\")\n","        return\n","\n","    # 2️⃣ Estrai solo i campi numerici (int/float) dal primo fold valido\n","    numeric_keys = [k for k, v in all_m[0].items() if isinstance(v, (int, float))]\n","\n","    # 3️⃣ Costruisci lista di dict solo con quei campi, trasformando None→NaN\n","    numeric_per_fold = []\n","    for m in all_m:\n","        filtered = {k: (m.get(k) if isinstance(m.get(k), (int, float)) else np.nan)\n","                    for k in numeric_keys}\n","        numeric_per_fold.append(filtered)\n","\n","    # 4️⃣ Debug: segnala NaN trovati\n","    print(f\"\\n🔍 Debug NaN per modello '{model_name}':\")\n","    nan_found = False\n","    for idx, m in enumerate(numeric_per_fold):\n","        f = available_folds[idx]\n","        for k in numeric_keys:\n","            if pd.isna(m[k]):\n","                print(f\"  ⚠️ NaN in fold {f}, metrica '{k}'\")\n","                nan_found = True\n","    if not nan_found:\n","        print(\"  ✅ Nessun NaN rilevato nelle metriche numeriche.\")\n","\n","    # 5️⃣ Aggrega mean±std e salva JSON\n","    summary = aggregate_fold_metrics(numeric_per_fold)\n","    agg_metrics_path = Path(_paths(model_name, available_folds[0])[\"agg_metrics\"])\n","    save_json(summary, agg_metrics_path)\n","    print(f\"💾 Saved aggregated metrics → {agg_metrics_path}\")\n","\n","    # 6️⃣ Heatmap delle medie\n","    dfm = pd.DataFrame(numeric_per_fold, index=available_folds)\n","    fig, ax = plt.subplots(figsize=(6, 4))\n","    sns.heatmap(\n","        dfm.mean()[numeric_keys].to_frame().T,\n","        annot=True, fmt=\".3f\", ax=ax\n","    )\n","    fig.savefig(Path(_paths(model_name, available_folds[0])[\"agg_summary\"]), bbox_inches=\"tight\")\n","    plt.close(fig)\n","    print(f\"✅ Aggregated {model_name}\")\n","\n","def aggregate_experiment():\n","    \"\"\"\n","    Costruisce il riepilogo tra modelli, include solo i modelli con JSON agg esistenti.\n","    \"\"\"\n","    rows = []\n","    for m in cfg[\"run_models\"]:\n","        exp_json_path = Path(_paths(m, cfg[\"folds\"][0])[\"agg_metrics\"])\n","        if exp_json_path.exists():\n","            met = json.load(open(exp_json_path))\n","            row = {\"model\": m}\n","            for k, v in met.items():\n","                row[k] = v[\"mean\"]\n","            rows.append(row)\n","        else:\n","            print(f\"⚠️ Saltato modello '{m}' in experiment-level perché manca agg_metrics\")\n","\n","    if not rows:\n","        print(\"❌ Nessun modello ha agg_metrics, skip experiment aggregation\")\n","        return\n","\n","    # 1️⃣ Salva JSON di esperimento\n","    exp_json_path = Path(_paths(cfg[\"run_models\"][0], cfg[\"folds\"][0])[\"exp_json\"])\n","    save_json(rows, exp_json_path)\n","    print(f\"💾 Saved experiment summary JSON → {exp_json_path}\")\n","\n","    # 2️⃣ Barplot accuracy\n","    df = pd.DataFrame(rows)\n","    fig, ax = plt.subplots(figsize=(8, 4))\n","    sns.barplot(data=df, x=\"model\", y=\"accuracy\", ax=ax)\n","    ax.set_title(\"Model Accuracy Comparison\")\n","    fig.savefig(Path(_paths(cfg[\"run_models\"][0], cfg[\"folds\"][0])[\"exp_img\"]), bbox_inches=\"tight\")\n","    plt.close(fig)\n","    print(\"✅ Experiment-level comparison complete\")\n","\n","# 🚀 Esegui aggregazioni\n","for m in cfg[\"run_models\"]:\n","    aggregate_model(m)\n","aggregate_experiment()\n"],"metadata":{"id":"9l1_lvR3rcQ8","executionInfo":{"status":"aborted","timestamp":1753296302394,"user_tz":-120,"elapsed":2529886,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]}]}