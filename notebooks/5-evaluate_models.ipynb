{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"18aVreXE_S816USEyxxUnhWV1s1hss58Y","authorship_tag":"ABX9TyNCzeCNoGRC62dLCirmtE/J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fbjhtw6U8RIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753293892181,"user_tz":-120,"elapsed":119435,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"e7da341a-276f-45e2-b809-da83a1e1ccf2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XnLTyrxFrF3N","executionInfo":{"status":"ok","timestamp":1753294006104,"user_tz":-120,"elapsed":113932,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fe88d6c-72a4-4aa8-d652-caae8c8180ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Cell 1 ‚Äì Install & Imports\n","!pip install --quiet torch torchvision webdataset tqdm pillow scikit-learn joblib matplotlib seaborn pyyaml\n","\n","import os, sys, json, yaml, joblib\n","from pathlib import Path\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# set matplotlib style\n","plt.rcParams.update({\"figure.max_open_warning\": 0})\n"]},{"cell_type":"code","source":["# Cell 2 ‚Äì Load Configuration & Paths (robust version)\n","import os\n","import yaml\n","from pathlib import Path\n","\n","# Define default root paths\n","DEFAULT_ENV_PATHS = {\n","    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n","    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n","}\n","\n","# Determine environment\n","IN_COLAB = Path(\"/content\").exists()\n","PROJECT_ROOT = Path(\n","    os.getenv(\"PROJECT_ROOT\", DEFAULT_ENV_PATHS[\"colab\" if IN_COLAB else \"local\"])\n",").resolve()\n","\n","# Path to YAML config (always in config/)\n","cfg_path = PROJECT_ROOT / \"config\" / \"training.yaml\"\n","\n","# Check config file\n","if not cfg_path.exists():\n","    raise FileNotFoundError(f\"‚ùå training.yaml not found at: {cfg_path}\")\n","\n","# Load YAML config\n","with cfg_path.open() as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Extract config values\n","EXP_CODE   = \"20250723141202\" #cfg.get(\"exp_code\") or os.getenv(\"EXP_CODE\") or \"missing_code\"\n","DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n","\n","# Build central experiment path\n","EXP_DIR = PROJECT_ROOT / cfg[\"output\"][\"exp_dir\"].format(\n","    dataset_id=DATASET_ID, exp_code=EXP_CODE\n",")\n","\n","# Ovverride general training yaml file\n","cfg_path = EXP_DIR / f\"training_{EXP_CODE}.yaml\"\n","# Check config file\n","if not cfg_path.exists():\n","    raise FileNotFoundError(f\"‚ùå training.yaml not found at: {cfg_path}\")\n","\n","# Load YAML config\n","with cfg_path.open() as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Display\n","print(f\"üìÅ PROJECT_ROOT ‚Üí {PROJECT_ROOT}\")\n","print(f\"üìÑ YAML loaded  ‚Üí {cfg_path.name}\")\n","print(f\"üîë EXP_CODE     ‚Üí {EXP_CODE}\")\n","print(f\"üìÇ EXP_DIR      ‚Üí {EXP_DIR}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ovj5W--3rN4J","executionInfo":{"status":"ok","timestamp":1753294011490,"user_tz":-120,"elapsed":5377,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"7530446d-02c8-441c-87c9-67566641b68e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÅ PROJECT_ROOT ‚Üí /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\n","üìÑ YAML loaded  ‚Üí training_20250723141202.yaml\n","üîë EXP_CODE     ‚Üí 20250723141202\n","üìÇ EXP_DIR      ‚Üí /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250723141202\n"]}]},{"cell_type":"code","source":["# Cell 3 ‚Äì Extend PYTHONPATH & Import Trainers + Utils\n","\n","import sys\n","import importlib\n","from pathlib import Path\n","\n","# Extend PYTHONPATH to include src/\n","SRC_DIR = PROJECT_ROOT / \"src\"\n","sys.path[:0] = [str(PROJECT_ROOT), str(SRC_DIR)]\n","\n","# Import training utils from utils.training_utils\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.device_io import (\n","    choose_device,\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json,\n","    save_joblib,\n",")\n","from utils.training_utils.data_utils import (\n","    build_loader,\n","    load_classifier,\n","    parse_label_from_filename,\n",")\n","from utils.training_utils.model_utils import mc_dropout_predictions\n","from utils.training_utils.metrics import (\n","    compute_classification_metrics,\n","    aggregate_fold_metrics,\n","    expected_calibration_error,\n",")\n","\n","# Import trainer modules dynamically to ensure registration\n","trainer_names = [\"simclr\", \"moco_v2\", \"rotation\", \"jepa\", \"supervised\", \"transfer\"]\n","for name in trainer_names:\n","    try:\n","        importlib.import_module(f\"trainers.{name}\")\n","        print(f\"‚úÖ Imported trainer module: {name}\")\n","    except ImportError as e:\n","        print(f\"‚ùå Failed to import trainer {name}: {e}\")\n","\n","# Verify registration\n","for name in trainer_names:\n","    assert name in TRAINER_REGISTRY, f\"‚ùå Missing trainer in registry: {name}\"\n","print(\"üìö All trainers successfully registered.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"047G_s2BrPQc","executionInfo":{"status":"ok","timestamp":1753294035350,"user_tz":-120,"elapsed":23852,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"a1376d7d-b0ec-4b3e-9c29-c2e04c666b85"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Imported trainer module: simclr\n","‚úÖ Imported trainer module: moco_v2\n","‚úÖ Imported trainer module: rotation\n","‚úÖ Imported trainer module: jepa\n","‚úÖ Imported trainer module: supervised\n","‚úÖ Imported trainer module: transfer\n","üìö All trainers successfully registered.\n"]}]},{"cell_type":"code","source":["# Cell 4 ‚Äì Evaluation Settings (debug-friendly)\n","\n","device = choose_device()\n","eval_cfg = cfg.get(\"evaluation\", {})\n","\n","# üîß Debug: riduci MC_PASSES ed ECE_BINS per velocizzare\n","MC_PASSES = min(int(eval_cfg.get(\"mc_dropout_passes\", 20)), 3)\n","ECE_BINS  = min(int(eval_cfg.get(\"ece_bins\", 15)), 5)\n","\n","GCAM_TOPK  = int(eval_cfg.get(\"gradcam\", {}).get(\"top_k\", 5))\n","GCAM_LAYER = eval_cfg.get(\"gradcam\", {}).get(\"layer\", None)\n","\n","print(f\"üñ•Ô∏è  Device:         {device}\")\n","print(f\"üîÑ  MC-dropout:     {MC_PASSES} passes\")\n","print(f\"üìä  ECE bins:       {ECE_BINS}\")\n","print(f\"üîç  GradCAM++ top-k:{GCAM_TOPK}\")\n","print(f\"üìê  GradCAM++ layer:{GCAM_LAYER}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mEfJ91DerQr1","executionInfo":{"status":"ok","timestamp":1753294035369,"user_tz":-120,"elapsed":15,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"494a5dce-9e35-4d25-a076-aca2fd9db163"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["üñ•Ô∏è  Device:         cpu\n","üîÑ  MC-dropout:     3 passes\n","üìä  ECE bins:       5\n","üîç  GradCAM++ top-k:5\n","üìê  GradCAM++ layer:layer4\n"]}]},{"cell_type":"code","source":["# Cell 5 ‚Äì Helper Functions & Path Centralization\n","\n","from pathlib import Path\n","\n","def _paths(model_name: str, fold: int, patient_id: str = None) -> dict[str, Path]:\n","    \"\"\"\n","    Build and return all relevant paths for a given model/fold,\n","    directly from the patterns in cfg['output'], WITHOUT formatting {epoch}.\n","    \"\"\"\n","    # Base placeholders\n","    ph = {\n","        \"dataset_id\": DATASET_ID,\n","        \"exp_code\":   EXP_CODE,\n","        \"model_name\": model_name,\n","        \"fold_idx\":   fold,\n","        \"patient_id\": patient_id or \"{patient_id}\",\n","    }\n","\n","    # Experiment directories\n","    ph[\"exp_dir\"]       = cfg[\"output\"][\"exp_dir\"].format(**ph)\n","    ph[\"exp_model_dir\"] = cfg[\"output\"][\"exp_model_dir\"].format(**ph)\n","\n","    out: dict[str, Path] = {}\n","\n","    # ‚îÄ‚îÄ‚îÄ Training ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    out[\"ckpt_dir\"] = PROJECT_ROOT / ph[\"exp_model_dir\"] / f\"fold{fold}\" / \"training\"\n","\n","    t = cfg[\"output\"][\"training\"]\n","    out[\"features_train\"] = PROJECT_ROOT / t[\"features\"].format(**ph)\n","    out[\"clf\"]            = PROJECT_ROOT / t[\"clf\"].format(**ph)\n","    out[\"scaler\"]         = PROJECT_ROOT / t[\"scaler\"].format(**ph)\n","    out[\"loss_json\"]      = PROJECT_ROOT / t[\"loss_json\"].format(**ph)\n","    out[\"log\"]            = PROJECT_ROOT / t[\"log\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    i = cfg[\"output\"][\"inference\"]\n","    out[\"patch_preds\"]   = PROJECT_ROOT / i[\"patch_preds\"].format(**ph)\n","    out[\"patient_preds\"] = PROJECT_ROOT / i[\"patient_preds\"].format(**ph)\n","    out[\"mc_logits\"]     = PROJECT_ROOT / i[\"mc_logits\"].format(**ph)\n","    out[\"metrics\"]       = PROJECT_ROOT / i[\"metrics\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Explainability ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    e = cfg[\"output\"][\"explain\"]\n","    out[\"gradcam_dir\"]  = PROJECT_ROOT / e[\"gradcam_dir\"].format(**ph)\n","    out[\"metadata_csv\"] = PROJECT_ROOT / e[\"metadata_csv\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Aggregation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    a = cfg[\"output\"][\"aggregate\"]\n","    out[\"agg_metrics\"] = PROJECT_ROOT / a[\"metrics\"].format(**ph)\n","    out[\"agg_summary\"] = PROJECT_ROOT / a[\"summary_img\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Experiment-Level ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    x = cfg[\"output\"][\"experiment_level\"]\n","    out[\"exp_json\"] = PROJECT_ROOT / x[\"comparison_json\"].format(**ph)\n","    out[\"exp_img\"]  = PROJECT_ROOT / x[\"comparison_img\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Ensure directories exist ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    for key, p in out.items():\n","        if \"dir\" in key:\n","            p.mkdir(parents=True, exist_ok=True)\n","        else:\n","            p.parent.mkdir(parents=True, exist_ok=True)\n","\n","    return out\n","\n","def _completed(paths: dict[str, Path], is_ssl: bool) -> bool:\n","    \"\"\"\n","    Returns True if all the necessary inference artifacts for this fold\n","    are already on disk, so we can skip evaluation.\n","    \"\"\"\n","    # Always require patch‚Äêlevel preds + metrics JSON\n","    required = [\"patch_preds\", \"metrics\"]\n","    # For SSL models also require MC logits and patient‚Äêlevel CSV\n","    if is_ssl:\n","        required += [\"mc_logits\", \"patient_preds\"]\n","    return all(paths[k].exists() for k in required)\n","\n","\n","def extract_patient_id(key: str) -> str:\n","    \"\"\"\n","    Extract patient ID from a key formatted like 'CLASS_HPxxxx_x_y'.\n","    \"\"\"\n","    parts = key.split(\"_\")\n","    return next((p for p in parts if p.startswith((\"HP\", \"H\"))), \"UNKNOWN\")\n","\n","\n","def ece(probs, labels):\n","    \"\"\"\n","    Expected Calibration Error (ECE) helper for quick access.\n","    \"\"\"\n","    return expected_calibration_error(probs, labels, n_bins=ECE_BINS)"],"metadata":{"id":"VkUXkgUyrR0e","executionInfo":{"status":"ok","timestamp":1753294035403,"user_tz":-120,"elapsed":25,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Cell 6a ‚Äì Core Evaluation Functions\n","\n","import torch\n","import torch.nn as nn\n","import joblib\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from tqdm import tqdm\n","from collections import defaultdict, Counter\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    roc_auc_score,\n","    confusion_matrix,\n","    classification_report\n",")\n","\n","from utils.training_utils.metrics import (\n","    TemperatureScaler,\n","    expected_calibration_error,\n","    mc_dropout_statistics\n",")\n","from utils.training_utils.data_utils import load_classifier, parse_label_from_filename\n","from utils.training_utils.device_io import (\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json\n",")\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.model_utils import mc_dropout_predictions\n","\n","def _fix_fc_from_ckpt(model: torch.nn.Module, ckpt_path: Path) -> None:\n","    \"\"\"\n","    Replace model.fc so that out_features matches the checkpoint metadata.\n","\n","    Parameters\n","    ----------\n","    model : torch.nn.Module\n","        The backbone with a (possibly wrong) final fully-connected layer.\n","    ckpt_path : Path\n","        Path to the .pt checkpoint; must contain 'class_to_idx' metadata.\n","    \"\"\"\n","    ckpt_data = torch.load(ckpt_path, map_location=\"cpu\")\n","    class_to_idx = ckpt_data.get(\"class_to_idx\", {})\n","    n_classes = len(class_to_idx)\n","    if n_classes <= 0:\n","        raise ValueError(\"Checkpoint lacks valid 'class_to_idx' metadata\")\n","    if not hasattr(model, \"fc\"):\n","        raise AttributeError(\"Model has no .fc attribute to patch\")\n","    if model.fc.out_features != n_classes:\n","        in_f = model.fc.in_features\n","        model.fc = nn.Linear(in_f, n_classes)\n","        print(f\"üîß Patched fc layer: in_features={in_f} ‚Üí out_features={n_classes}\")\n","\n","def load_model_and_components(model_name: str, fold: int):\n","    \"\"\"\n","    Carica:\n","      - il modello PyTorch dal checkpoint\n","      - per SSL: il probe (clf + label encoder) e, se presente, il TemperatureScaler\n","\n","    Restituisce:\n","      model        : nn.Module      ‚Äì backbone/encoder pronto per inferenza\n","      is_ssl       : bool           ‚Äì True se modello SSL\n","      clf          : Any            ‚Äì classificatore (probe) o None per SL\n","      le           : Any            ‚Äì LabelEncoder per SSL, altrimenti None\n","      temp_scaler  : TemperatureScaler | None\n","    \"\"\"\n","    paths   = _paths(model_name, fold)\n","    cfg_m   = cfg[\"models\"][model_name]\n","    is_ssl  = cfg_m[\"type\"] == \"ssl\"\n","    trainer = TRAINER_REGISTRY[model_name](cfg_m, cfg[\"data\"])\n","\n","    # 1Ô∏è‚É£ Scegli checkpoint\n","    if is_ssl and cfg.get(\"train_encoder_once\", False) and fold > 0:\n","        ckpt = get_latest_checkpoint(_paths(model_name, 0)[\"ckpt_dir\"])\n","        if ckpt is None:\n","            raise FileNotFoundError(f\"‚ùå Nessun checkpoint trovato in fold0 per {model_name}\")\n","        print(f\"   ‚ûî SSL+train_encoder_once ‚Üí encoder da fold0: {ckpt.name}\")\n","    else:\n","        ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","        if ckpt is None:\n","            raise FileNotFoundError(f\"‚ùå Nessun checkpoint trovato in fold{fold} per {model_name}\")\n","        print(f\"   ‚ûî Caricamento checkpoint fold{fold} ‚Üí {ckpt.name}\")\n","\n","    # 2Ô∏è‚É£ Carica pesi nel modello\n","    if is_ssl:\n","        full_model, _ = trainer.get_resume_model_and_optimizer()\n","        load_checkpoint(ckpt, model=full_model)\n","        feat_mod = getattr(trainer, \"encoder\", None) or getattr(trainer, \"model\", None)\n","        if feat_mod is None:\n","            raise AttributeError(f\"No feature submodule found on {trainer}\")\n","        model = feat_mod.to(device).eval()\n","    else:\n","        # Patch dinamica del layer fc per SL/transfer\n","        _fix_fc_from_ckpt(trainer.model, ckpt)\n","        load_checkpoint(ckpt, model=trainer.model)\n","        model = trainer.model.to(device).eval()\n","\n","    # 3Ô∏è‚É£ Carica probe / classifier / calibratore\n","    clf = le = temp_scaler = None\n","    if is_ssl:\n","        # classificatore joblib salvato in training SSL\n","        clf, le = load_classifier(paths[\"clf\"])\n","        # eventualmente temperature scaling\n","        scaler_path = paths[\"scaler\"]\n","        if scaler_path.exists():\n","            obj = joblib.load(scaler_path)\n","            if isinstance(obj, TemperatureScaler):\n","                temp_scaler = obj\n","                print(\"   ‚ûî Loaded TemperatureScaler (calibrate probs)\")\n","        else:\n","            print(f\"   ‚ûî No TemperatureScaler for {model_name} fold {fold}\")\n","\n","    # 4Ô∏è‚É£ Debug printout\n","    print(f\"   ‚ûî SSL pipeline?  {is_ssl}\")\n","    print(f\"   ‚ûî Classifier?    {clf is not None}\")\n","    print(f\"   ‚ûî Temp-scaler?   {temp_scaler is not None}\")\n","\n","    return model, is_ssl, clf, le, temp_scaler\n"],"metadata":{"id":"t21HY__kszl_","executionInfo":{"status":"ok","timestamp":1753294035692,"user_tz":-120,"elapsed":216,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#  Cell 6b ‚Äì Core Evaluation Functions\n","def run_patch_inference(model, loader, is_ssl, clf, temp_scaler):\n","    \"\"\"\n","    Inferenzia patch-level. Restituisce:\n","      keys (dummy), y_true, y_pred, probs (calibrated if temp_scaler)\n","    \"\"\"\n","    keys, y_true, y_pred = [], [], []\n","    probs_list = []\n","\n","    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Patches\")):\n","        imgs, labels = batch\n","        imgs = imgs.to(device)\n","\n","        if is_ssl:\n","            with torch.no_grad():\n","                feats = model(imgs).cpu().numpy()\n","            raw_p = clf.predict_proba(feats)\n","        else:\n","            with torch.no_grad():\n","                logits = model(imgs)\n","                raw_p = torch.softmax(logits, dim=1).cpu().numpy()\n","\n","        # calibration\n","        if temp_scaler is not None:\n","            logits_for_cal = np.log(raw_p + 1e-12)\n","            p = temp_scaler.transform_proba(logits_for_cal)\n","        else:\n","            p = raw_p\n","\n","        preds = p.argmax(axis=1)\n","        t     = labels.cpu().numpy()\n","\n","        y_true.extend(t.tolist())\n","        y_pred.extend(preds.tolist())\n","        probs_list.append(p)\n","\n","        print(f\"   ‚Ä¢ Batch {batch_idx} done.\")\n","\n","    probs = np.vstack(probs_list)\n","    return keys, np.array(y_true), np.array(y_pred), probs\n","\n","\n","def save_patch_outputs(model_name, fold, keys, y_true, y_pred, probs):\n","    torch.save({\n","        \"keys\": keys,\n","        \"true\": y_true,\n","        \"pred\": y_pred,\n","        \"probs\": probs\n","    }, _paths(model_name, fold)[\"patch_preds\"])\n","\n","\n","def save_mc_logits(model_name, fold, model, loader):\n","    mc = mc_dropout_predictions(model, loader, device=device, T=MC_PASSES)\n","    np.save(_paths(model_name, fold)[\"mc_logits\"], mc)\n","    return mc\n","\n","\n","def compute_and_save_metrics(model_name, fold, y_true, y_pred, probs, mc=None):\n","    \"\"\"\n","    Calcola tutte le metriche, MC-stats e ECE post-calibrazione.\n","    \"\"\"\n","    mc_stats = mc_dropout_statistics(mc) if mc is not None else {}\n","\n","    acc  = accuracy_score(y_true, y_pred)\n","    f1   = f1_score(y_true, y_pred, average=\"macro\")\n","    try:\n","        auc = roc_auc_score(y_true, y_pred, average=\"macro\", multi_class=\"ovo\")\n","    except ValueError:\n","        auc = None\n","    cm = confusion_matrix(y_true, y_pred).tolist()\n","    cr = classification_report(y_true, y_pred, output_dict=True)\n","\n","    mets = {\n","        \"accuracy\": acc,\n","        \"macro_f1\": f1,\n","        \"roc_auc\": auc,\n","        \"confusion_matrix\": cm,\n","        \"class_report\": cr,\n","        **mc_stats\n","    }\n","\n","    # ECE post-calibrazione\n","    mets[\"ece_post\"] = expected_calibration_error(probs, y_true, n_bins=ECE_BINS)\n","\n","    save_json(mets, _paths(model_name, fold)[\"metrics\"])\n","    return mets\n"],"metadata":{"id":"9PAs8FCDrTNz","executionInfo":{"status":"ok","timestamp":1753294035696,"user_tz":-120,"elapsed":190,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Cell 7 ‚Äì Per-Fold Evaluation (aggiornato)\n","\n","from utils.training_utils.data_utils import default_transforms, build_loader, KNOWN_LABELS, parse_label_from_filename\n","from utils.training_utils.device_io import get_latest_checkpoint\n","from collections import defaultdict, Counter\n","import numpy as np\n","import pandas as pd\n","\n","def aggregate_patient_results(model_name, fold, keys, y_pred, probs):\n","    \"\"\"\n","    Aggrega predizioni per paziente (majority voting) e salva CSV.\n","    \"\"\"\n","    by_pt = defaultdict(list)\n","    for k, y, conf in zip(keys, y_pred, probs.max(axis=1)):\n","        pid = extract_patient_id(k)\n","        by_pt[pid].append((y, conf))\n","\n","    rows = []\n","    for pid, recs in by_pt.items():\n","        votes, confs = zip(*recs)\n","        rows.append({\n","            \"patient_id\": pid,\n","            \"true_label\": parse_label_from_filename(pid),\n","            \"pred_label\": Counter(votes).most_common(1)[0][0],\n","            \"n_patches\": len(recs),\n","            \"mean_conf_raw\": float(np.mean(confs))\n","        })\n","\n","    pd.DataFrame(rows).to_csv(\n","        _paths(model_name, fold)[\"patient_preds\"], index=False\n","    )\n","\n","def evaluate_fold(model_name: str, fold: int):\n","    print(f\"\\nüîç Evaluating {model_name} fold {fold}‚Ä¶\")\n","\n","    # 0Ô∏è‚É£ Paths e checkpoint\n","    paths = _paths(model_name, fold)\n","    ckpt  = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","    if ckpt is None:\n","        print(f\"‚ö†Ô∏è Nessun checkpoint trovato per {model_name} fold {fold}, salto valutazione\")\n","        return\n","\n","    # 1Ô∏è‚É£ Skip se artefatti gi√† presenti\n","    is_ssl = (cfg[\"models\"][model_name][\"type\"] == \"ssl\")\n","    if _completed(paths, is_ssl):\n","        print(f\"‚ö° Skipping {model_name} fold {fold}: artifacts already present\")\n","        return\n","\n","    # 2Ô∏è‚É£ Carica modello, probe, scaler\n","    print(\"üì• Loading model, classifier, scaler‚Ä¶\")\n","    model, is_ssl, clf, le, temp_scaler = load_model_and_components(model_name, fold)\n","\n","    # 3Ô∏è‚É£ Build test loader\n","    patch_size = cfg[\"models\"][model_name].get(\"patch_size\", 224)\n","    batch_size = cfg[\"models\"][model_name][\"training\"][\"batch_size\"]\n","    test_rel   = cfg[\"data\"][\"test\"].format(\n","        fold_idx=fold,\n","        dataset_id=cfg[\"data\"][\"dataset_id\"]\n","    )\n","    test_wds   = (PROJECT_ROOT / test_rel).resolve()\n","    print(f\"üß™ Using test shard: {test_wds}\")\n","    assert test_wds.exists(), f\"‚ùå Test shard not found: {test_wds}\"\n","\n","    # ‚ùó mappatura globale delle classi\n","    all_classes  = sorted(KNOWN_LABELS)  # {\"ccRCC\",\"pRCC\",\"CHROMO\",\"ONCO\",\"not_tumor\"} :contentReference[oaicite:0]{index=0}\n","    class_to_idx = {cls: i for i, cls in enumerate(all_classes)}\n","\n","    loader = build_loader(\n","        str(test_wds),\n","        class_to_idx=class_to_idx,\n","        patch_size=patch_size,\n","        batch_size=batch_size,\n","        device=device,\n","        augment=False,\n","    )\n","    print(f\"üì¶ DataLoader ready with batch_size = {batch_size}\")\n","\n","    # 4Ô∏è‚É£ Patch-level inference\n","    print(\"‚ñ∂Ô∏è Running inference (patch-level)‚Ä¶\")\n","    keys, y_true, y_pred, probs = run_patch_inference(\n","        model, loader, is_ssl, clf, temp_scaler\n","    )\n","    print(f\"   ‚ûî Patches processed: {len(y_true)}\")\n","    save_patch_outputs(model_name, fold, keys, y_true, y_pred, probs)\n","    print(\"üíæ Saved patch-level outputs.\")\n","\n","    # 5Ô∏è‚É£ MC-Dropout (solo SSL)\n","    if is_ssl:\n","        print(\"üîÑ Running MC-Dropout‚Ä¶\")\n","        mc = save_mc_logits(model_name, fold, model, loader)\n","        print(\"üíæ Saved MC-Dropout logits.\")\n","    else:\n","        mc = None\n","\n","    # 6Ô∏è‚É£ Compute & save metrics\n","    print(\"üìä Computing metrics‚Ä¶\")\n","    compute_and_save_metrics(model_name, fold, y_true, y_pred, probs, mc)\n","    print(\"üíæ Saved metrics JSON.\")\n","\n","    # 7Ô∏è‚É£ Patient-level aggregation (solo SSL)\n","    if is_ssl:\n","        print(\"üë®‚Äç‚öïÔ∏è Aggregating patient-level results‚Ä¶\")\n","        aggregate_patient_results(model_name, fold, keys, y_pred, probs)\n","        print(\"üíæ Saved patient-level CSV.\")\n","\n","    print(f\"‚úÖ Done {model_name} fold {fold}\\n\")\n","\n","\n","# üöÄ Run evaluation for all models and folds\n","for model_name in cfg[\"run_models\"]:\n","    for fold_idx in cfg[\"folds\"]:\n","        evaluate_fold(model_name, fold_idx)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"t_oHDrpWrU3x","executionInfo":{"status":"error","timestamp":1753296300074,"user_tz":-120,"elapsed":2264496,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"f0d13e10-325a-4c06-8a5d-310ceaadde34"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîç Evaluating rotation fold 0‚Ä¶\n","üì• Loading model, classifier, scaler‚Ä¶\n","   ‚ûî Caricamento checkpoint fold0 ‚Üí RotationTrainer_bestepoch001.pt\n","   ‚ûî Loaded TemperatureScaler (calibrate probs)\n","   ‚ûî SSL pipeline?  True\n","   ‚ûî Classifier?    True\n","   ‚ûî Temp-scaler?   True\n","üß™ Using test shard: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test_holdout.tar\n","üì¶ DataLoader ready with batch_size = 64\n","‚ñ∂Ô∏è Running inference (patch-level)‚Ä¶\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 1it [00:34, 34.10s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 0 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 2it [01:08, 34.13s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 1 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 3it [01:41, 33.63s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 2 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 4it [02:13, 32.98s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 3 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 5it [02:45, 32.81s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 4 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 6it [03:18, 32.78s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 5 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 7it [03:50, 32.57s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 6 done.\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 8it [04:10, 31.36s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 7 done.\n","   ‚ûî Patches processed: 487\n","üíæ Saved patch-level outputs.\n","üîÑ Running MC-Dropout‚Ä¶\n","üîÅ MC-Dropout pass 1/3\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["    ‚Ä¢ Batch 1 / MC-pass 1\n","    ‚Ä¢ Batch 2 / MC-pass 1\n","    ‚Ä¢ Batch 3 / MC-pass 1\n","    ‚Ä¢ Batch 4 / MC-pass 1\n","    ‚Ä¢ Batch 5 / MC-pass 1\n","    ‚Ä¢ Batch 6 / MC-pass 1\n","    ‚Ä¢ Batch 7 / MC-pass 1\n","    ‚Ä¢ Batch 8 / MC-pass 1\n","üîÅ MC-Dropout pass 2/3\n","    ‚Ä¢ Batch 1 / MC-pass 2\n","    ‚Ä¢ Batch 2 / MC-pass 2\n","    ‚Ä¢ Batch 3 / MC-pass 2\n","    ‚Ä¢ Batch 4 / MC-pass 2\n","    ‚Ä¢ Batch 5 / MC-pass 2\n","    ‚Ä¢ Batch 6 / MC-pass 2\n","    ‚Ä¢ Batch 7 / MC-pass 2\n","    ‚Ä¢ Batch 8 / MC-pass 2\n","üîÅ MC-Dropout pass 3/3\n","    ‚Ä¢ Batch 1 / MC-pass 3\n","    ‚Ä¢ Batch 2 / MC-pass 3\n","    ‚Ä¢ Batch 3 / MC-pass 3\n","    ‚Ä¢ Batch 4 / MC-pass 3\n","    ‚Ä¢ Batch 5 / MC-pass 3\n","    ‚Ä¢ Batch 6 / MC-pass 3\n","    ‚Ä¢ Batch 7 / MC-pass 3\n","    ‚Ä¢ Batch 8 / MC-pass 3\n","‚úÖ MC-Dropout completato: 3 pass, 487 patch per pass.\n","üíæ Saved MC-Dropout logits.\n","üìä Computing metrics‚Ä¶\n","üíæ Saved metrics JSON.\n","üë®‚Äç‚öïÔ∏è Aggregating patient-level results‚Ä¶\n","üíæ Saved patient-level CSV.\n","‚úÖ Done rotation fold 0\n","\n","\n","üîç Evaluating rotation fold 1‚Ä¶\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["‚ö†Ô∏è Nessun checkpoint trovato per rotation fold 1, salto valutazione\n","\n","üîç Evaluating simclr fold 0‚Ä¶\n","üì• Loading model, classifier, scaler‚Ä¶\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"stream","name":"stdout","text":["   ‚ûî Caricamento checkpoint fold0 ‚Üí SimCLRTrainer_bestepoch003.pt\n","   ‚ûî Loaded TemperatureScaler (calibrate probs)\n","   ‚ûî SSL pipeline?  True\n","   ‚ûî Classifier?    True\n","   ‚ûî Temp-scaler?   True\n","üß™ Using test shard: /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/test_holdout.tar\n","üì¶ DataLoader ready with batch_size = 64\n","‚ñ∂Ô∏è Running inference (patch-level)‚Ä¶\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 1it [00:34, 34.92s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 0 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 2it [01:08, 33.95s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 1 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 3it [01:46, 36.10s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 2 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 4it [02:20, 35.12s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 3 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 5it [02:54, 34.71s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 4 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 6it [03:28, 34.54s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 5 done.\n"]},{"output_type":"stream","name":"stderr","text":["\rPatches: 7it [04:01, 33.97s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 6 done.\n"]},{"output_type":"stream","name":"stderr","text":["Patches: 8it [04:22, 32.79s/it]"]},{"output_type":"stream","name":"stdout","text":["   ‚Ä¢ Batch 7 done.\n","   ‚ûî Patches processed: 487\n","üíæ Saved patch-level outputs.\n","üîÑ Running MC-Dropout‚Ä¶\n","üîÅ MC-Dropout pass 1/3\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["    ‚Ä¢ Batch 1 / MC-pass 1\n","    ‚Ä¢ Batch 2 / MC-pass 1\n","    ‚Ä¢ Batch 3 / MC-pass 1\n","    ‚Ä¢ Batch 4 / MC-pass 1\n","    ‚Ä¢ Batch 5 / MC-pass 1\n","    ‚Ä¢ Batch 6 / MC-pass 1\n","    ‚Ä¢ Batch 7 / MC-pass 1\n","    ‚Ä¢ Batch 8 / MC-pass 1\n","üîÅ MC-Dropout pass 2/3\n","    ‚Ä¢ Batch 1 / MC-pass 2\n","    ‚Ä¢ Batch 2 / MC-pass 2\n","    ‚Ä¢ Batch 3 / MC-pass 2\n","    ‚Ä¢ Batch 4 / MC-pass 2\n","    ‚Ä¢ Batch 5 / MC-pass 2\n","    ‚Ä¢ Batch 6 / MC-pass 2\n","    ‚Ä¢ Batch 7 / MC-pass 2\n","    ‚Ä¢ Batch 8 / MC-pass 2\n","üîÅ MC-Dropout pass 3/3\n","    ‚Ä¢ Batch 1 / MC-pass 3\n","    ‚Ä¢ Batch 2 / MC-pass 3\n","    ‚Ä¢ Batch 3 / MC-pass 3\n","    ‚Ä¢ Batch 4 / MC-pass 3\n","    ‚Ä¢ Batch 5 / MC-pass 3\n","    ‚Ä¢ Batch 6 / MC-pass 3\n","    ‚Ä¢ Batch 7 / MC-pass 3\n","    ‚Ä¢ Batch 8 / MC-pass 3\n","‚úÖ MC-Dropout completato: 3 pass, 487 patch per pass.\n","üíæ Saved MC-Dropout logits.\n","üìä Computing metrics‚Ä¶\n","üíæ Saved metrics JSON.\n","üë®‚Äç‚öïÔ∏è Aggregating patient-level results‚Ä¶\n","üíæ Saved patient-level CSV.\n","‚úÖ Done simclr fold 0\n","\n","\n","üîç Evaluating simclr fold 1‚Ä¶\n","‚ö†Ô∏è Nessun checkpoint trovato per simclr fold 1, salto valutazione\n","\n","üîç Evaluating moco_v2 fold 0‚Ä¶\n","üì• Loading model, classifier, scaler‚Ä¶\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"stream","name":"stdout","text":["   ‚ûî Caricamento checkpoint fold0 ‚Üí MoCoV2Trainer_bestepoch001.pt\n"]},{"output_type":"error","ename":"AttributeError","evalue":"No feature submodule found on <trainers.moco_v2.MoCoV2Trainer object at 0x7c8f8cbe9810>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-9-3033625619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"folds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mevaluate_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-9-3033625619.py\u001b[0m in \u001b[0;36mevaluate_fold\u001b[0;34m(model_name, fold)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# 2Ô∏è‚É£ Carica modello, probe, scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üì• Loading model, classifier, scaler‚Ä¶\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# 3Ô∏è‚É£ Build test loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-7-3066692115.py\u001b[0m in \u001b[0;36mload_model_and_components\u001b[0;34m(model_name, fold)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mfeat_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeat_mod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No feature submodule found on {trainer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: No feature submodule found on <trainers.moco_v2.MoCoV2Trainer object at 0x7c8f8cbe9810>"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/\n"],"metadata":{"id":"BLBMOnmBzfsa","executionInfo":{"status":"aborted","timestamp":1753296301741,"user_tz":-120,"elapsed":2529236,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 8 ‚Äì Fold-Level & Experiment-Level Aggregation (robust)\n","\n","import json\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","\n","from utils.training_utils.device_io import save_json\n","from utils.training_utils.metrics import aggregate_fold_metrics\n","\n","def aggregate_model(model_name: str):\n","    \"\"\"\n","    Raccoglie e aggrega le metriche per ciascun fold disponibile.\n","    Salta automaticamente i fold senza file di metrics.\n","    \"\"\"\n","    # 1Ô∏è‚É£ Raccogli le metriche per fold esistenti\n","    all_m = []\n","    available_folds = []\n","    for f in cfg[\"folds\"]:\n","        metrics_path = Path(_paths(model_name, f)[\"metrics\"])\n","        if metrics_path.exists():\n","            all_m.append(json.load(open(metrics_path)))\n","            available_folds.append(f)\n","        else:\n","            print(f\"‚ö†Ô∏è Nessuna metrica trovata per {model_name} fold {f}, skip\")\n","\n","    if not all_m:\n","        print(f\"‚ùå Nessuna metrica disponibile per modello '{model_name}', skip aggregation\")\n","        return\n","\n","    # 2Ô∏è‚É£ Estrai solo i campi numerici (int/float) dal primo fold valido\n","    numeric_keys = [k for k, v in all_m[0].items() if isinstance(v, (int, float))]\n","\n","    # 3Ô∏è‚É£ Costruisci lista di dict solo con quei campi, trasformando None‚ÜíNaN\n","    numeric_per_fold = []\n","    for m in all_m:\n","        filtered = {k: (m.get(k) if isinstance(m.get(k), (int, float)) else np.nan)\n","                    for k in numeric_keys}\n","        numeric_per_fold.append(filtered)\n","\n","    # 4Ô∏è‚É£ Debug: segnala NaN trovati\n","    print(f\"\\nüîç Debug NaN per modello '{model_name}':\")\n","    nan_found = False\n","    for idx, m in enumerate(numeric_per_fold):\n","        f = available_folds[idx]\n","        for k in numeric_keys:\n","            if pd.isna(m[k]):\n","                print(f\"  ‚ö†Ô∏è NaN in fold {f}, metrica '{k}'\")\n","                nan_found = True\n","    if not nan_found:\n","        print(\"  ‚úÖ Nessun NaN rilevato nelle metriche numeriche.\")\n","\n","    # 5Ô∏è‚É£ Aggrega mean¬±std e salva JSON\n","    summary = aggregate_fold_metrics(numeric_per_fold)\n","    agg_metrics_path = Path(_paths(model_name, available_folds[0])[\"agg_metrics\"])\n","    save_json(summary, agg_metrics_path)\n","    print(f\"üíæ Saved aggregated metrics ‚Üí {agg_metrics_path}\")\n","\n","    # 6Ô∏è‚É£ Heatmap delle medie\n","    dfm = pd.DataFrame(numeric_per_fold, index=available_folds)\n","    fig, ax = plt.subplots(figsize=(6, 4))\n","    sns.heatmap(\n","        dfm.mean()[numeric_keys].to_frame().T,\n","        annot=True, fmt=\".3f\", ax=ax\n","    )\n","    fig.savefig(Path(_paths(model_name, available_folds[0])[\"agg_summary\"]), bbox_inches=\"tight\")\n","    plt.close(fig)\n","    print(f\"‚úÖ Aggregated {model_name}\")\n","\n","def aggregate_experiment():\n","    \"\"\"\n","    Costruisce il riepilogo tra modelli, include solo i modelli con JSON agg esistenti.\n","    \"\"\"\n","    rows = []\n","    for m in cfg[\"run_models\"]:\n","        exp_json_path = Path(_paths(m, cfg[\"folds\"][0])[\"agg_metrics\"])\n","        if exp_json_path.exists():\n","            met = json.load(open(exp_json_path))\n","            row = {\"model\": m}\n","            for k, v in met.items():\n","                row[k] = v[\"mean\"]\n","            rows.append(row)\n","        else:\n","            print(f\"‚ö†Ô∏è Saltato modello '{m}' in experiment-level perch√© manca agg_metrics\")\n","\n","    if not rows:\n","        print(\"‚ùå Nessun modello ha agg_metrics, skip experiment aggregation\")\n","        return\n","\n","    # 1Ô∏è‚É£ Salva JSON di esperimento\n","    exp_json_path = Path(_paths(cfg[\"run_models\"][0], cfg[\"folds\"][0])[\"exp_json\"])\n","    save_json(rows, exp_json_path)\n","    print(f\"üíæ Saved experiment summary JSON ‚Üí {exp_json_path}\")\n","\n","    # 2Ô∏è‚É£ Barplot accuracy\n","    df = pd.DataFrame(rows)\n","    fig, ax = plt.subplots(figsize=(8, 4))\n","    sns.barplot(data=df, x=\"model\", y=\"accuracy\", ax=ax)\n","    ax.set_title(\"Model Accuracy Comparison\")\n","    fig.savefig(Path(_paths(cfg[\"run_models\"][0], cfg[\"folds\"][0])[\"exp_img\"]), bbox_inches=\"tight\")\n","    plt.close(fig)\n","    print(\"‚úÖ Experiment-level comparison complete\")\n","\n","# üöÄ Esegui aggregazioni\n","for m in cfg[\"run_models\"]:\n","    aggregate_model(m)\n","aggregate_experiment()\n"],"metadata":{"id":"9l1_lvR3rcQ8","executionInfo":{"status":"aborted","timestamp":1753296302394,"user_tz":-120,"elapsed":2529886,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]}]}