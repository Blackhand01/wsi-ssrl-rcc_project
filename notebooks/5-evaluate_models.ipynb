{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"18aVreXE_S816USEyxxUnhWV1s1hss58Y","authorship_tag":"ABX9TyNhBcDAez70zhXPiQ8w0lXf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fbjhtw6U8RIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"XnLTyrxFrF3N","executionInfo":{"status":"ok","timestamp":1752630925233,"user_tz":240,"elapsed":125596,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f442429-88ef-49ca-e042-f0a7870af785"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Cell 1 ‚Äì Install & Imports\n","!pip install --quiet torch torchvision webdataset tqdm pillow scikit-learn joblib matplotlib seaborn pyyaml\n","\n","import os, sys, json, yaml, joblib\n","from pathlib import Path\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# set matplotlib style\n","plt.rcParams.update({\"figure.max_open_warning\": 0})\n"]},{"cell_type":"code","source":["# Cell 2 ‚Äì Load Configuration & Paths (robust version)\n","import os\n","import yaml\n","from pathlib import Path\n","\n","# Define default root paths\n","DEFAULT_ENV_PATHS = {\n","    \"colab\": \"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\",\n","    \"local\": \"/Users/stefanoroybisignano/Desktop/MLA/project/wsi-ssrl-rcc_project\",\n","}\n","\n","# Determine environment\n","IN_COLAB = Path(\"/content\").exists()\n","PROJECT_ROOT = Path(\n","    os.getenv(\"PROJECT_ROOT\", DEFAULT_ENV_PATHS[\"colab\" if IN_COLAB else \"local\"])\n",").resolve()\n","\n","# Path to YAML config (always in config/)\n","cfg_path = PROJECT_ROOT / \"config\" / \"training.yaml\"\n","\n","# Check config file\n","if not cfg_path.exists():\n","    raise FileNotFoundError(f\"‚ùå training.yaml not found at: {cfg_path}\")\n","\n","# Load YAML config\n","with cfg_path.open() as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Extract config values\n","EXP_CODE   = cfg.get(\"exp_code\") or os.getenv(\"EXP_CODE\") or \"missing_code\"\n","DATASET_ID = cfg[\"data\"][\"dataset_id\"]\n","\n","# Build central experiment path\n","EXP_DIR = PROJECT_ROOT / cfg[\"output\"][\"exp_dir\"].format(\n","    dataset_id=DATASET_ID, exp_code=EXP_CODE\n",")\n","\n","# Ovverride general training yaml file\n","cfg_path = EXP_DIR / f\"training_{EXP_CODE}.yaml\"\n","# Check config file\n","if not cfg_path.exists():\n","    raise FileNotFoundError(f\"‚ùå training.yaml not found at: {cfg_path}\")\n","\n","# Load YAML config\n","with cfg_path.open() as f:\n","    cfg = yaml.safe_load(f)\n","\n","# Display\n","print(f\"üìÅ PROJECT_ROOT ‚Üí {PROJECT_ROOT}\")\n","print(f\"üìÑ YAML loaded  ‚Üí {cfg_path.name}\")\n","print(f\"üîë EXP_CODE     ‚Üí {EXP_CODE}\")\n","print(f\"üìÇ EXP_DIR      ‚Üí {EXP_DIR}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ovj5W--3rN4J","executionInfo":{"status":"ok","timestamp":1752630928756,"user_tz":240,"elapsed":3510,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"d1644c10-8ec0-44b7-b7e4-7bf518ec211c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÅ PROJECT_ROOT ‚Üí /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project\n","üìÑ YAML loaded  ‚Üí training_20250709142129.yaml\n","üîë EXP_CODE     ‚Üí 20250709142129\n","üìÇ EXP_DIR      ‚Üí /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/experiments/20250709142129\n"]}]},{"cell_type":"code","source":["# Cell 3 ‚Äì Extend PYTHONPATH & Import Trainers + Utils\n","\n","import sys\n","import importlib\n","from pathlib import Path\n","\n","# Extend PYTHONPATH to include src/\n","SRC_DIR = PROJECT_ROOT / \"src\"\n","sys.path[:0] = [str(PROJECT_ROOT), str(SRC_DIR)]\n","\n","# Import training utils from utils.training_utils\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.device_io import (\n","    choose_device,\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json,\n","    save_joblib,\n",")\n","from utils.training_utils.data_utils import (\n","    build_loader,\n","    load_classifier,\n","    parse_label_from_filename,\n",")\n","from utils.training_utils.model_utils import mc_dropout_predictions\n","from utils.training_utils.metrics import (\n","    compute_classification_metrics,\n","    aggregate_fold_metrics,\n","    expected_calibration_error,\n",")\n","\n","# Import trainer modules dynamically to ensure registration\n","trainer_names = [\"simclr\", \"moco_v2\", \"rotation\", \"jepa\", \"supervised\", \"transfer\"]\n","for name in trainer_names:\n","    try:\n","        importlib.import_module(f\"trainers.{name}\")\n","        print(f\"‚úÖ Imported trainer module: {name}\")\n","    except ImportError as e:\n","        print(f\"‚ùå Failed to import trainer {name}: {e}\")\n","\n","# Verify registration\n","for name in trainer_names:\n","    assert name in TRAINER_REGISTRY, f\"‚ùå Missing trainer in registry: {name}\"\n","print(\"üìö All trainers successfully registered.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"047G_s2BrPQc","executionInfo":{"status":"ok","timestamp":1752630952677,"user_tz":240,"elapsed":23901,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"8bd846bf-1bf4-4365-ad13-ddf9d11c7591"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Imported trainer module: simclr\n","‚úÖ Imported trainer module: moco_v2\n","‚úÖ Imported trainer module: rotation\n","‚úÖ Imported trainer module: jepa\n","‚úÖ Imported trainer module: supervised\n","‚úÖ Imported trainer module: transfer\n","üìö All trainers successfully registered.\n"]}]},{"cell_type":"code","source":["# Cell 4 ‚Äì Evaluation Settings (debug-friendly)\n","\n","device = choose_device()\n","eval_cfg = cfg.get(\"evaluation\", {})\n","\n","# üîß Debug: riduci MC_PASSES ed ECE_BINS per velocizzare\n","MC_PASSES = min(int(eval_cfg.get(\"mc_dropout_passes\", 20)), 3)\n","ECE_BINS  = min(int(eval_cfg.get(\"ece_bins\", 15)), 5)\n","\n","GCAM_TOPK  = int(eval_cfg.get(\"gradcam\", {}).get(\"top_k\", 5))\n","GCAM_LAYER = eval_cfg.get(\"gradcam\", {}).get(\"layer\", None)\n","\n","print(f\"üñ•Ô∏è  Device:         {device}\")\n","print(f\"üîÑ  MC-dropout:     {MC_PASSES} passes\")\n","print(f\"üìä  ECE bins:       {ECE_BINS}\")\n","print(f\"üîç  GradCAM++ top-k:{GCAM_TOPK}\")\n","print(f\"üìê  GradCAM++ layer:{GCAM_LAYER}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mEfJ91DerQr1","executionInfo":{"status":"ok","timestamp":1752630952699,"user_tz":240,"elapsed":14,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"bcbf094e-9295-4ced-c3bc-b726f74f8228"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["üñ•Ô∏è  Device:         cpu\n","üîÑ  MC-dropout:     3 passes\n","üìä  ECE bins:       5\n","üîç  GradCAM++ top-k:5\n","üìê  GradCAM++ layer:layer4\n"]}]},{"cell_type":"code","source":["# Cell 5 ‚Äì Helper Functions & Path Centralization\n","\n","from pathlib import Path\n","\n","def _paths(model_name: str, fold: int, patient_id: str = None) -> dict[str, Path]:\n","    \"\"\"\n","    Build and return all relevant paths for a given model/fold,\n","    directly from the patterns in cfg['output'], WITHOUT formatting {epoch}.\n","    \"\"\"\n","    # Base placeholders\n","    ph = {\n","        \"dataset_id\": DATASET_ID,\n","        \"exp_code\":   EXP_CODE,\n","        \"model_name\": model_name,\n","        \"fold_idx\":   fold,\n","        \"patient_id\": patient_id or \"{patient_id}\",\n","    }\n","\n","    # Experiment directories\n","    ph[\"exp_dir\"]       = cfg[\"output\"][\"exp_dir\"].format(**ph)\n","    ph[\"exp_model_dir\"] = cfg[\"output\"][\"exp_model_dir\"].format(**ph)\n","\n","    out: dict[str, Path] = {}\n","\n","    # ‚îÄ‚îÄ‚îÄ Training ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    out[\"ckpt_dir\"] = PROJECT_ROOT / ph[\"exp_model_dir\"] / f\"fold{fold}\" / \"training\"\n","\n","    t = cfg[\"output\"][\"training\"]\n","    out[\"features_train\"] = PROJECT_ROOT / t[\"features\"].format(**ph)\n","    out[\"clf\"]            = PROJECT_ROOT / t[\"clf\"].format(**ph)\n","    out[\"scaler\"]         = PROJECT_ROOT / t[\"scaler\"].format(**ph)\n","    out[\"loss_json\"]      = PROJECT_ROOT / t[\"loss_json\"].format(**ph)\n","    out[\"log\"]            = PROJECT_ROOT / t[\"log\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    i = cfg[\"output\"][\"inference\"]\n","    out[\"patch_preds\"]   = PROJECT_ROOT / i[\"patch_preds\"].format(**ph)\n","    out[\"patient_preds\"] = PROJECT_ROOT / i[\"patient_preds\"].format(**ph)\n","    out[\"mc_logits\"]     = PROJECT_ROOT / i[\"mc_logits\"].format(**ph)\n","    out[\"metrics\"]       = PROJECT_ROOT / i[\"metrics\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Explainability ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    e = cfg[\"output\"][\"explain\"]\n","    out[\"gradcam_dir\"]  = PROJECT_ROOT / e[\"gradcam_dir\"].format(**ph)\n","    out[\"metadata_csv\"] = PROJECT_ROOT / e[\"metadata_csv\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Aggregation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    a = cfg[\"output\"][\"aggregate\"]\n","    out[\"agg_metrics\"] = PROJECT_ROOT / a[\"metrics\"].format(**ph)\n","    out[\"agg_summary\"] = PROJECT_ROOT / a[\"summary_img\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Experiment-Level ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    x = cfg[\"output\"][\"experiment_level\"]\n","    out[\"exp_json\"] = PROJECT_ROOT / x[\"comparison_json\"].format(**ph)\n","    out[\"exp_img\"]  = PROJECT_ROOT / x[\"comparison_img\"].format(**ph)\n","\n","    # ‚îÄ‚îÄ‚îÄ Ensure directories exist ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","    for key, p in out.items():\n","        if \"dir\" in key:\n","            p.mkdir(parents=True, exist_ok=True)\n","        else:\n","            p.parent.mkdir(parents=True, exist_ok=True)\n","\n","    return out\n","\n","def _completed(paths: dict[str, Path], is_ssl: bool) -> bool:\n","    \"\"\"\n","    Returns True if all the necessary inference artifacts for this fold\n","    are already on disk, so we can skip evaluation.\n","    \"\"\"\n","    # Always require patch‚Äêlevel preds + metrics JSON\n","    required = [\"patch_preds\", \"metrics\"]\n","    # For SSL models also require MC logits and patient‚Äêlevel CSV\n","    if is_ssl:\n","        required += [\"mc_logits\", \"patient_preds\"]\n","    return all(paths[k].exists() for k in required)\n","\n","\n","def extract_patient_id(key: str) -> str:\n","    \"\"\"\n","    Extract patient ID from a key formatted like 'CLASS_HPxxxx_x_y'.\n","    \"\"\"\n","    parts = key.split(\"_\")\n","    return next((p for p in parts if p.startswith((\"HP\", \"H\"))), \"UNKNOWN\")\n","\n","\n","def ece(probs, labels):\n","    \"\"\"\n","    Expected Calibration Error (ECE) helper for quick access.\n","    \"\"\"\n","    return expected_calibration_error(probs, labels, n_bins=ECE_BINS)"],"metadata":{"id":"VkUXkgUyrR0e","executionInfo":{"status":"ok","timestamp":1752630952740,"user_tz":240,"elapsed":31,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Cell 6 ‚Äì Core Evaluation Functions\n","\n","import torch\n","import joblib\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import defaultdict, Counter\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    roc_auc_score,\n","    confusion_matrix,\n","    classification_report\n",")\n","\n","from utils.training_utils.metrics import (\n","    TemperatureScaler,\n","    expected_calibration_error,\n","    mc_dropout_statistics\n",")\n","from utils.training_utils.data_utils import load_classifier, parse_label_from_filename\n","from utils.training_utils.device_io import (\n","    get_latest_checkpoint,\n","    load_checkpoint,\n","    save_json\n",")\n","from utils.training_utils.registry import TRAINER_REGISTRY\n","from utils.training_utils.model_utils import mc_dropout_predictions\n","\n","def load_model_and_components(model_name: str, fold: int):\n","    \"\"\"\n","    Carica:\n","      - il modello PyTorch (encoder o rete completa) dal checkpoint\n","      - se SSL: il probe (clf + label encoder) e, se presente, il TemperatureScaler\n","    Restituisce: model, is_ssl, clf, le, temp_scaler\n","    \"\"\"\n","    paths   = _paths(model_name, fold)\n","    cfg_m   = cfg[\"models\"][model_name]\n","    is_ssl  = cfg_m[\"type\"] == \"ssl\"\n","    trainer = TRAINER_REGISTRY[model_name](cfg_m, cfg[\"data\"])\n","\n","    # 1Ô∏è‚É£ Scegli checkpoint\n","    ckpt = None\n","    if is_ssl and cfg.get(\"train_encoder_once\", False) and fold > 0:\n","        ckpt = get_latest_checkpoint(_paths(model_name, 0)[\"ckpt_dir\"])\n","        if ckpt is None:\n","            raise FileNotFoundError(f\"‚ùå Nessun checkpoint trovato in fold0 per {model_name}\")\n","        print(f\"   ‚ûî SSL+train_encoder_once: caricamento encoder da fold0 ‚Üí {ckpt.name}\")\n","    else:\n","        ckpt = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","        if ckpt is None:\n","            raise FileNotFoundError(f\"‚ùå Nessun checkpoint trovato in fold{fold} per {model_name}\")\n","        print(f\"   ‚ûî Caricamento checkpoint fold{fold} ‚Üí {ckpt.name}\")\n","\n","    # 2Ô∏è‚É£ Carica pesi\n","    if is_ssl:\n","        # prende sia encoder che eventuale proiettore\n","        full_model, _ = trainer.get_resume_model_and_optimizer()\n","        load_checkpoint(ckpt, model=full_model)\n","        # feature-extractor: preferisci `encoder`, altrimenti `model`\n","        feat_mod = getattr(trainer, \"encoder\", None) or getattr(trainer, \"model\", None)\n","        if feat_mod is None:\n","            raise AttributeError(f\"No feature submodule found on {trainer}\")\n","        model = feat_mod.to(device).eval()\n","    else:\n","        # supervisati e transfer espongono `trainer.model`\n","        load_checkpoint(ckpt, model=trainer.model)\n","        model = trainer.model.to(device).eval()\n","\n","    # 3Ô∏è‚É£ Se SSL: carica probe + temp_scaler\n","    clf = le = temp_scaler = None\n","    if is_ssl:\n","        clf, le = load_classifier(paths[\"clf\"])\n","        scaler_path = paths[\"scaler\"]\n","        if scaler_path.exists():\n","            obj = joblib.load(scaler_path)\n","            if isinstance(obj, TemperatureScaler):\n","                temp_scaler = obj\n","                print(\"   ‚ûî Loaded TemperatureScaler (calibrate probs)\")\n","        else:\n","            print(f\"   ‚ûî No TemperatureScaler found for {model_name} fold {fold}\")\n","\n","    # 4Ô∏è‚É£ Debug printout\n","    print(f\"   ‚ûî SSL pipeline?  {is_ssl}\")\n","    print(f\"   ‚ûî Classifier?    {clf is not None}\")\n","    print(f\"   ‚ûî Temp-scaler?   {temp_scaler is not None}\")\n","\n","    return model, is_ssl, clf, le, temp_scaler\n","\n","\n","def run_patch_inference(model, loader, is_ssl, clf, temp_scaler):\n","    \"\"\"\n","    Inferenzia patch-level. Restituisce:\n","      keys (dummy), y_true, y_pred, probs (calibrated if temp_scaler)\n","    \"\"\"\n","    keys, y_true, y_pred = [], [], []\n","    probs_list = []\n","\n","    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Patches\")):\n","        imgs, labels = batch\n","        imgs = imgs.to(device)\n","\n","        if is_ssl:\n","            with torch.no_grad():\n","                feats = model(imgs).cpu().numpy()\n","            raw_p = clf.predict_proba(feats)\n","        else:\n","            with torch.no_grad():\n","                logits = model(imgs)\n","                raw_p = torch.softmax(logits, dim=1).cpu().numpy()\n","\n","        # calibration\n","        if temp_scaler is not None:\n","            logits_for_cal = np.log(raw_p + 1e-12)\n","            p = temp_scaler.transform_proba(logits_for_cal)\n","        else:\n","            p = raw_p\n","\n","        preds = p.argmax(axis=1)\n","        t     = labels.cpu().numpy()\n","\n","        y_true.extend(t.tolist())\n","        y_pred.extend(preds.tolist())\n","        probs_list.append(p)\n","\n","        print(f\"   ‚Ä¢ Batch {batch_idx} done.\")\n","        if batch_idx == 1:\n","            print(\"üõë DEBUG: stopping after batch_idx=1\")\n","            break\n","\n","    probs = np.vstack(probs_list)\n","    return keys, np.array(y_true), np.array(y_pred), probs\n","\n","\n","def save_patch_outputs(model_name, fold, keys, y_true, y_pred, probs):\n","    torch.save({\n","        \"keys\": keys,\n","        \"true\": y_true,\n","        \"pred\": y_pred,\n","        \"probs\": probs\n","    }, _paths(model_name, fold)[\"patch_preds\"])\n","\n","\n","def save_mc_logits(model_name, fold, model, loader):\n","    mc = mc_dropout_predictions(model, loader, device=device, T=MC_PASSES)\n","    np.save(_paths(model_name, fold)[\"mc_logits\"], mc)\n","    return mc\n","\n","\n","def compute_and_save_metrics(model_name, fold, y_true, y_pred, probs, mc=None):\n","    \"\"\"\n","    Calcola tutte le metriche, MC-stats e ECE post-calibrazione.\n","    \"\"\"\n","    mc_stats = mc_dropout_statistics(mc) if mc is not None else {}\n","\n","    acc  = accuracy_score(y_true, y_pred)\n","    f1   = f1_score(y_true, y_pred, average=\"macro\")\n","    try:\n","        auc = roc_auc_score(y_true, y_pred, average=\"macro\", multi_class=\"ovo\")\n","    except ValueError:\n","        auc = None\n","    cm = confusion_matrix(y_true, y_pred).tolist()\n","    cr = classification_report(y_true, y_pred, output_dict=True)\n","\n","    mets = {\n","        \"accuracy\": acc,\n","        \"macro_f1\": f1,\n","        \"roc_auc\": auc,\n","        \"confusion_matrix\": cm,\n","        \"class_report\": cr,\n","        **mc_stats\n","    }\n","\n","    # ECE post-calibrazione\n","    mets[\"ece_post\"] = expected_calibration_error(probs, y_true, n_bins=ECE_BINS)\n","\n","    save_json(mets, _paths(model_name, fold)[\"metrics\"])\n","    return mets\n"],"metadata":{"id":"9PAs8FCDrTNz","executionInfo":{"status":"ok","timestamp":1752630952782,"user_tz":240,"elapsed":12,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Cell 7 ‚Äì Per-Fold Evaluation (aggiornato)\n","\n","from utils.training_utils.data_utils import default_transforms, build_loader\n","from utils.training_utils.device_io import get_latest_checkpoint\n","from collections import defaultdict, Counter\n","import numpy as np\n","import pandas as pd\n","\n","def aggregate_patient_results(model_name, fold, keys, y_pred, probs):\n","    \"\"\"\n","    Aggrega predizioni per paziente (majority voting) e salva CSV.\n","    \"\"\"\n","    by_pt = defaultdict(list)\n","    for k, y, conf in zip(keys, y_pred, probs.max(axis=1)):\n","        pid = extract_patient_id(k)\n","        by_pt[pid].append((y, conf))\n","\n","    rows = []\n","    for pid, recs in by_pt.items():\n","        votes, confs = zip(*recs)\n","        rows.append({\n","            \"patient_id\": pid,\n","            \"true_label\": parse_label_from_filename(pid),\n","            \"pred_label\": Counter(votes).most_common(1)[0][0],\n","            \"n_patches\": len(recs),\n","            \"mean_conf_raw\": float(np.mean(confs))\n","        })\n","\n","    pd.DataFrame(rows).to_csv(\n","        _paths(model_name, fold)[\"patient_preds\"], index=False\n","    )\n","\n","\n","def evaluate_fold(model_name: str, fold: int):\n","    print(f\"\\nüîç Evaluating {model_name} fold {fold}‚Ä¶\")\n","\n","    # 0Ô∏è‚É£ Paths e checkpoint\n","    paths = _paths(model_name, fold)\n","    ckpt  = get_latest_checkpoint(paths[\"ckpt_dir\"])\n","    if ckpt is None:\n","        print(f\"‚ö†Ô∏è Nessun checkpoint trovato per {model_name} fold {fold}, salto valutazione\")\n","        return\n","\n","    # 1Ô∏è‚É£ Skip se artefatti gi√† tutti presenti\n","    is_ssl = (cfg[\"models\"][model_name][\"type\"] == \"ssl\")\n","    if _completed(paths, is_ssl):\n","        print(f\"‚ö° Skipping {model_name} fold {fold}: artifacts already present\")\n","        return\n","\n","    # 2Ô∏è‚É£ Carica modello, probe, scaler\n","    print(\"üì• Loading model, classifier, scaler‚Ä¶\")\n","    model, is_ssl, clf, le, temp_scaler = load_model_and_components(model_name, fold)\n","    print(f\"   ‚ûî SSL pipeline?  {is_ssl}\")\n","    print(f\"   ‚ûî Classifier?    {clf is not None}\")\n","    print(f\"   ‚ûî Temp-scaler?   {temp_scaler is not None}\")\n","\n","    # 3Ô∏è‚É£ Build test loader\n","    patch_size = cfg[\"models\"][model_name].get(\"patch_size\", 224)\n","    batch_size = cfg[\"models\"][model_name][\"training\"][\"batch_size\"]\n","    test_rel   = cfg[\"data\"][\"test\"].format(\n","        fold_idx=fold,\n","        dataset_id=cfg[\"data\"][\"dataset_id\"]\n","    )\n","    test_wds   = (PROJECT_ROOT / test_rel).resolve()\n","    print(f\"üß™ Using test shard: {test_wds}\")\n","    assert test_wds.exists(), f\"‚ùå Test shard not found: {test_wds}\"\n","\n","    # ‚ùó mappatura globale delle classi (inclusa 'CHROMO' quindi)\n","    all_classes  = cfg[\"data\"][\"classes\"]\n","    class_to_idx = {cls: i for i, cls in enumerate(all_classes)}\n","\n","    loader = build_loader(\n","        str(test_wds),\n","        class_to_idx=class_to_idx,\n","        patch_size=patch_size,\n","        batch_size=batch_size,\n","        device=device,\n","        augment=False,\n","    )\n","    print(f\"üì¶ DataLoader ready with batch_size = {batch_size}\")\n","\n","    # 4Ô∏è‚É£ Patch-level inference\n","    print(\"‚ñ∂Ô∏è Running inference (patch-level)‚Ä¶\")\n","    keys, y_true, y_pred, probs = run_patch_inference(\n","        model, loader, is_ssl, clf, temp_scaler\n","    )\n","    print(f\"   ‚ûî Patches processed: {len(y_true)}\")\n","    save_patch_outputs(model_name, fold, keys, y_true, y_pred, probs)\n","    print(\"üíæ Saved patch-level outputs.\")\n","\n","    # 5Ô∏è‚É£ MC-Dropout (solo SSL)\n","    if is_ssl:\n","        print(\"üîÑ Running MC-Dropout‚Ä¶\")\n","        mc = save_mc_logits(model_name, fold, model, loader)\n","        print(\"üíæ Saved MC-Dropout logits.\")\n","    else:\n","        mc = None\n","\n","    # 6Ô∏è‚É£ Compute & save metrics\n","    print(\"üìä Computing metrics‚Ä¶\")\n","    compute_and_save_metrics(model_name, fold, y_true, y_pred, probs, mc)\n","    print(\"üíæ Saved metrics JSON.\")\n","\n","    # 7Ô∏è‚É£ Patient-level aggregation (solo SSL)\n","    if is_ssl:\n","        print(\"üë®‚Äç‚öïÔ∏è Aggregating patient-level results‚Ä¶\")\n","        aggregate_patient_results(model_name, fold, keys, y_pred, probs)\n","        print(\"üíæ Saved patient-level CSV.\")\n","\n","    print(f\"‚úÖ Done {model_name} fold {fold}\\n\")\n","\n","\n","# üöÄ Run evaluation for all models and folds\n","for model_name in cfg[\"run_models\"]:\n","    for fold_idx in cfg[\"folds\"]:\n","        evaluate_fold(model_name, fold_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":860},"id":"t_oHDrpWrU3x","executionInfo":{"status":"error","timestamp":1752630966875,"user_tz":240,"elapsed":14091,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}},"outputId":"d601e353-33c5-4179-8cc2-d17afb027c02"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîç Evaluating simclr fold 0‚Ä¶\n","‚ö° Skipping simclr fold 0: artifacts already present\n","\n","üîç Evaluating simclr fold 1‚Ä¶\n","‚ö†Ô∏è Nessun checkpoint trovato per simclr fold 1, salto valutazione\n","\n","üîç Evaluating jepa fold 0‚Ä¶\n","‚ö° Skipping jepa fold 0: artifacts already present\n","\n","üîç Evaluating jepa fold 1‚Ä¶\n","‚ö†Ô∏è Nessun checkpoint trovato per jepa fold 1, salto valutazione\n","\n","üîç Evaluating moco_v2 fold 0‚Ä¶\n","‚ö†Ô∏è Nessun checkpoint trovato per moco_v2 fold 0, salto valutazione\n","\n","üîç Evaluating moco_v2 fold 1‚Ä¶\n","‚ö†Ô∏è Nessun checkpoint trovato per moco_v2 fold 1, salto valutazione\n","\n","üîç Evaluating rotation fold 0‚Ä¶\n","‚ö° Skipping rotation fold 0: artifacts already present\n","\n","üîç Evaluating rotation fold 1‚Ä¶\n","‚ö†Ô∏è Nessun checkpoint trovato per rotation fold 1, salto valutazione\n","\n","üîç Evaluating supervised fold 0‚Ä¶\n","üì• Loading model, classifier, scaler‚Ä¶\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"stream","name":"stdout","text":["   ‚ûî Caricamento checkpoint fold0 ‚Üí SupervisedTrainer_bestepoch001.pt\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([5, 2048]) from checkpoint, the shape in current model is torch.Size([0, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([0]).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-7-829524257.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"folds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mevaluate_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-7-829524257.py\u001b[0m in \u001b[0;36mevaluate_fold\u001b[0;34m(model_name, fold)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# 2Ô∏è‚É£ Carica modello, probe, scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üì• Loading model, classifier, scaler‚Ä¶\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   ‚ûî SSL pipeline?  {is_ssl}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   ‚ûî Classifier?    {clf is not None}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-6-3865164236.py\u001b[0m in \u001b[0;36mload_model_and_components\u001b[0;34m(model_name, fold)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# supervisati e transfer espongono `trainer.model`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/src/utils/training_utils/device_io.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(path, model, optimizer)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"optim\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([5, 2048]) from checkpoint, the shape in current model is torch.Size([0, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([0])."]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed/dataset_9f30917e/webdataset/\n"],"metadata":{"id":"BLBMOnmBzfsa","executionInfo":{"status":"aborted","timestamp":1752630966894,"user_tz":240,"elapsed":29,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 8 ‚Äì Fold-Level & Experiment-Level Aggregation\n","def aggregate_model(model_name: str):\n","    # collect per-fold metrics\n","    all_m = []\n","    for f in cfg[\"folds\"]:\n","        p = _paths(model_name, f)[\"metrics\"]\n","        all_m.append(json.load(open(p)))\n","    summary = aggregate_fold_metrics(all_m)\n","    save_json(_paths(model_name,0)[\"agg_metrics\"], summary)\n","    # plot heatmap of means\n","    dfm = pd.DataFrame(all_m)\n","    fig, ax = plt.subplots(figsize=(6,4))\n","    sns.heatmap(dfm.mean()[[\"accuracy\",\"macro_f1\",\"roc_auc\"]].to_frame().T,\n","                annot=True, fmt=\".3f\", ax=ax)\n","    fig.savefig(_paths(model_name,0)[\"agg_summary\"], bbox_inches=\"tight\")\n","    plt.close(fig)\n","    print(f\"Aggregated {model_name}\")\n","\n","def aggregate_experiment():\n","    models = cfg[\"run_models\"]\n","    rows = []\n","    for m in models:\n","        met = json.load(open(_paths(m,0)[\"agg_metrics\"]))\n","        row = {\"model\": m}\n","        for k,v in met.items(): row[k] = v[\"mean\"]\n","        rows.append(row)\n","    save_json(_paths(models[0],0)[\"exp_json\"], rows)\n","    df = pd.DataFrame(rows)\n","    fig, ax = plt.subplots(figsize=(8,4))\n","    sns.barplot(data=df, x=\"model\", y=\"accuracy\", ax=ax)\n","    ax.set_title(\"Model Accuracy Comparison\")\n","    fig.savefig(_paths(models[0],0)[\"exp_img\"], bbox_inches=\"tight\")\n","    plt.close(fig)\n","    print(\"Experiment-level comparison complete\")\n","\n","# run aggregations\n","for m in cfg[\"run_models\"]:\n","    aggregate_model(m)\n","aggregate_experiment()\n"],"metadata":{"id":"9l1_lvR3rcQ8","executionInfo":{"status":"aborted","timestamp":1752630966899,"user_tz":240,"elapsed":26,"user":{"displayName":"Stefano Bisignano","userId":"12037847569875379268"}}},"execution_count":null,"outputs":[]}]}