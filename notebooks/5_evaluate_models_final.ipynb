{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "848a37f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "848a37f5",
        "outputId": "06c2b91d-e0b8-46bd-eddc-d575f49f6e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "!pip install --quiet torch torchvision webdataset tqdm pillow scikit-learn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "262cef02",
      "metadata": {
        "id": "262cef02"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import csv, logging, joblib, sys, importlib, yaml, torch, numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "# ğŸ“ Configurazione percorso progetto\n",
        "config_path = Path('/content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/config/training.yaml')\n",
        "with config_path.open('r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "colab_root = Path(cfg['env_paths']['colab'])\n",
        "local_root = Path(cfg['env_paths']['local'])\n",
        "PROJECT_ROOT = colab_root if colab_root.exists() else local_root\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "\n",
        "trainer_modules = [\n",
        "    \"trainers.simclr\",\n",
        "    \"trainers.moco_v2\",\n",
        "    \"trainers.rotation\",\n",
        "    \"trainers.jigsaw\",\n",
        "    \"trainers.supervised\",\n",
        "    \"trainers.transfer\",\n",
        "]\n",
        "for m in trainer_modules:\n",
        "    if m in sys.modules:\n",
        "        importlib.reload(sys.modules[m])\n",
        "    else:\n",
        "        importlib.import_module(m)\n",
        "from utils.training_utils import TRAINER_REGISTRY, load_checkpoint\n",
        "# Normalizza i path dei dati\n",
        "for split in ['train','val','test']:\n",
        "    rel = cfg['data'].get(split)\n",
        "    if rel:\n",
        "        cfg['data'][split] = str(PROJECT_ROOT / rel)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"EVAL\")\n",
        "\n",
        "# ğŸ“ Setup esperimento specifico\n",
        "experiment_dir = PROJECT_ROOT / \"data/processed2/dataset_9f30917e/experiments/prova\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def extract_patient_id(key: str) -> str:\n",
        "    for p in key.split(\"_\"):\n",
        "        if p.startswith(\"HP\") or p.startswith(\"H\"):\n",
        "            return p\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "def extract_label_from_key(key: str) -> str:\n",
        "    return \"not_tumor\" if key.startswith(\"not_tumor\") else key.split(\"_\")[0]\n",
        "\n",
        "def compute_metrics(keys, y_pred, le):\n",
        "    # ground-truth per paziente\n",
        "    all_labels = defaultdict(list)\n",
        "    for k in keys:\n",
        "        all_labels[extract_patient_id(k)].append(extract_label_from_key(k))\n",
        "    true_labels = {}\n",
        "    for pid, labs in all_labels.items():\n",
        "        tumor = [l for l in labs if l!=\"not_tumor\"]\n",
        "        if len(set(tumor))==1:\n",
        "            true_labels[pid]=tumor[0]\n",
        "    # votazioni\n",
        "    preds = defaultdict(list)\n",
        "    for k,p in zip(keys, y_pred):\n",
        "        pid=extract_patient_id(k)\n",
        "        if le.classes_[p]!=\"not_tumor\":\n",
        "            preds[pid].append(p)\n",
        "    y_true, y_maj, valid = [], [], []\n",
        "    for pid,votes in preds.items():\n",
        "        if pid in true_labels and votes:\n",
        "            gt = true_labels[pid]\n",
        "            maj = Counter(votes).most_common(1)[0][0]\n",
        "            y_true.append(le.transform([gt])[0])\n",
        "            y_maj.append(maj)\n",
        "            valid.append(pid)\n",
        "    if not y_true:\n",
        "        raise RuntimeError(\"Nessun paziente valutabile\")\n",
        "    report = classification_report(y_true, y_maj, target_names=[c for c in le.classes_ if c!=\"not_tumor\"])\n",
        "    cm     = confusion_matrix(y_true, y_maj)\n",
        "    acc    = np.mean(np.array(y_true)==np.array(y_maj))\n",
        "    f1     = f1_score(y_true, y_maj, average=\"macro\")\n",
        "    prec   = precision_score(y_true, y_maj, average=\"macro\")\n",
        "    rec    = recall_score(y_true, y_maj, average=\"macro\")\n",
        "    return dict(\n",
        "        y_true=y_true, y_maj=y_maj, valid=valid,\n",
        "        report=report, cm=cm,\n",
        "        metrics=(acc,f1,prec,rec),\n",
        "    )\n",
        "\n",
        "def write_md_log(save_dir: Path, model_name: str, cm, report: str, valid: list[str], metrics: tuple[float,float,float,float], y_true: list[int], y_maj: list[int], le):\n",
        "    acc,f1,prec,rec = metrics\n",
        "    md = save_dir/\"evals_log.md\"\n",
        "    with open(md, \"w\") as f:\n",
        "        f.write(f\"# ğŸ§  Modello: {model_name}\\n\\n\")\n",
        "        f.write(\"## ğŸ“Š Risultati Majority Voting (paziente-level)\\n```text\\n\")\n",
        "        f.write(report)\n",
        "        f.write(\"\\n```\\n\\n\")\n",
        "        f.write(\"## ğŸ“‰ Confusion Matrix\\n```text\\n\")\n",
        "        f.write(str(cm))\n",
        "        f.write(\"\\n```\\n\\n\")\n",
        "        f.write(f\"âœ… Totale pazienti classificati: {len(valid)}\\n\\n\")\n",
        "        # sezione per-paziente\n",
        "        f.write(\"## ğŸ§¾ Predizione per paziente\\n```text\\n\")\n",
        "        for pid, t_enc, p_enc in zip(valid, y_true, y_maj):\n",
        "            true_lbl = le.inverse_transform([t_enc])[0]\n",
        "            pred_lbl = le.inverse_transform([p_enc])[0]\n",
        "            f.write(f\"Paziente {pid}: predetto = {pred_lbl} | reale = {true_lbl}\\n\")\n",
        "        f.write(\"```\\n\\n\")\n",
        "        f.write(\"## ğŸ“ˆ Metriche sintetiche\\n\")\n",
        "        f.write(f\"- Accuracy        : {acc:.4f}\\n\")\n",
        "        f.write(f\"- Macro F1        : {f1:.4f}\\n\")\n",
        "        f.write(f\"- Macro Precision : {prec:.4f}\\n\")\n",
        "        f.write(f\"- Macro Recall    : {rec:.4f}\\n\")\n",
        "\n",
        "def append_summary_csv(common_csv:Path, model_name:str, metrics):\n",
        "    if not common_csv.exists():\n",
        "        with open(common_csv, \"w\", newline=\"\") as f:\n",
        "            writer=csv.writer(f)\n",
        "            writer.writerow([\"Model\",\"Accuracy\",\"Macro F1\",\"Macro Precision\",\"Macro Recall\",\"N_Patients\"])\n",
        "    acc,f1,prec,rec = metrics\n",
        "    # N_Patients Ã¨ giÃ  nel report: la lunghezza di valid\n",
        "    writer=csv.writer(open(common_csv,\"a\",newline=\"\"))\n",
        "    writer.writerow([model_name,acc,f1,prec,rec])\n"
      ],
      "metadata": {
        "id": "Qd2xfrAyuBAZ"
      },
      "id": "Qd2xfrAyuBAZ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "309c640e",
      "metadata": {
        "id": "309c640e"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from utils.training_utils import TRAINER_REGISTRY, load_checkpoint\n",
        "from trainers.extract_features import extract_features\n",
        "import webdataset as wds\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "def make_loader(test_path):\n",
        "    ds = (\n",
        "        wds.WebDataset(\n",
        "            test_path,\n",
        "            shardshuffle=False,\n",
        "            handler=wds.warn_and_continue,\n",
        "            empty_check=False,            # evita generator vuoti\n",
        "        )\n",
        "        .decode(\"pil\")\n",
        "        .map(lambda s: {\n",
        "            # se non trova immagini, next torna None â†’ fallirÃ  convert, ma almeno non StopIteration\n",
        "            \"img\": T.ToTensor()(\n",
        "                next((v for v in s.values() if isinstance(v, Image.Image)), None)\n",
        "                .convert(\"RGB\")\n",
        "            ),\n",
        "            # se non trova key *.jpg, torna stringa vuota\n",
        "            \"key\": s[\"__key__\"]\n",
        "                   + \".\"\n",
        "                   + next((k for k in s.keys() if k.endswith(\".jpg\")), \"\")\n",
        "        })\n",
        "    )\n",
        "    return torch.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=64,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate_model(trainer, test_path: str, save_dir: Path, model_name: str, ssl: bool):\n",
        "    # 1) carica il checkpoint migliore\n",
        "    ckpts = sorted(save_dir.glob(f\"*Trainer_best_epoch*.pt\"))\n",
        "    ckpt  = ckpts[-1]\n",
        "\n",
        "    # 2) ricarica i pesi\n",
        "    if ssl:\n",
        "        import torch.nn as nn\n",
        "        # --- SimCLR / MoCo: encoder + projector\n",
        "        if hasattr(trainer, \"encoder\") and hasattr(trainer, \"projector\"):\n",
        "            seq = nn.Sequential(trainer.encoder, trainer.projector)\n",
        "            load_checkpoint(ckpt, model=seq)\n",
        "            trainer.encoder, trainer.projector = seq[0], seq[1]\n",
        "        # --- Jigsaw: encoder + head\n",
        "        elif hasattr(trainer, \"encoder\") and hasattr(trainer, \"head\"):\n",
        "            seq = nn.Sequential(trainer.encoder, trainer.head)\n",
        "            load_checkpoint(ckpt, model=seq)\n",
        "            trainer.encoder, trainer.head = seq[0], seq[1]\n",
        "        # --- Rotation: usa direttamente trainer.model\n",
        "        else:\n",
        "            load_checkpoint(ckpt, model=trainer.model)\n",
        "    else:\n",
        "        # supervised/transfer\n",
        "        load_checkpoint(ckpt, model=trainer.model)\n",
        "\n",
        "    # 3) carica il classificatore per ssl\n",
        "    if ssl:\n",
        "        clf_data = joblib.load(save_dir / f\"{model_name}_classifier.joblib\")\n",
        "        clf, le = clf_data[\"model\"], clf_data[\"label_encoder\"]\n",
        "        # sceglie feature extractor: encoder se presente, altrimenti model\n",
        "        if hasattr(trainer, \"encoder\"):\n",
        "            feat_mod = trainer.encoder\n",
        "        else:\n",
        "            feat_mod = trainer.model\n",
        "    else:\n",
        "        clf, le = None, trainer.label_encoder\n",
        "        feat_mod  = trainer.model\n",
        "\n",
        "    feat_mod = feat_mod.to(trainer.device)\n",
        "\n",
        "    # 4) estrai le predizioni\n",
        "    loader = make_loader(test_path)\n",
        "    if ssl:\n",
        "        feats = extract_features(feat_mod, loader, trainer.device)\n",
        "        X, keys = feats[\"features\"].numpy(), feats[\"keys\"]\n",
        "        y_pred = clf.predict(X)\n",
        "    else:\n",
        "        y_pred, keys = [], []\n",
        "        for batch in loader:\n",
        "            imgs = batch[\"img\"].to(trainer.device)\n",
        "            logits = feat_mod(imgs)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            y_pred += preds.tolist()\n",
        "            keys   += batch[\"key\"]\n",
        "\n",
        "    # 5) metriche, log e CSV\n",
        "    out = compute_metrics(keys, y_pred, le)\n",
        "    write_md_log(save_dir, model_name, out[\"cm\"], out[\"report\"], out[\"valid\"], out[\"metrics\"], out[\"y_true\"], out[\"y_maj\"], le)\n",
        "    append_summary_csv(save_dir.parent / \"evaluation_summary_all_models.csv\", model_name, out[\"metrics\"])\n",
        "    print(f\"âœ”ï¸ Finished eval for {model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "af011c17",
      "metadata": {
        "id": "af011c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8c3256-0d7b-4617-bb05-ce79dd2308d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "Extracting features: 8it [01:06,  8.27s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ”ï¸ Finished eval for simclr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "Extracting features: 8it [00:54,  6.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ”ï¸ Finished eval for moco_v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 8it [00:53,  6.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ”ï¸ Finished eval for rotation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "Extracting features: 8it [00:52,  6.51s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ”ï¸ Finished eval for jigsaw\n",
            "âœ”ï¸ Finished eval for supervised\n",
            "âœ”ï¸ Finished eval for transfer\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€â”€ Main loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "run_model = cfg.get(\"run_model\",\"all\").lower()\n",
        "models = cfg[\"models\"].items() if run_model==\"all\" else [(run_model,cfg[\"models\"][run_model])]\n",
        "for name, m_cfg in models:\n",
        "    if name not in TRAINER_REGISTRY:\n",
        "        logger.warning(f\"Trainer '{name}' non trovato, skip.\")\n",
        "        continue\n",
        "    trainer = TRAINER_REGISTRY[name](m_cfg, cfg[\"data\"])\n",
        "    test_p = str(cfg[\"data\"][\"test\"])\n",
        "    model_dir = experiment_dir/name\n",
        "    if not model_dir.exists():\n",
        "        logger.warning(f\"Dir {model_dir} mancante, skip.\")\n",
        "        continue\n",
        "    is_ssl = (name not in [\"supervised\",\"transfer\"])\n",
        "    evaluate_model(trainer, test_p, model_dir, name, ssl=is_ssl)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}