{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "848a37f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "848a37f5",
        "outputId": "06c2b91d-e0b8-46bd-eddc-d575f49f6e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "!pip install --quiet torch torchvision webdataset tqdm pillow scikit-learn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "262cef02",
      "metadata": {
        "id": "262cef02"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import csv, logging, joblib, sys, importlib, yaml, torch, numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "# üìÅ Configurazione percorso progetto\n",
        "config_path = Path('/content/drive/MyDrive/Colab Notebooks/MLA_PROJECT/wsi-ssrl-rcc_project/config/training.yaml')\n",
        "with config_path.open('r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "colab_root = Path(cfg['env_paths']['colab'])\n",
        "local_root = Path(cfg['env_paths']['local'])\n",
        "PROJECT_ROOT = colab_root if colab_root.exists() else local_root\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
        "\n",
        "\n",
        "trainer_modules = [\n",
        "    \"trainers.simclr\",\n",
        "    \"trainers.moco_v2\",\n",
        "    \"trainers.rotation\",\n",
        "    \"trainers.jigsaw\",\n",
        "    \"trainers.supervised\",\n",
        "    \"trainers.transfer\",\n",
        "]\n",
        "for m in trainer_modules:\n",
        "    if m in sys.modules:\n",
        "        importlib.reload(sys.modules[m])\n",
        "    else:\n",
        "        importlib.import_module(m)\n",
        "from utils.training_utils import TRAINER_REGISTRY, load_checkpoint\n",
        "# Normalizza i path dei dati\n",
        "for split in ['train','val','test']:\n",
        "    rel = cfg['data'].get(split)\n",
        "    if rel:\n",
        "        cfg['data'][split] = str(PROJECT_ROOT / rel)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"EVAL\")\n",
        "yaml_exp = cfg.get(\"exp_code\", \"\")\n",
        "if yaml_exp:\n",
        "    EXP_CODE = yaml_exp\n",
        "else:\n",
        "  EXP_CODE = \"error\"\n",
        "# üìÅ Setup esperimento specifico\n",
        "experiment_dir = PROJECT_ROOT / \"data/processed2/dataset_9f30917e/experiments\" / EXP_CODE\n",
        "print(f\"üìÅ Esperimento in {experiment_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qd2xfrAyuBAZ",
      "metadata": {
        "id": "Qd2xfrAyuBAZ"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def extract_patient_id(key: str) -> str:\n",
        "    for p in key.split(\"_\"):\n",
        "        if p.startswith(\"HP\") or p.startswith(\"H\"):\n",
        "            return p\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "def extract_label_from_key(key: str) -> str:\n",
        "    return \"not_tumor\" if key.startswith(\"not_tumor\") else key.split(\"_\")[0]\n",
        "\n",
        "def compute_metrics(keys, y_pred, le):\n",
        "    # ground-truth per paziente\n",
        "    all_labels = defaultdict(list)\n",
        "    for k in keys:\n",
        "        all_labels[extract_patient_id(k)].append(extract_label_from_key(k))\n",
        "    true_labels = {}\n",
        "    for pid, labs in all_labels.items():\n",
        "        tumor = [l for l in labs if l!=\"not_tumor\"]\n",
        "        if len(set(tumor))==1:\n",
        "            true_labels[pid]=tumor[0]\n",
        "    # votazioni\n",
        "    preds = defaultdict(list)\n",
        "    for k,p in zip(keys, y_pred):\n",
        "        pid=extract_patient_id(k)\n",
        "        if le.classes_[p]!=\"not_tumor\":\n",
        "            preds[pid].append(p)\n",
        "    y_true, y_maj, valid = [], [], []\n",
        "    for pid,votes in preds.items():\n",
        "        if pid in true_labels and votes:\n",
        "            gt = true_labels[pid]\n",
        "            maj = Counter(votes).most_common(1)[0][0]\n",
        "            y_true.append(le.transform([gt])[0])\n",
        "            y_maj.append(maj)\n",
        "            valid.append(pid)\n",
        "    if not y_true:\n",
        "        raise RuntimeError(\"Nessun paziente valutabile\")\n",
        "    report = classification_report(y_true, y_maj, target_names=[c for c in le.classes_ if c!=\"not_tumor\"])\n",
        "    cm     = confusion_matrix(y_true, y_maj)\n",
        "    acc    = np.mean(np.array(y_true)==np.array(y_maj))\n",
        "    f1     = f1_score(y_true, y_maj, average=\"macro\")\n",
        "    prec   = precision_score(y_true, y_maj, average=\"macro\")\n",
        "    rec    = recall_score(y_true, y_maj, average=\"macro\")\n",
        "    return dict(\n",
        "        y_true=y_true, y_maj=y_maj, valid=valid,\n",
        "        report=report, cm=cm,\n",
        "        metrics=(acc,f1,prec,rec),\n",
        "    )\n",
        "\n",
        "def write_md_log(save_dir: Path, model_name: str, cm, report: str, valid: list[str], metrics: tuple[float,float,float,float], y_true: list[int], y_maj: list[int], le):\n",
        "    acc,f1,prec,rec = metrics\n",
        "    md = save_dir/\"evals_log.md\"\n",
        "    with open(md, \"w\") as f:\n",
        "        f.write(f\"# üß† Modello: {model_name}\\n\\n\")\n",
        "        f.write(\"## üìä Risultati Majority Voting (paziente-level)\\n```text\\n\")\n",
        "        f.write(report)\n",
        "        f.write(\"\\n```\\n\\n\")\n",
        "        f.write(\"## üìâ Confusion Matrix\\n```text\\n\")\n",
        "        f.write(str(cm))\n",
        "        f.write(\"\\n```\\n\\n\")\n",
        "        f.write(f\"‚úÖ Totale pazienti classificati: {len(valid)}\\n\\n\")\n",
        "        # sezione per-paziente\n",
        "        f.write(\"## üßæ Predizione per paziente\\n```text\\n\")\n",
        "        for pid, t_enc, p_enc in zip(valid, y_true, y_maj):\n",
        "            true_lbl = le.inverse_transform([t_enc])[0]\n",
        "            pred_lbl = le.inverse_transform([p_enc])[0]\n",
        "            f.write(f\"Paziente {pid}: predetto = {pred_lbl} | reale = {true_lbl}\\n\")\n",
        "        f.write(\"```\\n\\n\")\n",
        "        f.write(\"## üìà Metriche sintetiche\\n\")\n",
        "        f.write(f\"- Accuracy        : {acc:.4f}\\n\")\n",
        "        f.write(f\"- Macro F1        : {f1:.4f}\\n\")\n",
        "        f.write(f\"- Macro Precision : {prec:.4f}\\n\")\n",
        "        f.write(f\"- Macro Recall    : {rec:.4f}\\n\")\n",
        "\n",
        "def append_summary_csv(common_csv:Path, model_name:str, metrics):\n",
        "    if not common_csv.exists():\n",
        "        with open(common_csv, \"w\", newline=\"\") as f:\n",
        "            writer=csv.writer(f)\n",
        "            writer.writerow([\"Model\",\"Accuracy\",\"Macro F1\",\"Macro Precision\",\"Macro Recall\",\"N_Patients\"])\n",
        "    acc,f1,prec,rec = metrics\n",
        "    # N_Patients √® gi√† nel report: la lunghezza di valid\n",
        "    writer=csv.writer(open(common_csv,\"a\",newline=\"\"))\n",
        "    writer.writerow([model_name,acc,f1,prec,rec])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309c640e",
      "metadata": {
        "id": "309c640e"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ Evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from utils.training_utils import TRAINER_REGISTRY, load_checkpoint\n",
        "from trainers.extract_features import extract_features\n",
        "import webdataset as wds\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import joblib\n",
        "import torch\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "def make_loader(test_path: str, batch_size: int = 64):\n",
        "    ds = (\n",
        "        wds.WebDataset(test_path,\n",
        "                       shardshuffle=False,\n",
        "                       handler=wds.warn_and_continue,\n",
        "                       empty_check=False)\n",
        "         .decode(\"pil\")\n",
        "         .map(lambda s: {\n",
        "             \"img\": T.ToTensor()(\n",
        "                 next((v for v in s.values() if isinstance(v, Image.Image)), None)\n",
        "                 .convert(\"RGB\")\n",
        "             ),\n",
        "             \"key\": s[\"__key__\"] + \".\" + next((k for k in s.keys() if k.endswith(\".jpg\")), \"\")\n",
        "         })\n",
        "    )\n",
        "    return torch.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "def evaluate_model(trainer, test_path: str, save_dir: Path, model_name: str, ssl: bool):\n",
        "    eval_md = save_dir / \"evals_log.md\"\n",
        "    if eval_md.exists():\n",
        "        logger.info(f\"üìÑ evals_log.md gi√† presente in {save_dir.name}, skip evaluation.\")\n",
        "        print(f\"\\nüìÑ evals_log.md gi√† presente per {model_name} ===\")\n",
        "        return\n",
        "    # 1) checkpoint migliore\n",
        "    ckpt = sorted(save_dir.glob(\"*Trainer_best_epoch*.pt\"))[-1]\n",
        "\n",
        "    # 2) carica SOLO l'encoder per i SSL\n",
        "    if ssl:\n",
        "        sd = torch.load(ckpt, map_location=\"cpu\")[\"model_state_dict\"]\n",
        "        # estrai solo i parametri che iniziano con \"0.\"\n",
        "        enc_sd = {k.split(\".\",1)[1]: v for k, v in sd.items() if k.startswith(\"0.\")}\n",
        "        # scegli il modulo encoder corretto\n",
        "        if hasattr(trainer, \"encoder_q\"):\n",
        "            trainer.encoder_q.load_state_dict(enc_sd)\n",
        "            feat_mod = trainer.encoder_q\n",
        "        elif hasattr(trainer, \"encoder\"):\n",
        "            trainer.encoder.load_state_dict(enc_sd)\n",
        "            feat_mod = trainer.encoder\n",
        "        else:\n",
        "            # Rotation ha solo .model\n",
        "            load_checkpoint(ckpt, model=trainer.model)\n",
        "            feat_mod = trainer.model\n",
        "    else:\n",
        "        # supervised / transfer\n",
        "        load_checkpoint(ckpt, model=trainer.model)\n",
        "        feat_mod = trainer.model\n",
        "\n",
        "    feat_mod = feat_mod.to(trainer.device).eval()\n",
        "\n",
        "    # 3) classificatore o label-encoder\n",
        "    if ssl:\n",
        "        clf_data = joblib.load(save_dir / f\"{model_name}_classifier.joblib\")\n",
        "        clf, le, scaler = (\n",
        "            clf_data[\"model\"],\n",
        "            clf_data[\"label_encoder\"],\n",
        "            clf_data.get(\"scaler\", None),\n",
        "        )\n",
        "    else:\n",
        "        le, scaler = trainer.label_encoder, None\n",
        "\n",
        "    # 4) DataLoader\n",
        "    loader = make_loader(test_path)\n",
        "\n",
        "    # 5) estrai predizioni\n",
        "    if ssl:\n",
        "        feats = extract_features(feat_mod, loader, trainer.device)\n",
        "        X, keys = feats[\"features\"].numpy(), feats[\"keys\"]\n",
        "        if scaler: X = scaler.transform(X)\n",
        "        y_pred = clf.predict(X)\n",
        "\n",
        "        # patch-level report\n",
        "        true_patch = [extract_label_from_key(k) for k in keys]\n",
        "        pred_patch = [le.classes_[p] for p in y_pred]\n",
        "        print(f\"\\n=== Patch-level report per {model_name} ===\")\n",
        "        print(classification_report(\n",
        "            true_patch, pred_patch,\n",
        "            labels=[c for c in le.classes_ if c!=\"not_tumor\"],\n",
        "            target_names=[c for c in le.classes_ if c!=\"not_tumor\"]\n",
        "        ))\n",
        "\n",
        "    else:\n",
        "        # streaming per-paziente\n",
        "        keys, y_pred = [], []\n",
        "        true_labels  = {}\n",
        "        vote_counts  = defaultdict(Counter)\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                imgs = batch[\"img\"].to(trainer.device)\n",
        "                ks   = batch[\"key\"]\n",
        "                logits = feat_mod(imgs)\n",
        "                preds  = torch.argmax(logits, dim=1).cpu().tolist()\n",
        "                for k,p in zip(ks, preds):\n",
        "                    keys.append(k); y_pred.append(p)\n",
        "                    pid = extract_patient_id(k)\n",
        "                    lbl = extract_label_from_key(k)\n",
        "                    if lbl!=\"not_tumor\" and pid not in true_labels:\n",
        "                        true_labels[pid] = lbl\n",
        "                    if le.inverse_transform([p])[0]!=\"not_tumor\":\n",
        "                        vote_counts[pid][p] += 1\n",
        "\n",
        "    # 6) metriche & log per tutti i modelli\n",
        "    out = compute_metrics(keys, y_pred, le)\n",
        "    write_md_log(\n",
        "        save_dir, model_name,\n",
        "        out[\"cm\"], out[\"report\"],\n",
        "        out[\"valid\"], out[\"metrics\"],\n",
        "        out[\"y_true\"], out[\"y_maj\"], le\n",
        "    )\n",
        "    append_summary_csv(\n",
        "        save_dir.parent / \"evaluation_summary_all_models.csv\",\n",
        "        model_name, out[\"metrics\"]\n",
        "    )\n",
        "    print(f\"‚úîÔ∏è Finished eval for {model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af011c17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af011c17",
        "outputId": "4f8c3256-0d7b-4617-bb05-ce79dd2308d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "Extracting features: 8it [01:06,  8.27s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úîÔ∏è Finished eval for simclr\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "Extracting features: 8it [00:54,  6.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úîÔ∏è Finished eval for moco_v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 8it [00:53,  6.65s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úîÔ∏è Finished eval for rotation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "Extracting features: 8it [00:52,  6.51s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úîÔ∏è Finished eval for jigsaw\n",
            "‚úîÔ∏è Finished eval for supervised\n",
            "‚úîÔ∏è Finished eval for transfer\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ Main loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "run_model = cfg.get(\"run_model\",\"all\").lower()\n",
        "models = cfg[\"models\"].items() if run_model==\"all\" else [(run_model,cfg[\"models\"][run_model])]\n",
        "for name, m_cfg in models:\n",
        "    if name not in TRAINER_REGISTRY:\n",
        "        logger.warning(f\"Trainer '{name}' non trovato, skip.\")\n",
        "        continue\n",
        "    trainer = TRAINER_REGISTRY[name](m_cfg, cfg[\"data\"])\n",
        "    test_p = str(cfg[\"data\"][\"test\"])\n",
        "    model_dir = experiment_dir/name\n",
        "    if not model_dir.exists():\n",
        "        logger.warning(f\"Dir {model_dir} mancante, skip.\")\n",
        "        continue\n",
        "    is_ssl = (name not in [\"supervised\",\"transfer\"])\n",
        "    evaluate_model(trainer, test_p, model_dir, name, ssl=is_ssl)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
