{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQPBoCuOsFeoXcehD/Ad5K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pvqOlD3ymH1U"},"outputs":[],"source":["# Cell 1 – Imports & Configuration\n","import os\n","import sys\n","from pathlib import Path\n","\n","import torch\n","import clip\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# Optional: seaborn style for nicer plots\n","import seaborn as sns\n","sns.set(style=\"whitegrid\")\n","\n","# Device\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Paths (adapt to your project structure)\n","DATA_ROOT   = Path(\"/content/drive/MyDrive/ColabNotebooks/wsi-ssrl-rcc_project/data/processed\")\n","EXPERIMENT  = \"dataset_9f30917e/experiments/20250712091227\"\n","PATCH_DIR   = DATA_ROOT / EXPERIMENT / \"simclr/fold0/training/patches\"  # or wherever your .jpg patches live\n","OUTPUT_DIR  = Path(\"explainability_outputs\")\n","OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n","\n","print(f\"Using device:    {DEVICE}\")\n","print(f\"Patches folder:  {PATCH_DIR}\")\n","print(f\"Results to:      {OUTPUT_DIR}\")\n"]},{"cell_type":"code","source":["# Cell 2 – Load CLIP & (Optional) Classification Model\n","# CLIP for concept matching\n","clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n","clip_model.eval()\n","\n","# If you also want to overlay GradCAM on your SSL classifier:\n","# from torchvision import models\n","# clf = torch.load(Path(EXPERIMENT)/\"simclr/fold0/training/simclr_bestepoch002_fold0.pt\")[\"model\"]\n","# clf = clf.to(DEVICE).eval()\n"],"metadata":{"id":"Bp1bWoNjmPJm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 3 – Define & Encode Pathology Concepts\n","CONCEPTS = [\n","    \"necrosis\",\n","    \"clear cytoplasm\",\n","    \"mitotic figures\",\n","    \"nuclear atypia\",\n","    \"fibrovascular stroma\",\n","    \"inflammatory infiltrate\",\n","    \"blood vessels\",\n","    \"cellular pleomorphism\",\n","    \"stromal cells\",\n","    \"tumor cell clusters\",\n","    \"normal kidney tissue\",\n","    \"glomerulus\",\n","    \"tubules\",\n","    \"fibrosis\",\n","    \"calcification\",\n","    \"hemorrhage\",\n","]\n","\n","with torch.no_grad():\n","    tokens = clip.tokenize(CONCEPTS).to(DEVICE)\n","    concept_embeddings = clip_model.encode_text(tokens)\n","    concept_embeddings /= concept_embeddings.norm(dim=-1, keepdim=True)\n","\n","print(f\"→ Encoded {len(CONCEPTS)} concepts into CLIP embeddings\")\n"],"metadata":{"id":"9UjtN_FimSH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 4 – Patch‐Level Concept Explanation\n","def explain_patch_concepts(\n","    patch: Image.Image,\n","    clip_model,\n","    clip_preprocess,\n","    concept_embeddings: torch.Tensor,\n","    concepts: list[str],\n","    top_k: int = 3,\n",") -> list[tuple[str, float]]:\n","    \"\"\"\n","    Return top_k (concept, cosine_score) for a given image patch.\n","    \"\"\"\n","    # preprocess & encode\n","    x = clip_preprocess(patch).unsqueeze(0).to(DEVICE)\n","    with torch.no_grad():\n","        img_emb = clip_model.encode_image(x)\n","        img_emb /= img_emb.norm(dim=-1, keepdim=True)\n","    # similarity scores\n","    sims = (img_emb @ concept_embeddings.T).squeeze(0).cpu().numpy()\n","    idxs = np.argsort(sims)[::-1][:top_k]\n","    return [(concepts[i], float(sims[i])) for i in idxs]\n"],"metadata":{"id":"dHgasFf3mQ7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 5 – Install & Import GradCAM++\n","!pip install grad-cam --quiet\n","\n","from pytorch_grad_cam import GradCAMPlusPlus\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n"],"metadata":{"id":"KSzY2Rc2mUzD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 6 – GradCAM++ Explanation Function\n","\n","def explain_patch_gradcam(\n","    model: torch.nn.Module,\n","    target_layer: torch.nn.Module,\n","    patch: Image.Image,\n","    preprocess_fn,\n","    target_class: int | None = None,\n",") -> np.ndarray:\n","    \"\"\"\n","    Compute GradCAM++ heatmap for one patch.\n","    Returns a H×W float32 array in [0,1].\n","    \"\"\"\n","    # preprocess\n","    img_tensor = preprocess_fn(patch).unsqueeze(0).to(DEVICE)\n","    rgb = np.array(patch.resize((img_tensor.shape[-1], img_tensor.shape[-2]))) / 255.0\n","\n","    cam = GradCAMPlusPlus(\n","        model=model,\n","        target_layers=[target_layer],\n","        use_cuda=(DEVICE == \"cuda\"),\n","    )\n","    targets = [ClassifierOutputTarget(target_class)] if target_class is not None else None\n","    grayscale_cam = cam(input_tensor=img_tensor, targets=targets)[0]\n","\n","    # overlay on RGB (for plotting)\n","    overlay = show_cam_on_image(rgb, grayscale_cam, use_rgb=True)\n","    return grayscale_cam, overlay\n"],"metadata":{"id":"7oIo9ZuwmV3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cell 7 – Batch‐Process All Patches & Save CSV + Heatmaps\n","\n","def batch_explain_patches(\n","    patch_dir: Path,\n","    clip_model, clip_preprocess,\n","    concept_embeddings, concepts: list[str],\n","    classifier: torch.nn.Module | None,\n","    cam_layer: torch.nn.Module | None,\n","    top_k: int = 3,\n","):\n","    records = []\n","    for img_path in patch_dir.rglob(\"*.jpg\"):\n","        pid = img_path.stem\n","        try:\n","            img = Image.open(img_path).convert(\"RGB\")\n","            # concept match\n","            top_concepts = explain_patch_concepts(\n","                img, clip_model, clip_preprocess, concept_embeddings, concepts, top_k\n","            )\n","            rec = {\"patch_id\": pid}\n","            for i, (c, s) in enumerate(top_concepts, 1):\n","                rec[f\"concept_{i}\"] = c\n","                rec[f\"score_{i}\"]   = s\n","\n","            # GradCAM++ (optional)\n","            if classifier is not None and cam_layer is not None:\n","                # assume patch-level classification prediction\n","                classifier.eval()\n","                with torch.no_grad():\n","                    # forward to get predicted class\n","                    inp = clip_preprocess(img).unsqueeze(0).to(DEVICE)\n","                    out = classifier(inp)\n","                    cls = int(out.argmax(dim=-1).item())\n","                cam_map, _ = explain_patch_gradcam(\n","                    classifier, cam_layer, img, clip_preprocess, cls\n","                )\n","                # save heatmap as .npy\n","                np.save(OUTPUT_DIR / f\"{pid}_gradcam.npy\", cam_map)\n","                rec[\"predicted_class\"] = cls\n","            records.append(rec)\n","\n","        except Exception as e:\n","            print(f\"⚠️ Skipping {pid}: {e}\", file=sys.stderr)\n","\n","    df = pd.DataFrame(records)\n","    csv_path = OUTPUT_DIR / \"patch_explanations.csv\"\n","    df.to_csv(csv_path, index=False)\n","    print(f\"✔️ Saved CSV to {csv_path}\")\n","    return df\n","\n","# Example usage (if you have a classifier and know its target conv layer):\n","# df = batch_explain_patches(PATCH_DIR, clip_model, clip_preprocess,\n","#                            concept_embeddings, CONCEPTS,\n","#                            classifier=clf, cam_layer=clf.layer4[-1], top_k=3)\n","\n","# If you only want concepts:\n","df = batch_explain_patches(PATCH_DIR, clip_model, clip_preprocess,\n","                           concept_embeddings, CONCEPTS,\n","                           classifier=None, cam_layer=None, top_k=3)\n"],"metadata":{"id":"iwMSJZjAmXWl"},"execution_count":null,"outputs":[]}]}